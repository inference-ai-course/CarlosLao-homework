Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities
View PDF HTML (experimental)Abstract:While question-answering~(QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on \emph{cascaded question disclosure} that provides a more accurate estimate of the models' problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models. We further validate our findings by extensive ablation studies.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 31 Jul 2025]
Title:SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model
View PDF HTML (experimental)Abstract:AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \modelname improves the success of flight search from 0\% to 32.2\%. World-model-based planning, in particular, shows consistent advantage of up to 124\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \modelname with pretrained LLMs, available as a research demo for public testing.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 31 Jul 2025]
Title:CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks
View PDF HTML (experimental)Abstract:We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs
View PDF HTML (experimental)Abstract:Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at this https URL}{this https URL.
Submission history
From: Nasim Shirvani-Mahdavi [view email][v1] Thu, 31 Jul 2025 17:24:04 UTC (46 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 31 Jul 2025]
Title:Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving
View PDFAbstract:LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 31 Jul 2025]
Title:TextQuests: How Good are LLMs at Text-Based Video Games?
View PDF HTML (experimental)Abstract:Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 31 Jul 2025]
Title:TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.
Submission history
From: Muhammad Taha Cheema [view email][v1] Thu, 31 Jul 2025 15:50:57 UTC (471 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning
View PDF HTML (experimental)Abstract:Hate speech identification in social media has become an increasingly important issue in recent years. In this research, we address two problems: 1) to detect hate speech in Arabic text, 2) to clean a given text from hate speech. The meaning of cleaning here is replacing each bad word with stars based on the number of letters for each word. Regarding the first problem, we conduct several experiments using deep learning models and transformers to determine the best model in terms of the F1 score. Regarding second problem, we consider it as a machine translation task, where the input is a sentence containing dirty text and the output is the same sentence with masking the dirty text. The presented methods achieve the best model in hate speech detection with a 92\% Macro F1 score and 95\% accuracy. Regarding the text cleaning experiment, the best result in the hate speech masking model reached 0.3 in BLEU score with 1-gram, which is a good result compared with the state of the art machine translation systems.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 31 Jul 2025]
Title:Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates
View PDF HTML (experimental)Abstract:Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:DiffLoRA: Differential Low-Rank Adapters for Large Language Models
View PDF HTML (experimental)Abstract:Differential Transformer has recently been proposed to improve performance in Transformer models by canceling out noise through a denoiser attention mechanism. In this work, we introduce DiffLoRA, a parameter-efficient adaptation of the differential attention mechanism, with low-rank adapters on both positive and negative attention terms. This approach retains the efficiency of LoRA while aiming to benefit from the performance gains of differential attention. We evaluate DiffLoRA across a broad range of NLP tasks, including general benchmarks, many-shot in-context learning, RAG, and long-context tests. We observe that, although DiffLoRA falls short of other parameter-efficient fine-tuning methods in most evaluation tasks, it shows interesting results in certain domains (+11 pts on LoRA for HumanEval). We analyze the attention patterns post-finetuning to identify the reasons for this behavior.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text
View PDF HTML (experimental)Abstract:The proliferation of sophisticated text generation models necessitates the development of robust detection methods capable of identifying machine-generated content, particularly text designed to evade detection through adversarial perturbations. Existing zero-shot detectors often rely on statistical measures that implicitly assume Gaussian distributions, a premise that falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. This paper introduces T-Detect, a novel detection method that fundamentally redesigns the statistical core of curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning
View PDFAbstract:In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented **R**easoning framework driven by progressive **R**einforcement learning. In this framework, we first develop the model's ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the model's retrieval and reasoning coordination. Extensive experiments indicate that **Med-R$^3$** could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53\%.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Electrical Engineering and Systems Science > Audio and Speech Processing
[Submitted on 31 Jul 2025]
Title:MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks
View PDF HTML (experimental)Abstract:While large audio-language models have advanced open-ended audio understanding, they still fall short of nuanced human-level comprehension. This gap persists largely because current benchmarks, limited by data annotations and evaluation metrics, fail to reliably distinguish between generic and highly detailed model outputs. To this end, this work introduces MECAT, a Multi-Expert Constructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via a pipeline that integrates analysis from specialized expert models with Chain-of-Thought large language model reasoning, MECAT provides multi-perspective, fine-grained captions and open-set question-answering pairs. The benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced Audio Text Evaluation). This metric penalizes generic terms and rewards detailed descriptions by combining single-sample semantic similarity with cross-sample discriminability. A comprehensive evaluation of state-of-the-art audio models is also presented, providing new insights into their current capabilities and limitations. The data and code are available at this https URL
Current browse context:
eess.AS
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains
View PDFAbstract:Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%), with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Role-Aware Language Models for Secure and Contextualized Access Control in Organizations
View PDF HTML (experimental)Abstract:As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Cryptography and Security
[Submitted on 31 Jul 2025]
Title:Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems
View PDF HTML (experimental)Abstract:This paper investigates defenses for LLM-based evaluation systems against prompt injection. We formalize a class of threats called blind attacks, where a candidate answer is crafted independently of the true answer to deceive the evaluator. To counter such attacks, we propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates the submission against a deliberately false ground-truth answer. An attack is detected if the system validates an answer under both standard and counterfactual conditions. Experiments show that while standard evaluation is highly vulnerable, our SE+CFE framework significantly improves security by boosting attack detection with minimal performance trade-offs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration
View PDF HTML (experimental)Abstract:Critical thinking is essential for building robust AI systems, preventing them from blindly accepting flawed data or biased reasoning. However, prior work has primarily focused on passive critical thinking, where models simply reject problematic queries without taking constructive steps to address user requests. In this work, we introduce proactive critical thinking, a paradigm where models actively seek missing or clarifying information from users to resolve their queries better. To evaluate this capability, we present GSM-MC and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math problems with a key variable deliberately removed, requiring models to identify and request the missing information. GSM-MCE further increases the difficulty by introducing irrelevant details to test robustness against distractions. Experiments on Qwen3 and Llama series models show that, while these models excel in traditional reasoning tasks due to extensive post-training and inference-time scaling, they struggle with proactive critical thinking, especially smaller ones. However, we demonstrate that reinforcement learning (RL) can significantly improve this ability. Using our enhanced RL algorithm, we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to 73.98% on GSM-MC. We hope this work advances models that collaborate more effectively with users in problem-solving through proactive critical thinking.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Enhanced Arabic Text Retrieval with Attentive Relevance Scoring
View PDF HTML (experimental)Abstract:Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at \href{this https URL}{GitHub}.
Submission history
From: Salah Eddine Bekhouche SE. Bekhouche [view email][v1] Thu, 31 Jul 2025 10:18:28 UTC (166 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization
View PDF HTML (experimental)Abstract:The core challenge faced by multi-document summarization is the complexity of relationships among documents and the presence of information redundancy. Graph clustering is an effective paradigm for addressing this issue, as it models the complex relationships among documents using graph structures and reduces information redundancy through clustering, achieving significant research progress. However, existing methods often only consider single-relational graphs and require a predefined number of clusters, which hinders their ability to fully represent rich relational information and adaptively partition sentence groups to reduce redundancy. To overcome these limitations, we propose MRGSEM-Sum, an unsupervised multi-document summarization framework based on multi-relational graphs and structural entropy minimization. Specifically, we construct a multi-relational graph that integrates semantic and discourse relations between sentences, comprehensively modeling the intricate and dynamic connections among sentences across documents. We then apply a two-dimensional structural entropy minimization algorithm for clustering, automatically determining the optimal number of clusters and effectively organizing sentences into coherent groups. Finally, we introduce a position-aware compression mechanism to distill each cluster, generating concise and informative summaries. Extensive experiments on four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently outperforms previous unsupervised methods and, in several cases, achieves performance comparable to supervised models and large language models. Human evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high consistency and coverage, approaching human-level quality.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators
View PDFAbstract:The rapid proliferation of Large Language Models presents both opportunities and challenges for the translation field. While commercial, cloud-based AI chatbots have garnered significant attention in translation studies, concerns regarding data privacy, security, and equitable access necessitate exploration of alternative deployment models. This paper investigates the feasibility and performance of locally deployable, free language models as a viable alternative to proprietary, cloud-based AI solutions. This study evaluates three open-source models installed on CPU-based platforms and compared against commercially available online chat-bots. The evaluation focuses on functional performance rather than a comparative analysis of human-machine translation quality, an area already subject to extensive research. The platforms assessed were chosen for their accessibility and ease of use across various operating systems. While local deployment introduces its own challenges, the benefits of enhanced data control, improved privacy, and reduced dependency on cloud services are compelling. The findings of this study contribute to a growing body of knowledge concerning the democratization of AI technology and inform future research and development efforts aimed at making LLMs more accessible and practical for a wider range of users, specifically focusing on the needs of individual translators and small businesses.
Submission history
From: Peter Sandrini Dr. [view email][v1] Thu, 31 Jul 2025 10:13:48 UTC (3,561 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models
View PDF HTML (experimental)Abstract:Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models
View PDF HTML (experimental)Abstract:Multimodal planning capabilities refer to the ability to predict, reason, and design steps for task execution with multimodal context, which is essential for complex reasoning and decision-making across multiple steps. However, current benchmarks face two key challenges: (1) they cannot directly assess multimodal real-world planning capabilities, and (2) they lack constraints or implicit constraints across modalities. To address these issues, we introduce Multimodal Planning with Complex Constraints (MPCC), the first benchmark to systematically evaluate MLLMs' ability to handle multimodal constraints in planning. To address the first challenge, MPCC focuses on three real-world tasks: Flight Planning, Calendar Planning, and Meeting Planning. To solve the second challenge, we introduce complex constraints (e.g. budget, temporal, and spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to separate constraint complexity from search space expansion. Experiments on 13 advanced MLLMs reveal significant challenges: closed-source models achieve only 21.3% feasible plans, while open-source models average below 11%. Additionally, we observe that MLLMs are highly sensitive to constraint complexity and that traditional multimodal prompting strategies fail in multi-constraint scenarios. Our work formalizes multimodal constraints in planning, provides a rigorous evaluation framework, and highlights the need for advancements in constraint-aware reasoning for real-world MLLM applications.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Information Retrieval
[Submitted on 31 Jul 2025]
Title:Holistic Evaluations of Topic Models
View PDF HTML (experimental)Abstract:Topic models are gaining increasing commercial and academic interest for their ability to summarize large volumes of unstructured text. As unsupervised machine learning methods, they enable researchers to explore data and help general users understand key themes in large text collections. However, they risk becoming a 'black box', where users input data and accept the output as an accurate summary without scrutiny. This article evaluates topic models from a database perspective, drawing insights from 1140 BERTopic model runs. The goal is to identify trade-offs in optimizing model parameters and to reflect on what these findings mean for the interpretation and responsible use of topic models
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Software Engineering
[Submitted on 31 Jul 2025]
Title:SWE-Exp: Experience-Driven Software Issue Resolution
View PDFAbstract:Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.
Current browse context:
cs.SE
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Text-to-SQL Task-oriented Dialogue Ontology Construction
View PDF HTML (experimental)Abstract:Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Software Engineering
[Submitted on 31 Jul 2025]
Title:SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution
View PDFAbstract:Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.
Current browse context:
cs.SE
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 31 Jul 2025]
Title:DSBC : Data Science task Benchmarking with Context engineering
View PDF HTML (experimental)Abstract:Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.
Submission history
From: Ram Mohan Rao Kadiyala [view email][v1] Thu, 31 Jul 2025 08:32:37 UTC (12,021 KB)
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation
View PDF HTML (experimental)Abstract:Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content
View PDF HTML (experimental)Abstract:Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions. This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts. Our experiments indicate that GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 31 Jul 2025]
Title:SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy
View PDF HTML (experimental)Abstract:We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at this https URL.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Unveiling Super Experts in Mixture-of-Experts Large Language Models
View PDF HTML (experimental)Abstract:Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis
View PDF HTML (experimental)Abstract:Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at this https URL.
Submission history
From: Shimanto Bhowmik [view email][v1] Thu, 31 Jul 2025 05:16:43 UTC (1,051 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication
View PDF HTML (experimental)Abstract:There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 31 Jul 2025]
Title:Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents
View PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3 for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QR's potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts.
Current browse context:
cs.CV
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs
View PDF HTML (experimental)Abstract:Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders
View PDF HTML (experimental)Abstract:Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \textit{topic judge}, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples
View PDF HTML (experimental)Abstract:Large Language Models exhibit powerful few-shot in-context learning (ICL) capabilities, but the performance is highly sensitive to provided examples.
Recent research has focused on retrieving corresponding examples for each input query, not only enhancing the efficiency and scalability of the learning process but also mitigating inherent biases in manual example selection.
However, these studies have primarily emphasized leveraging Positive samples while overlooking the additional information within Negative samples for contextual learning.
We propose a novel method that utilizes Negative samples to better select Positive sample examples, thereby enhancing the performance of few-shot ICL. Initially, we construct Positive and Negative sample corpora based on Zero-Shot-Cot. Then, during inference, we employ a semantic similarity-based approach to select the most similar examples from both the Positive and Negative corpora for a given query. Subsequently, we further retrieve Positive examples from the Positive sample corpus based on semantic similarity to the Negative examples, then concatenating them with the previously selected Positive examples to serve as ICL demonstrations. Experimental results demonstrate that our approach surpasses methods solely relying on the most similar positive examples for context, validating that the additional information in negative samples aids in enhancing ICL performance through improved Positive sample selection.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks
View PDF HTML (experimental)Abstract:The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 31 Jul 2025]
Title:LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have demonstrated impressive performance across various tasks, with different models excelling in distinct domains and specific abilities. Effectively combining the predictions of multiple LLMs is crucial for enhancing system robustness and performance. However, existing ensemble methods often rely on simple techniques like voting or logits ensembling, which overlook the varying confidence and reliability of models in different contexts. In this work, we propose LENS (Learning ENsemble confidence from Neural States), a novel approach that learns to estimate model confidence by analyzing internal representations. For each LLM, we train a lightweight linear confidence predictor that leverages layer-wise hidden states and normalized probabilities as inputs. This allows for more nuanced weighting of model predictions based on their context-dependent reliability. Our method does not require modifying the model parameters and requires negligible additional computation. Experimental results on multiple-choice and boolean question-answering tasks demonstrate that LENS outperforms traditional ensemble methods by a substantial margin. Our findings suggest that internal representations provide valuable signals for determining model confidence and can be effectively leveraged for ensemble learning.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal
View PDF HTML (experimental)Abstract:Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the user's initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans
View PDF HTML (experimental)Abstract:Understanding causal relationships across modalities is a core challenge for multimodal models operating in real-world environments. We introduce ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text. Each example presents an image of a task step and a text snippet from a plan, with the goal of deciding whether the visual step occurs before or after the referenced text step. Evaluation results on ten frontier vision-language models show underwhelming performance: the best zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further highlights concrete directions for improving causal understanding in multimodal models.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity
View PDF HTML (experimental)Abstract:In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL
View PDFAbstract:Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity
View PDFAbstract:We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Context-aware Rotary Position Embedding
View PDF HTML (experimental)Abstract:Positional encoding is a vital component of Transformer architectures, enabling models to incorporate sequence order into self-attention mechanisms. Rotary Positional Embeddings (RoPE) have become a widely adopted solution due to their compatibility with relative position encoding and computational efficiency. However, RoPE relies on static, input-independent sinusoidal frequency patterns, limiting its ability to model context-sensitive relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional Embedding), a novel generalization of RoPE that dynamically generates head-specific frequency patterns conditioned on token embeddings. This design introduces token- and context-sensitive positional representations while preserving RoPE efficiency and architectural simplicity. CARoPE computes input-dependent phase shifts using a bounded transformation of token embeddings and integrates them into the rotary mechanism across attention heads. We evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on next-token prediction tasks. Experimental results show that CARoPE consistently outperforms RoPE and other common positional encoding baselines, achieving significantly lower perplexity, even at longer context lengths. Additionally, CARoPE enables faster training throughput without sacrificing model stability. These findings demonstrate that CARoPE offers a scalable, expressive, and efficient upgrade to existing positional encoding strategies in Transformer models.
Submission history
From: Hamidreza Amirzadeh [view email][v1] Wed, 30 Jul 2025 20:32:19 UTC (32 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Exploring In-Context Learning for Frame-Semantic Parsing
View PDF HTML (experimental)Abstract:Frame Semantic Parsing (FSP) entails identifying predicates and labeling their arguments according to Frame Semantics. This paper investigates the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP without model fine-tuning. We propose a method that automatically generates task-specific prompts for the Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks, relying solely on the FrameNet database. These prompts, constructed from frame definitions and annotated examples, are used to guide six different LLMs. Experiments are conducted on a subset of frames related to violent events. The method achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers a practical and effective alternative to traditional fine-tuning for domain-specific FSP tasks.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Math Natural Language Inference: this should be easy!
View PDF HTML (experimental)Abstract:We ask whether contemporary LLMs are able to perform natural language inference (NLI) tasks on mathematical texts. We call this the Math NLI problem. We construct a corpus of Math NLI pairs whose premises are from extant mathematical text and whose hypotheses and gold labels were provided by people with experience in both research-level mathematics and also in the NLI field. We also investigate the quality of corpora using the same premises but whose hypotheses are provided by LLMs themselves. We not only investigate the performance but also the inter-group consistency of the diverse group of LLMs. We have both positive and negative findings. Among our positive findings: in some settings, using a majority vote of LLMs is approximately equivalent to using human-labeled data in the Math NLI area. On the negative side: LLMs still struggle with mathematical language. They occasionally fail at even basic inferences. Current models are not as prone to hypothesis-only "inference" in our data the way the previous generation had been. In addition to our findings, we also provide our corpora as data to support future work on Math NLI.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning
View PDF HTML (experimental)Abstract:In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.
Submission history
From: Kwesi Adu Cobbina [view email][v1] Wed, 30 Jul 2025 17:59:46 UTC (9,561 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations
View PDF HTML (experimental)Abstract:Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Information Retrieval
[Submitted on 30 Jul 2025 (v1), last revised 31 Jul 2025 (this version, v2)]
Title:RecGPT Technical Report
View PDFAbstract:Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.
To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.
Submission history
From: Jiakai Tang [view email][v1] Wed, 30 Jul 2025 17:55:06 UTC (5,727 KB)
[v2] Thu, 31 Jul 2025 16:54:43 UTC (5,725 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Information Retrieval
[Submitted on 30 Jul 2025]
Title:GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis
View PDF HTML (experimental)Abstract:Detecting, analyzing, and predicting power outages is crucial for grid risk assessment and disaster mitigation. Numerous outages occur each year, exacerbated by extreme weather events such as hurricanes. Existing outage data are typically reported at the county level, limiting their spatial resolution and making it difficult to capture localized patterns. However, it offers excellent temporal granularity. In contrast, nighttime light satellite image data provides significantly higher spatial resolution and enables a more comprehensive spatial depiction of outages, enhancing the accuracy of assessing the geographic extent and severity of power loss after disaster events. However, these satellite data are only available on a daily basis. Integrating spatiotemporal visual and time-series data sources into a unified knowledge representation can substantially improve power outage detection, analysis, and predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S. We describe our method for constructing GeoOutageKG by aligning source data with a developed ontology, GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual outage records spanning from 2014 to 2024, 300,000 NTL images spanning from 2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration. We demonstrate its use through multiresolution analysis of geospatiotemporal power outages.
Current browse context:
cs.IR
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 30 Jul 2025]
Title:The Incomplete Bridge: How AI Research (Mis)Engages with Psychology
View PDF HTML (experimental)Abstract:Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization
View PDF HTML (experimental)Abstract:Query-focused table summarization requires complex reasoning, often approached through step-by-step natural language (NL) plans. However, NL plans are inherently ambiguous and lack structure, limiting their conversion into executable programs like SQL and hindering scalability, especially for multi-table tasks. To address this, we propose a paradigm shift to structured representations. We introduce a new structured plan, TaSoF, inspired by formalism in traditional multi-agent systems, and a framework, SPaGe, that formalizes the reasoning process in three phases: 1) Structured Planning to generate TaSoF from a query, 2) Graph-based Execution to convert plan steps into SQL and model dependencies via a directed cyclic graph for parallel execution, and 3) Summary Generation to produce query-focused summaries. Our method explicitly captures complex dependencies and improves reliability. Experiments on three public benchmarks show that SPaGe consistently outperforms prior models in both single- and multi-table settings, demonstrating the advantages of structured representations for robust and scalable summarization.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph
View PDF HTML (experimental)Abstract:In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer of the LLM.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:MASCA: LLM based-Multi Agents System for Credit Assessment
View PDF HTML (experimental)Abstract:Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Opportunities and Challenges of LLMs in Education: An NLP Perspective
View PDF HTML (experimental)Abstract:Interest in the role of large language models (LLMs) in education is increasing, considering the new opportunities they offer for teaching, learning, and assessment. In this paper, we examine the impact of LLMs on educational NLP in the context of two main application scenarios: {\em assistance} and {\em assessment}, grounding them along the four dimensions -- reading, writing, speaking, and tutoring. We then present the new directions enabled by LLMs, and the key challenges to address. We envision that this holistic overview would be useful for NLP researchers and practitioners interested in exploring the role of LLMs in developing language-focused and NLP-enabled educational applications of the future.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset
View PDFAbstract:We introduce a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. We also provide strong baselines using state-of-the-art large language models (LLMs). Our dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations. It includes both purely textual questions and those requiring visual understanding. As a baseline, we evaluate state-of-the-art LLMs through prompting and complement this with human judgments of answer correctness. Using these human evaluations, we analyze the reliability of existing automatic evaluation metrics. Our baseline results highlight a significant gap in regional knowledge among current LLMs. Moreover, apart from LLM-based evaluation, there is minimal correlation between automated metrics and human judgment. We release this dataset as a resource to (1) assess regional knowledge in LLMs, (2) study cross-lingual generation consistency in a challenging setting, and (3) advance the development of evaluation metrics for open-ended question answering.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Sound
[Submitted on 30 Jul 2025]
Title:Next Tokens Denoising for Speech Synthesis
View PDF HTML (experimental)Abstract:While diffusion and autoregressive (AR) models have significantly advanced generative modeling, they each present distinct limitations. AR models, which rely on causal attention, cannot exploit future context and suffer from slow generation speeds. Conversely, diffusion models struggle with key-value (KV) caching. To overcome these challenges, we introduce Dragon-FM, a novel text-to-speech (TTS) design that unifies AR and flow-matching. This model processes 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per second rate. This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Consequently, the proposed model can utilize KV-cache across chunks and incorporate future context within each chunk. Furthermore, it bridges continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers. This efficient codec and fast chunk-autoregressive architecture also makes the proposed model particularly effective for generating extended content. Experiment for demos of our work} on podcast datasets demonstrate its capability to efficiently generate high-quality zero-shot podcasts.
Current browse context:
cs.SD
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index
View PDF HTML (experimental)Abstract:Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI.
Submission history
From: Praveenkumar Katwe [view email][v1] Wed, 30 Jul 2025 15:00:00 UTC (673 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Investigating Hallucination in Conversations for Low Resource Languages
View PDFAbstract:Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs
View PDF HTML (experimental)Abstract:Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment
View PDF HTML (experimental)Abstract:Interview performance assessment is essential for determining candidates' suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores ``365'' aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Multilingual Political Views of Large Language Models: Identification and Steering
View PDF HTML (experimental)Abstract:Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.
In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at this https URL.
Submission history
From: Daniil Gurgurov [view email][v1] Wed, 30 Jul 2025 12:42:35 UTC (16,970 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation
View PDF HTML (experimental)Abstract:Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.
Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at this https URL.
Submission history
From: Daniil Gurgurov [view email][v1] Wed, 30 Jul 2025 12:23:39 UTC (23,788 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 30 Jul 2025 (v1), last revised 31 Jul 2025 (this version, v2)]
Title:VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning
View PDF HTML (experimental)Abstract:Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.
Submission history
From: Hao Zhang [view email][v1] Wed, 30 Jul 2025 12:23:21 UTC (603 KB)
[v2] Thu, 31 Jul 2025 09:09:45 UTC (610 KB)
Current browse context:
cs.CV
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:BALSAM: A Platform for Benchmarking Arabic Large Language Models
View PDF HTML (experimental)Abstract:The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.
Submission history
From: Raghad Al-Rasheed [view email][v1] Wed, 30 Jul 2025 12:16:39 UTC (8,910 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025 (v1), last revised 31 Jul 2025 (this version, v2)]
Title:Unveiling the Influence of Amplifying Language-Specific Neurons
View PDF HTML (experimental)Abstract:Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
Submission history
From: Inaya Rahmanisa [view email][v1] Wed, 30 Jul 2025 11:23:30 UTC (2,661 KB)
[v2] Thu, 31 Jul 2025 03:32:19 UTC (2,661 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 30 Jul 2025]
Title:Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning
View PDF HTML (experimental)Abstract:The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.
Submission history
From: Afshin Khadangi [view email][v1] Wed, 30 Jul 2025 10:46:53 UTC (32,990 KB)
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:ControlMed: Adding Reasoning Control to Medical Language Model
View PDF HTML (experimental)Abstract:Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 30 Jul 2025]
Title:Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law
View PDF HTML (experimental)Abstract:Tokenization is a fundamental step in natural language processing (NLP) and other sequence modeling domains, where the choice of vocabulary size significantly impacts model performance. Despite its importance, selecting an optimal vocabulary size remains underexplored, typically relying on heuristics or dataset-specific choices. In this work, we propose a principled method for determining the vocabulary size by analyzing token frequency distributions through Zipf's law. We show that downstream task performance correlates with how closely token distributions follow power-law behavior, and that aligning with Zipfian scaling improves both model efficiency and effectiveness. Extensive experiments across NLP, genomics, and chemistry demonstrate that models consistently achieve peak performance when the token distribution closely adheres to Zipf's law, establishing Zipfian alignment as a robust and generalizable criterion for vocabulary size selection.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support
View PDF HTML (experimental)Abstract:With the rapid growth of Artificial Intelligence, Large Language Models (LLMs) have become essential for Question Answering (QA) systems, improving efficiency and reducing human workload in customer service. The emergence of Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a practical choice for their accuracy, efficiency, and privacy benefits. However, domain-specific evaluations remain limited, and the absence of benchmark datasets reflecting real customer interactions makes it difficult for enterprises to select suitable models for support applications. To address this gap, we introduce the Customer Support Conversations Dataset (CSConDa), a curated benchmark of over 9,000 QA pairs drawn from real interactions with human advisors at a large Vietnamese software company. Covering diverse topics such as pricing, product availability, and technical troubleshooting, CSConDa provides a representative basis for evaluating ViLLMs in practical scenarios. We further present a comprehensive evaluation framework, benchmarking 11 lightweight open-source ViLLMs on CSConDa with both automatic metrics and syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns. This study offers insights into model behavior, explains performance differences, and identifies key areas for improvement, supporting the development of next-generation ViLLMs. By establishing a robust benchmark and systematic evaluation, our work enables informed model selection for customer service QA and advances research on Vietnamese LLMs. The dataset is publicly available at this https URL.
Submission history
From: Long Nguyen Song Thien [view email][v1] Wed, 30 Jul 2025 10:14:31 UTC (1,752 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:SLM-SQL: An Exploration of Small Language Models for Text-to-SQL
View PDF HTML (experimental)Abstract:Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy (EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset, model, and code to github: this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Electrical Engineering and Systems Science > Audio and Speech Processing
[Submitted on 30 Jul 2025]
Title:Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR
View PDFAbstract:The dynamic characteristics of speech signal provides temporal information and play an important role in enhancing Automatic Speech Recognition (ASR). In this work, we characterized the acoustic transitions in a ratio plane of Spectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture the dynamic characteristics of the speech and minimize spectral variation. These dynamic parameters were combined with Mel-Frequency Cepstral Coefficients (MFCCs) in Vietnamese ASR to capture more detailed spectral information. The SSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to describe the tonal information robustly. The findings showed that the proposed parameters significantly reduce word error rates and exhibit greater gender independence than the baseline MFCCs.
Submission history
From: Sotheara Leang [view email] [via CCSD proxy][v1] Wed, 30 Jul 2025 08:25:55 UTC (1,813 KB)
Current browse context:
eess.AS
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:IFEvalCode: Controlled Code Generation
View PDF HTML (experimental)Abstract:Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models
View PDF HTML (experimental)Abstract:Recent work has argued that large language models (LLMs) are not "abstract reasoners", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an "abstract reasoner", and why it matters whether LLMs fit the bill.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance
View PDF HTML (experimental)Abstract:In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini
View PDFAbstract:Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a 1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI.
Submission history
From: Jill Walker Rettberg [view email][v1] Wed, 30 Jul 2025 07:44:28 UTC (2,150 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models
View PDF HTML (experimental)Abstract:The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at this https URL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Question Generation for Assessing Early Literacy Reading Comprehension
View PDF HTML (experimental)Abstract:Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs
View PDF HTML (experimental)Abstract:Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research.
Submission history
From: Homaira Huda Shomee [view email][v1] Wed, 30 Jul 2025 05:17:35 UTC (1,320 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors
View PDF HTML (experimental)Abstract:Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors, with asynchronous patterns across modalities. It was hard to model personality semantics with traditional superficial features and seemed impossible to achieve effective cross-modal understanding. To address these challenges, we propose a novel personality assessment framework called \textit{\textbf{Traits Run Deep}}. It employs \textit{\textbf{psychology-informed prompts}} to elicit high-level personality-relevant semantic representations. Besides, it devises a \textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text semantics to align and integrate asynchronous signals from other modalities. To be specific, such fusion module includes a Chunk-Wise Projector to decrease dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for effective modality fusion and an ensemble regression head to improve generalization in data-scarce situations. To our knowledge, we are the first to apply personality-specific prompts to guide large language models (LLMs) in extracting personality-aware semantics for improved representation quality. Furthermore, extracting and fusing audio-visual apparent behavior features further improves the accuracy. Experimental results on the AVI validation set have demonstrated the effectiveness of the proposed components, i.e., approximately a 45\% reduction in mean squared error (MSE). Final evaluations on the test set of the AVI Challenge 2025 confirm our method's superiority, ranking first in the Personality Assessment track. The source code will be made available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 30 Jul 2025 (v1), last revised 31 Jul 2025 (this version, v2)]
Title:LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models
View PDF HTML (experimental)Abstract:Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness).
Submission history
From: Qianhong Guo [view email][v1] Wed, 30 Jul 2025 03:50:46 UTC (2,178 KB)
[v2] Thu, 31 Jul 2025 03:28:30 UTC (742 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 30 Jul 2025]
Title:A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers
View PDF HTML (experimental)Abstract:Understanding and solving complex reasoning tasks is vital for addressing the information needs of a user. Although dense neural models learn contextualised embeddings, they still underperform on queries containing negation. To understand this phenomenon, we study negation in both traditional neural information retrieval and LLM-based models. We (1) introduce a taxonomy of negation that derives from philosophical, linguistic, and logical definitions; (2) generate two benchmark datasets that can be used to evaluate the performance of neural information retrieval models and to fine-tune models for a more robust performance on negation; and (3) propose a logic-based classification mechanism that can be used to analyze the performance of retrieval models on existing datasets. Our taxonomy produces a balanced data distribution over negation types, providing a better training setup that leads to faster convergence on the NevIR dataset. Moreover, we propose a classification schema that reveals the coverage of negation types in existing datasets, offering insights into the factors that might affect the generalization of fine-tuned models on negation.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations
View PDF HTML (experimental)Abstract:Intent recognition is a fundamental component in task-oriented dialogue systems (TODS). Determining user intents and detecting whether an intent is Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However, traditional TODS require large amount of annotated data. In this work we propose a hybrid approach to combine BERT and LLMs in zero and few-shot settings to recognize intents and detect OOS utterances. Our approach leverages LLMs generalization power and BERT's computational efficiency in such scenarios. We evaluate our method on multi-party conversation corpora and observe that sharing information from BERT outputs to LLMs leads to system performance improvement.
Submission history
From: Galo Castillo-López [view email][v1] Tue, 29 Jul 2025 23:48:41 UTC (695 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs
View PDF HTML (experimental)Abstract:The usage-based constructionist (UCx) approach posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze the neural representations of the English dative constructions (Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied for human-rated preference strength. A macro-level geometric analysis finds that the separability between construction representations, as measured by Energy Distance or Jensen-Shannon Divergence, is systematically modulated by gradient preference strength. More prototypical exemplars of each construction occupy more distinct regions in the activation space of LLMs. These results provide strong evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures of basic constructionist principles in LLMs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 29 Jul 2025]
Title:CoEx -- Co-evolving World-model and Exploration
View PDFAbstract:Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation
View PDF HTML (experimental)Abstract:Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.
Submission history
From: Jude Dongyub Lee [view email][v1] Tue, 29 Jul 2025 20:35:35 UTC (7,798 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?
View PDF HTML (experimental)Abstract:Contextual entropy is a psycholinguistic measure capturing the anticipated difficulty of processing a word just before it is encountered. Recent studies have tested for entropy-related effects as a potential complement to well-known effects from surprisal. For convenience, entropy is typically estimated based on a language model's probability distribution over a word's first subword token. However, this approximation results in underestimation and potential distortion of true word entropy. To address this, we generate Monte Carlo (MC) estimates of word entropy that allow words to span a variable number of tokens. Regression experiments on reading times show divergent results between first-token and MC word entropy, suggesting a need for caution in using first-token approximations of contextual entropy.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:The role of media memorability in facilitating startups' access to venture capital funding
View PDFAbstract:Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.
Submission history
From: Andrea Fronzetti Colladon PhD [view email][v1] Tue, 29 Jul 2025 19:58:41 UTC (822 KB)
Current browse context:
cs.CL
Change to browse by:
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 29 Jul 2025]
Title:Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence
View PDFAbstract:This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity--not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the "systematicity challenge" to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed "the systematicity of thought." I offer a conceptual framework for thinking about "the systematicity of thought" that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the "hard systematicity challenge." However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models
View PDF HTML (experimental)Abstract:We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles
View PDF HTML (experimental)Abstract:Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with "non-standard" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Cryptography and Security
[Submitted on 29 Jul 2025]
Title:Strategic Deflection: Defending LLMs from Logit Manipulation
View PDF HTML (experimental)Abstract:With the growing adoption of Large Language Models (LLMs) in critical areas, ensuring their security against jailbreaking attacks is paramount. While traditional defenses primarily rely on refusing malicious prompts, recent logit-level attacks have demonstrated the ability to bypass these safeguards by directly manipulating the token-selection process during generation. We introduce Strategic Deflection (SDeflection), a defense that redefines the LLM's response to such advanced attacks. Instead of outright refusal, the model produces an answer that is semantically adjacent to the user's request yet strips away the harmful intent, thereby neutralizing the attacker's harmful intent. Our experiments demonstrate that SDeflection significantly lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This work presents a critical shift in defensive strategies, moving from simple refusal to strategic content redirection to neutralize advanced threats.
Current browse context:
cs.CR
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian
View PDFAbstract:Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset specifically designed to evaluate the naturalness and quality of LLM-generated text. All annotations are natively written in Indonesian and evaluated using Krippendorff's alpha, demonstrating strong inter-annotator agreement. Additionally, we benchmark the dataset across multiple LLMs and assess the output quality of each model.
Submission history
From: Genta Indra Winata [view email][v1] Tue, 29 Jul 2025 18:46:25 UTC (274 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Cryptography and Security
[Submitted on 29 Jul 2025]
Title:Prompt Optimization and Evaluation for LLM Automated Red Teaming
View PDF HTML (experimental)Abstract:Applications that use Large Language Models (LLMs) are becoming widespread, making the identification of system vulnerabilities increasingly important. Automated Red Teaming accelerates this effort by using an LLM to generate and execute attacks against target systems. Attack generators are evaluated using the Attack Success Rate (ASR) the sample mean calculated over the judgment of success for each attack. In this paper, we introduce a method for optimizing attack generator prompts that applies ASR to individual attacks. By repeating each attack multiple times against a randomly seeded target, we measure an attack's discoverability the expectation of the individual attack success. This approach reveals exploitable patterns that inform prompt optimization, ultimately enabling more robust evaluation and refinement of generators.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 29 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]
Title:Meta CLIP 2: A Worldwide Scaling Recipe
View PDF HTML (experimental)Abstract:Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.
Submission history
From: Hu Xu [view email][v1] Tue, 29 Jul 2025 17:59:58 UTC (837 KB)
[v2] Wed, 30 Jul 2025 22:14:32 UTC (837 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]
Title:DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches. Our codes are available at this https URL.
Submission history
From: Minghao Guo [view email][v1] Tue, 29 Jul 2025 17:55:23 UTC (426 KB)
[v2] Wed, 30 Jul 2025 15:51:29 UTC (427 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 29 Jul 2025]
Title:UserBench: An Interactive Gym Environment for User-Centric Agents
View PDF HTML (experimental)Abstract:Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 29 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]
Title:UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding
View PDF HTML (experimental)Abstract:The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a "Simple Thinking" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.
Submission history
From: Shuquan Lian [view email][v1] Tue, 29 Jul 2025 17:22:07 UTC (2,092 KB)
[v2] Wed, 30 Jul 2025 12:17:53 UTC (2,111 KB)
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models
View PDF HTML (experimental)Abstract:Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models (LLMs) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that LLMs not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that LLMs can effectively reason over sparse, heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation
View PDF HTML (experimental)Abstract:In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Post-Training Large Language Models via Reinforcement Learning from Self-Feedback
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.
RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.
By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]
Title:Training language models to be warm and empathetic makes them less reliable and more sycophantic
View PDF HTML (experimental)Abstract:Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.
Submission history
From: Lujain Ibrahim [view email][v1] Tue, 29 Jul 2025 15:33:20 UTC (1,073 KB)
[v2] Wed, 30 Jul 2025 10:11:59 UTC (1,071 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs
View PDF HTML (experimental)Abstract:Rote learning is a memorization technique based on repetition. It is commonly believed to hinder generalization by encouraging verbatim memorization rather than deeper understanding. This insight holds for even learning factual knowledge that inevitably requires a certain degree of memorization. In this work, we demonstrate that LLMs can be trained to generalize from rote memorized data. We introduce a two-phase memorize-then-generalize framework, where the model first rote memorizes factual subject-object associations using a semantically meaningless token and then learns to generalize by fine-tuning on a small set of semantically meaningful prompts. Extensive experiments over 8 LLMs show that the models can reinterpret rote memorized data through the semantically meaningful prompts, as evidenced by the emergence of structured, semantically aligned latent representations between the two. This surprising finding opens the door to both effective and efficient knowledge injection and possible risks of repurposing the memorized data for malicious usage.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Social and Information Networks
[Submitted on 29 Jul 2025 (v1), last revised 31 Jul 2025 (this version, v2)]
Title:Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation
View PDF HTML (experimental)Abstract:As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article.
Submission history
From: Tiviatis Sim [view email][v1] Tue, 29 Jul 2025 15:14:39 UTC (5,154 KB)
[v2] Thu, 31 Jul 2025 15:33:32 UTC (5,152 KB)
Current browse context:
cs.SI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning
View PDF HTML (experimental)Abstract:Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning
View PDF HTML (experimental)Abstract:Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences
View PDFAbstract:LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Modelling Adjectival Modification Effects on Semantic Plausibility
View PDF HTML (experimental)Abstract:While the task of assessing the plausibility of events such as ''news is relevant'' has been addressed by a growing body of work, less attention has been paid to capturing changes in plausibility as triggered by event modification. Understanding changes in plausibility is relevant for tasks such as dialogue generation, commonsense reasoning, and hallucination detection as it allows to correctly model, for example, ''gentle sarcasm'' as a sign of closeness rather than unkindness among friends [9]. In this work, we tackle the ADEPT challenge benchmark [6] consisting of 16K English sentence pairs differing by exactly one adjectival modifier. Our modeling experiments provide a conceptually novel method by using sentence transformers, and reveal that both they and transformer-based models struggle with the task at hand, and sentence transformers - despite their conceptual alignment with the task - even under-perform in comparison to models like RoBERTa. Furthermore, an in-depth comparison with prior work highlights the importance of a more realistic, balanced evaluation method: imbalances distort model performance and evaluation metrics, and weaken result trustworthiness.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs
View PDF HTML (experimental)Abstract:Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish
View PDF HTML (experimental)Abstract:This paper summarizes the main findings of ADoBo 2025, the shared task on anglicism identification in Spanish proposed in the context of IberLEF 2025. Participants of ADoBo 2025 were asked to detect English lexical borrowings (or anglicisms) from a collection of Spanish journalistic texts. Five teams submitted their solutions for the test phase. Proposed systems included LLMs, deep learning models, Transformer-based models and rule-based systems. The results range from F1 scores of 0.17 to 0.99, which showcases the variability in performance different systems can have for this task.
Submission history
From: Elena Álvarez-Mellado [view email][v1] Tue, 29 Jul 2025 13:45:08 UTC (62 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:ChartMark: A Structured Grammar for Chart Annotation
View PDF HTML (experimental)Abstract:Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:The Problem with Safety Classification is not just the Models
View PDFAbstract:Studying the robustness of Large Language Models (LLMs) to unsafe behaviors is an important topic of research today. Building safety classification models or guard models, which are fine-tuned models for input/output safety classification for LLMs, is seen as one of the solutions to address the issue. Although there is a lot of research on the safety testing of LLMs themselves, there is little research on evaluating the effectiveness of such safety classifiers or the evaluation datasets used for testing them, especially in multilingual scenarios. In this position paper, we demonstrate how multilingual disparities exist in 5 safety classification models by considering datasets covering 18 languages. At the same time, we identify potential issues with the evaluation datasets, arguing that the shortcomings of current safety classifiers are not only because of the models themselves. We expect that these findings will contribute to the discussion on developing better methods to identify harmful content in LLM inputs across languages.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models
View PDF HTML (experimental)Abstract:In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal
View PDF HTML (experimental)Abstract:Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases
View PDF HTML (experimental)Abstract:As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at this https URL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Libra: Assessing and Improving Reward Model by Learning to Think
View PDF HTML (experimental)Abstract:Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Multilingual JobBERT for Cross-Lingual Job Title Matching
View PDF HTML (experimental)Abstract:We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual job title matching. Building on the state-of-the-art monolingual JobBERT-V2, our approach extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a balanced multilingual dataset of over 21 million job titles. The model retains the efficiency-focused architecture of its predecessor while enabling robust alignment across languages without requiring task-specific supervision. Extensive evaluations on the TalentCLEF 2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual baselines and achieves consistent performance across both monolingual and cross-lingual settings. While not the primary focus, we also show that the model can be effectively used to rank relevant skills for a given job title, demonstrating its broader applicability in multilingual labor market intelligence. The model is publicly available: this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025 (v1), last revised 31 Jul 2025 (this version, v2)]
Title:Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages
View PDF HTML (experimental)Abstract:This paper explores sequence-level knowledge distillation (KD) of multilingual pre-trained encoder-decoder translation models. We argue that the teacher model's output distribution holds valuable insights for the student, beyond the approximated mode obtained through beam search (the standard decoding method), and present Multi-Hypothesis Distillation (MHD), a sequence-level KD method that generates multiple translations for each source sentence. This provides a larger representation of the teacher model distribution and exposes the student model to a wider range of target-side prefixes. We leverage $n$-best lists from beam search to guide the student's learning and examine alternative decoding methods to address issues like low variability and the under-representation of infrequent tokens. For low-resource languages, our research shows that while sampling methods may slightly compromise translation quality compared to beam search based approaches, they enhance the generated corpora with greater variability and lexical richness. This ultimately improves student model performance and mitigates the gender bias amplification often associated with KD.
Submission history
From: Aarón Galiano-Jiménez [view email][v1] Tue, 29 Jul 2025 07:59:20 UTC (6,521 KB)
[v2] Thu, 31 Jul 2025 08:13:22 UTC (6,521 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers
View PDF HTML (experimental)Abstract:This study investigates the cognitive plausibility of the Spanish irregular morphomic pattern by directly comparing transformer-based neural networks to human behavioral data from \citet{Nevins2015TheRA}. Using the same analytical framework as the original human study, we evaluate whether transformer models can replicate human-like sensitivity to a complex linguistic phenomena, the morphome, under controlled input conditions. Our experiments focus on three frequency conditions: natural, low-frequency, and high-frequency distributions of verbs exhibiting irregular morphomic patterns. While the models outperformed humans in stem and suffix accuracy, a clear divergence emerged in response preferences. Unlike humans, who consistently favored natural responses across all test items, models' preferred irregular responses and were influenced by the proportion of irregular verbs in their training data. Additionally, models trained on the natural and low-frequency distributions, but not the high-frequency distribution, were sensitive to the phonological similarity between test items and real Spanish L-shaped verbs.
Submission history
From: Akhilesh Kakolu Ramarao [view email][v1] Tue, 29 Jul 2025 07:40:32 UTC (543 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation
View PDF HTML (experimental)Abstract:Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language
View PDFAbstract:To address a critical resource gap in Uyghur Natural Language Processing (NLP), this study introduces a dependency annotation framework designed to overcome the limitations of existing treebanks for the low-resource, agglutinative language. This inventory includes 18 main relations and 26 subtypes, with specific labels such as cop:zero for verbless clauses and instr:case=loc/dat for nuanced instrumental functions. To empirically validate the necessity of this tailored approach, we conducted a cross-standard evaluation using a pre-trained Universal Dependencies parser. The analysis revealed a systematic 47.9% divergence in annotations, pinpointing the inadequacy of universal schemes for handling Uyghur-specific structures. Grounded in nine annotation principles that ensure typological accuracy and semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a more accurate and semantically transparent representation, designed to enable significant improvements in parsing and downstream NLP tasks, and offers a replicable model for other morphologically complex languages.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Automatic Classification of User Requirements from Online Feedback -- A Replication Study
View PDF HTML (experimental)Abstract:Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static sparse attention methods typically degrade accuracy, while dynamic sparsity methods introduce additional computational overhead due to runtime sparse index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance LLM inference efficiency.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting
View PDF HTML (experimental)Abstract:End-to-end automatic speech recognition (ASR) systems based on transformer architectures, such as Whisper, offer high transcription accuracy and robustness. However, their autoregressive decoding is computationally expensive, hence limiting deployment on CPU-based and resource-constrained devices. Speculative decoding (SD) mitigates this issue by using a smaller draft model to propose candidate tokens, which are then verified by the main model. However, this approach is impractical for devices lacking hardware accelerators like GPUs. To address this, we propose \emph{Token Map Drafting}, a model-free SD technique that eliminates the need for a separate draft model. Instead, we leverage a precomputed n-gram token map derived from domain-specific training data, enabling efficient speculative decoding with minimal overhead. Our method significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results demonstrate decoding speed-ups of $1.27\times$ on the CI-AVSR dataset and $1.37\times$ on our internal dataset without degrading recognition accuracy. Additionally, our approach achieves a $10\%$ absolute improvement in decoding speed over the Distill-spec baseline running on CPU, highlighting its effectiveness for on-device ASR applications.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 29 Jul 2025]
Title:What Does it Mean for a Neural Network to Learn a "World Model"?
View PDF HTML (experimental)Abstract:We propose a set of precise criteria for saying a neural net learns and uses a "world model." The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent "state space" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a "world model" is not a trivial consequence of the neural net's data or task.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Persona Vectors: Monitoring and Controlling Character Traits in Language Models
View PDFAbstract:Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:VN-MTEB: Vietnamese Massive Text Embedding Benchmark
View PDF HTML (experimental)Abstract:Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: this https URL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs
View PDF HTML (experimental)Abstract:Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\%.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench
View PDF HTML (experimental)Abstract:We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and this http URL, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour
View PDF HTML (experimental)Abstract:This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 29 Jul 2025]
Title:MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations
View PDF HTML (experimental)Abstract:Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 29 Jul 2025]
Title:ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs
View PDF HTML (experimental)Abstract:The computational cost of training multimodal large language models (MLLMs) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student framework in which the MLLM being trained serves as the student, and a frozen reference large language model (LLM) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 28 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]
Title:Multimodal LLMs as Customized Reward Models for Text-to-Image Generation
View PDF HTML (experimental)Abstract:We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations. In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.
Submission history
From: Shijie Zhou [view email][v1] Mon, 28 Jul 2025 23:52:53 UTC (2,491 KB)
[v2] Wed, 30 Jul 2025 04:49:38 UTC (2,498 KB)
Current browse context:
cs.CV
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 28 Jul 2025]
Title:Teaching Language Models To Gather Information Proactively
View PDF HTML (experimental)Abstract:Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Turbocharging Web Automation: The Impact of Compressed History States
View PDF HTML (experimental)Abstract:Language models have led to a leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequences and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation
View PDF HTML (experimental)Abstract:Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.
Submission history
From: Satyananda Kashyap [view email][v1] Mon, 28 Jul 2025 21:20:44 UTC (788 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:A Deep Learning Automatic Speech Recognition Model for Shona Language
View PDFAbstract:This study presented the development of a deep learning-based Automatic Speech Recognition system for Shona, a low-resource language characterized by unique tonal and grammatical complexities. The research aimed to address the challenges posed by limited training data, lack of labelled data, and the intricate tonal nuances present in Shona speech, with the objective of achieving significant improvements in recognition accuracy compared to traditional statistical models. The research first explored the feasibility of using deep learning to develop an accurate ASR system for Shona. Second, it investigated the specific challenges involved in designing and implementing deep learning architectures for Shona speech recognition and proposed strategies to mitigate these challenges. Lastly, it compared the performance of the deep learning-based model with existing statistical models in terms of accuracy. The developed ASR system utilized a hybrid architecture consisting of a Convolutional Neural Network for acoustic modelling and a Long Short-Term Memory network for language modelling. To overcome the scarcity of data, data augmentation techniques and transfer learning were employed. Attention mechanisms were also incorporated to accommodate the tonal nature of Shona speech. The resulting ASR system achieved impressive results, with a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These metrics indicated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona. This study contributed to the advancement of ASR technology for under-resourced languages like Shona, ultimately fostering improved accessibility and communication for Shona speakers worldwide.
Submission history
From: Leslie Wellington Sirora [view email][v1] Mon, 28 Jul 2025 20:57:26 UTC (496 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Do Large Language Models Understand Morality Across Cultures?
View PDF HTML (experimental)Abstract:Recent advancements in large language models (LLMs) have established them as powerful tools across numerous domains. However, persistent concerns about embedded biases, such as gender, racial, and cultural biases arising from their training data, raise significant questions about the ethical use and societal consequences of these technologies. This study investigates the extent to which LLMs capture cross-cultural differences and similarities in moral perspectives. Specifically, we examine whether LLM outputs align with patterns observed in international survey data on moral attitudes. To this end, we employ three complementary methods: (1) comparing variances in moral scores produced by models versus those reported in surveys, (2) conducting cluster alignment analyses to assess correspondence between country groupings derived from LLM outputs and survey data, and (3) directly probing models with comparative prompts using systematically chosen token pairs. Our results reveal that current LLMs often fail to reproduce the full spectrum of cross-cultural moral variation, tending to compress differences and exhibit low alignment with empirical survey patterns. These findings highlight a pressing need for more robust approaches to mitigate biases and improve cultural representativeness in LLMs. We conclude by discussing the implications for the responsible development and global deployment of LLMs, emphasizing fairness and ethical alignment.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Can human clinical rationales improve the performance and explainability of clinical text classification models?
View PDF HTML (experimental)Abstract:AI-driven clinical text classification is vital for explainable automated retrieval of population-level health information. This work investigates whether human-based clinical rationales can serve as additional supervision to improve both performance and explainability of transformer-based models that automatically encode clinical documents. We analyzed 99,125 human-based clinical rationales that provide plausible explanations for primary cancer site diagnoses, using them as additional training samples alongside 128,649 electronic pathology reports to evaluate transformer-based models for extracting primary cancer sites. We also investigated sufficiency as a way to measure rationale quality for pre-selecting rationales. Our results showed that clinical rationales as additional training data can improve model performance in high-resource scenarios but produce inconsistent behavior when resources are limited. Using sufficiency as an automatic metric to preselect rationales also leads to inconsistent results. Importantly, models trained on rationales were consistently outperformed by models trained on additional reports instead. This suggests that clinical rationales don't consistently improve model performance and are outperformed by simply using more reports. Therefore, if the goal is optimizing accuracy, annotation efforts should focus on labeling more reports rather than creating rationales. However, if explainability is the priority, training models on rationale-supplemented data may help them better identify rationale-like features. We conclude that using clinical rationales as additional training data results in smaller performance improvements and only slightly better explainability (measured as average token-level rationale coverage) compared to training on additional reports.
Submission history
From: Drahomira Herrmannova [view email][v1] Mon, 28 Jul 2025 19:44:25 UTC (980 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 28 Jul 2025]
Title:LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems
View PDF HTML (experimental)Abstract:Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 28 Jul 2025]
Title:CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting
View PDF HTML (experimental)Abstract:Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.
Submission history
From: David Maria Schmidt [view email][v1] Mon, 28 Jul 2025 18:20:41 UTC (224 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach
View PDFAbstract:In the current digital landscape, misinformation circulates rapidly, shaping public perception and causing societal divisions. It is difficult to identify hyperpartisan news in Bangla since there aren't many sophisticated natural language processing methods available for this low-resource language. Without effective detection methods, biased content can spread unchecked, posing serious risks to informed discourse. To address this gap, our research fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model, designed to enhance classification accuracy for hyperpartisan news. We evaluate its performance against traditional machine learning models and implement semi-supervised learning to enhance predictions further. Not only that, we use LIME to provide transparent explanations of the model's decision-making process, which helps to build trust in its outcomes. With a remarkable accuracy score of 95.65%, Bangla BERT outperforms conventional approaches, according to our trial data. The findings of this study demonstrate the usefulness of transformer models even in environments with limited resources, which opens the door to further improvements in this area.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability
View PDFAbstract:In recent years, social media platforms have become prominent spaces for individuals to express their opinions on ongoing events, including criminal incidents. As a result, public sentiment can shift dynamically over time. This study investigates the evolving public perception of crime-related news by classifying user-generated comments into three categories: positive, negative, and neutral. A newly curated dataset comprising 28,528 Bangla-language social media comments was developed for this purpose. We propose a transformer-based model utilizing the XLM-RoBERTa Base architecture, which achieves a classification accuracy of 97%, outperforming existing state-of-the-art methods in Bangla sentiment analysis. To enhance model interpretability, explainable AI technique is employed to identify the most influential features driving sentiment classification. The results underscore the effectiveness of transformer-based models in processing low-resource languages such as Bengali and demonstrate their potential to extract actionable insights that can support public policy formulation and crime prevention strategies.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation
View PDF HTML (experimental)Abstract:Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Memorization in Fine-Tuned Large Language Models
View PDF HTML (experimental)Abstract:This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.
Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.
Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.
These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 28 Jul 2025]
Title:LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning
View PDF HTML (experimental)Abstract:Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by "Thinking, Fast and Slow," which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different "subregions" of an LLM's parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Quantitative Finance > Portfolio Management
[Submitted on 28 Jul 2025]
Title:Your AI, Not Your View: The Bias of LLMs in Investment Analysis
View PDF HTML (experimental)Abstract:In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a model's embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models' latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.
Current browse context:
q-fin.PM
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models
View PDF HTML (experimental)Abstract:Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 28 Jul 2025]
Title:Dissecting Persona-Driven Reasoning in Language Models via Activation Patching
View PDF HTML (experimental)Abstract:Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]
Title:FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models
View PDFAbstract:Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at this https URL.
Submission history
From: Kuan-Wei Huang [view email][v1] Mon, 28 Jul 2025 15:41:53 UTC (79 KB)
[v2] Wed, 30 Jul 2025 17:19:41 UTC (79 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models
View PDF HTML (experimental)Abstract:Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
Submission history
From: Adrian Jaques Böck MSc [view email][v1] Mon, 28 Jul 2025 15:30:17 UTC (41 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation
View PDF HTML (experimental)Abstract:This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025 (v1), last revised 29 Jul 2025 (this version, v2)]
Title:Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning
View PDF HTML (experimental)Abstract:In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks by conditioning on input-output examples in the prompt, without requiring any update in model parameters. While widely adopted, it remains unclear whether prompting with multiple examples is the most effective and efficient way to convey task information. In this work, we propose Soft Injection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts and repeatedly used during inference. Soft injection is performed by softly mixing task embeddings with attention head activations using pre-optimized mixing parameters, referred to as soft head-selection parameters. This method not only allows a desired task to be performed without in-prompt demonstrations but also significantly outperforms existing ICL approaches while reducing memory usage and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs, spanning four model families of sizes from 4B to 70B. Averaged across 57 tasks, our method outperforms 10-shot ICL by 10.2%-14.3% across 12 LLMs. Additional analyses show that our method also serves as an insightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant head positions selected by our method transfer across similar tasks but not across dissimilar ones -- underscoring the task-specific nature of head functionality. Our soft injection method opens a new paradigm for reducing prompt length and improving task performance by shifting task conditioning from the prompt space to the activation space.
Submission history
From: Jungwon Park [view email][v1] Mon, 28 Jul 2025 14:59:17 UTC (758 KB)
[v2] Tue, 29 Jul 2025 02:15:56 UTC (758 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 28 Jul 2025]
Title:$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement
View PDF HTML (experimental)Abstract:Img2LaTeX is a practically significant task that involves converting mathematical expressions or tabular data from images into LaTeX code. In recent years, vision-language models (VLMs) have demonstrated strong performance across a variety of visual understanding tasks, owing to their generalization capabilities. While some studies have explored the use of VLMs for the Img2LaTeX task, their performance often falls short of expectations. Empirically, VLMs sometimes struggle with fine-grained visual elements, leading to inaccurate LaTeX predictions. To address this challenge, we propose $A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement, a framework that effectively integrates attention localization and iterative refinement within a visual reasoning framework, enabling VLMs to perform self-correction and progressively improve prediction quality. For effective evaluation, we introduce a new dataset, Img2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging examples designed to rigorously evaluate the capabilities of VLMs within this task domain. Extensive experimental results demonstrate that: (1) $A^2R^2$ significantly improves model performance across six evaluation metrics spanning both textual and visual levels, consistently outperforming other baseline methods; (2) Increasing the number of inference rounds yields notable performance gains, underscoring the potential of $A^2R^2$ in test-time scaling scenarios; (3) Ablation studies and human evaluations validate the practical effectiveness of our approach, as well as the strong synergy among its core components during inference.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Software Engineering
[Submitted on 28 Jul 2025]
Title:Enhancing Project-Specific Code Completion by Inferring Internal API Information
View PDF HTML (experimental)Abstract:Project-specific code completion is a critical task that leverages context from a project to generate accurate code. State-of-the-art methods use retrieval-augmented generation (RAG) with large language models (LLMs) and project information for code completion. However, they often struggle to incorporate internal API information, which is crucial for accuracy, especially when APIs are not explicitly imported in the file.
To address this, we propose a method to infer internal API information without relying on imports. Our method extends the representation of APIs by constructing usage examples and semantic descriptions, building a knowledge base for LLMs to generate relevant completions. We also introduce ProjBench, a benchmark that avoids leaked imports and consists of large-scale real-world projects.
Experiments on ProjBench and CrossCodeEval show that our approach significantly outperforms existing methods, improving code exact match by 22.72% and identifier exact match by 18.31%. Additionally, integrating our method with existing baselines boosts code match by 47.80% and identifier match by 35.55%.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computer Vision and Pattern Recognition
[Submitted on 28 Jul 2025 (v1), last revised 29 Jul 2025 (this version, v2)]
Title:The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?
View PDF HTML (experimental)Abstract:Non-manual facial features play a crucial role in sign language communication, yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regionseyes, mouth, and full faceusing two different deep learning models (a CNN-based model and a transformer-based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR.
Submission history
From: Dinh Nam Pham [view email][v1] Mon, 28 Jul 2025 14:36:46 UTC (4,828 KB)
[v2] Tue, 29 Jul 2025 18:27:34 UTC (4,828 KB)
Current browse context:
cs.CV
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings
View PDF HTML (experimental)Abstract:Medical reports contain rich clinical information but are often unstructured and written in domain-specific language, posing challenges for information extraction. While proprietary large language models (LLMs) have shown promise in clinical natural language processing, their lack of transparency and data privacy concerns limit their utility in healthcare. This study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which includes 28 clinical information extraction tasks in Dutch. We developed \texttt{llm\_extractinator}, a publicly available framework for information extraction using open-source generative LLMs, and used it to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results, while the bigger Llama-3.3-70B model achieved slightly higher performance at greater computational cost. Translation to English prior to inference consistently degraded performance, highlighting the need of native-language processing. These findings demonstrate that open-source LLMs, when used with our framework, offer effective, scalable, and privacy-conscious solutions for clinical information extraction in low-resource settings.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:A survey of diversity quantification in natural language processing: The why, what, where and how
View PDF HTML (experimental)Abstract:The concept of diversity has received increased consideration in Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting and inclusion, approximating human linguistic behavior, and increasing systems' performance. Diversity has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6 years, with "diversity" or "diverse" in their title. We find a wide range of settings in which diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized approach. We believe that this study paves the way towards a better formalization of diversity in NLP, which should bring a better understanding of this notion and a better comparability between various approaches.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Latent Inter-User Difference Modeling for LLM Personalization
View PDF HTML (experimental)Abstract:Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models
View PDFAbstract:Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales, flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously been constrained by the manual effort required to identify and code relevant cases. In 2025, the Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports ($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely on manual curation and coding. We evaluated whether a fully automated, open source "text-to-table" language-model pipeline (PFD Toolkit) could reproduce the ONS's identification and thematic analysis of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports published from July 2013 to November 2023 were processed via PFD Toolkit's large language model pipelines. Automated screening identified cases where the coroner attributed death to suicide in individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports - almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based workflow showed substantial to almost-perfect agreement (Cohen's $\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming a process that previously took months into one that can be completed in minutes. This demonstrates that automated LLM analysis can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable, reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available for future research.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey
View PDFAbstract:Text embeddings have attracted growing interest due to their effectiveness across a wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering, bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich, transferable representations. The general architecture of GPTE typically leverages PLMs to derive dense text representations, which are then optimized through contrastive learning on large-scale pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the roles PLMs play in driving its development. We first examine the fundamental architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity enhancement, training strategies, learning objectives, and data construction. Then, we describe advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code understanding, and scenario-specific adaptation. Finally, we highlight potential future research directions that move beyond traditional improvement goals, including ranking integration, safety considerations, bias mitigation, structural information incorporation, and the cognitive extension of embeddings. This survey aims to serve as a valuable reference for both newcomers and established researchers seeking to understand the current state and future potential of GPTE.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Multilingual Self-Taught Faithfulness Evaluators
View PDFAbstract:The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents Self-Taught Evaluators for Multilingual Faithfulness, a framework that learns exclusively from synthetic multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study
View PDFAbstract:While Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities, their substantial computational and memory requirements pose significant barriers to practical deployment. Current parameter reduction techniques primarily involve training MLLMs from Small Language Models (SLMs), but these methods offer limited flexibility and remain computationally intensive. To address this gap, we propose to directly compress existing MLLMs through structural pruning combined with efficient recovery training. Specifically, we investigate two structural pruning paradigms--layerwise and widthwise pruning--applied to the language model backbone of MLLMs, alongside supervised finetuning and knowledge distillation. Additionally, we assess the feasibility of conducting recovery training with only a small fraction of the available data. Our results show that widthwise pruning generally maintains better performance in low-resource scenarios with limited computational resources or insufficient finetuning data. As for the recovery training, finetuning only the multimodal projector is sufficient at small compression levels (< 20%). Furthermore, a combination of supervised finetuning and hidden-state distillation yields optimal recovery across various pruning levels. Notably, effective recovery can be achieved with as little as 5% of the original training data, while retaining over 95% of the original performance. Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming to compress MLLMs effectively without extensive computation resources or sufficient data.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models
View PDFAbstract:The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification
View PDFAbstract:The rapid spread of multilingual misinformation requires robust automated fact verification systems capable of handling fine-grained veracity assessments across diverse languages. While large language models have shown remarkable capabilities across many NLP tasks, their effectiveness for multilingual claim verification with nuanced classification schemes remains understudied. We conduct a comprehensive evaluation of five state-of-the-art language models on the X-Fact dataset, which spans 25 languages with seven distinct veracity categories. Our experiments compare small language models (encoder-based XLM-R and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo) using both prompting and fine-tuning approaches. Surprisingly, we find that XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B parameters), achieving 57.7% macro-F1 compared to the best LLM performance of 16.9%. This represents a 15.8% improvement over the previous state-of-the-art (41.9%), establishing new performance benchmarks for multilingual fact verification. Our analysis reveals problematic patterns in LLM behavior, including systematic difficulties in leveraging evidence and pronounced biases toward frequent categories in imbalanced data settings. These findings suggest that for fine-grained multilingual fact verification, smaller specialized models may be more effective than general-purpose large models, with important implications for practical deployment of fact-checking systems.
Submission history
From: Hanna Shcharbakova [view email][v1] Mon, 28 Jul 2025 10:49:04 UTC (1,921 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Geometric-Mean Policy Optimization
View PDF HTML (experimental)Abstract:Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Ontology-Enhanced Knowledge Graph Completion using Large Language Models
View PDFAbstract:Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior
View PDFAbstract:Antisocial behavior (ASB) on social media-including hate speech, harassment, and trolling-poses growing challenges for platform safety and societal wellbeing. While prior work has primarily focused on detecting harmful content after it appears, predictive approaches aim to forecast future harmful behaviors-such as hate speech propagation, conversation derailment, or user recidivism-before they fully unfold. Despite increasing interest, the field remains fragmented, lacking a unified taxonomy or clear synthesis of existing methods. This paper presents a systematic review of over 49 studies on ASB prediction, offering a structured taxonomy of five core task types: early harm detection, harm emergence prediction, harm propagation prediction, behavioral risk prediction, and proactive moderation support. We analyze how these tasks differ by temporal framing, prediction granularity, and operational goals. In addition, we examine trends in modeling techniques-from classical machine learning to pre-trained language models-and assess the influence of dataset characteristics on task feasibility and generalization. Our review highlights methodological challenges, such as dataset scarcity, temporal drift, and limited benchmarks, while outlining emerging research directions including multilingual modeling, cross-platform generalization, and human-in-the-loop systems. By organizing the field around a coherent framework, this survey aims to guide future work toward more robust and socially responsible ASB prediction.
Submission history
From: Anais ollagnier [view email] [via CCSD proxy][v1] Mon, 28 Jul 2025 08:27:58 UTC (1,059 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning
View PDFAbstract:We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning. Our zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble similarity scores from CLIP, SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the visual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position on the private test set, demonstrating the effectiveness of combining foundation models through ensembling and prompting. Our code is available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Enhancing Hallucination Detection via Future Context
View PDFAbstract:Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 28 Jul 2025]
Title:Kimi K2: Open Agentic Intelligence
View PDF HTML (experimental)Abstract:We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations
View PDFAbstract:We introduce a novel multi-labeled scheme for joint annotation of hate and counter-hate speech in social media conversations, categorizing hate and counter-hate messages into thematic and rhetorical dimensions. The thematic categories outline different discursive aspects of each type of speech, while the rhetorical dimension captures how hate and counter messages are communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a sample of 92 conversations, consisting of 720 tweets, and conduct statistical analyses, incorporating public metrics, to explore patterns of interaction between the thematic and rhetorical dimensions within and between hate and counter-hate speech. Our findings provide insights into the spread of hate messages on social media, the strategies used to counter them, and their potential impact on online behavior.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025 (v1), last revised 29 Jul 2025 (this version, v2)]
Title:SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers
View PDFAbstract:The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via a new \textbf{Difficulty Hiking} step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by \textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: \href{this https URL}{this https URL}
Submission history
From: Prakamya Mishra [view email][v1] Mon, 28 Jul 2025 05:17:48 UTC (3,394 KB)
[v2] Tue, 29 Jul 2025 17:02:27 UTC (3,395 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 28 Jul 2025]
Title:Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition
View PDFAbstract:Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.
Current browse context:
cs.AI
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:AQUA: A Large Language Model for Aquaculture & Fisheries
View PDFAbstract:Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
Submission history
From: Praneeth Narisetty [view email][v1] Mon, 28 Jul 2025 05:06:07 UTC (689 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Machine Learning
[Submitted on 28 Jul 2025]
Title:Customize Multi-modal RAI Guardrails with Precedent-based predictions
View PDFAbstract:A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model's judgment on "precedents", which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
Current browse context:
cs.LG
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
IArxiv Recommender
(What is IArxiv?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 28 Jul 2025]
Title:Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems
View PDF HTML (experimental)Abstract:Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.
To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.
Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:CodeNER: Code Prompting for Named Entity Recognition
View PDF HTML (experimental)Abstract:Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?
View PDFAbstract:Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.
Submission history
From: Khloud Al Jallad [view email][v1] Sun, 27 Jul 2025 21:30:50 UTC (1,146 KB)
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning
View PDF HTML (experimental)Abstract:Multilingual vision-language models have made significant strides in image captioning, yet they still lag behind their English counterparts due to limited multilingual training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG) offers a promising alternative by conditioning caption generation on retrieved examples in the target language, reducing the need for extensive multilingual training. However, multilingual RAG captioning models often depend on retrieved captions translated from English, which can introduce mismatches and linguistic biases relative to the source language. We introduce CONCAP, a multilingual image captioning model that integrates retrieved captions with image-specific concepts, enhancing the contextualization of the input image and grounding the captioning process across different languages. Experiments on the XM3600 dataset indicate that CONCAP enables strong performance on low- and mid-resource languages, with highly reduced data requirements. Our findings highlight the effectiveness of concept-aware retrieval augmentation in bridging multilingual performance gaps.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations
View PDF HTML (experimental)Abstract:Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Length Representations in Large Language Models
View PDF HTML (experimental)Abstract:Large language models (LLMs) have shown remarkable capabilities across various tasks, that are learned from massive amounts of text-based data. Although LLMs can control output sequence length, particularly in instruction-based settings, the internal mechanisms behind this control have been unexplored yet. In this study, we provide empirical evidence on how output sequence length information is encoded within the internal representations in LLMs. In particular, our findings show that multi-head attention mechanisms are critical in determining output sequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units within the model, we can control the output sequence length without losing the informativeness of the generated text, thereby indicating that length information is partially disentangled from semantic information. Moreover, some hidden units become increasingly active as prompts become more length-specific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that LLMs have learned robust and adaptable internal mechanisms for controlling output length without any external control.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing
View PDF HTML (experimental)Abstract:Recent advancements in Large Language Models (LLMs) have shown outstanding potential for role-playing applications. Evaluating these capabilities is becoming crucial yet remains challenging. Existing benchmarks mostly adopt a \textbf{character-centric} approach, simplify user-character interactions to isolated Q&A tasks, and fail to reflect real-world applications. To address this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric} bilingual role-playing benchmark featuring 80 diverse characters and over 8,000 dialogue rounds. RMTBench includes custom characters with detailed backgrounds and abstract characters defined by simple traits, enabling evaluation across various user scenarios. Our benchmark constructs dialogues based on explicit user motivations rather than character descriptions, ensuring alignment with practical user applications. Furthermore, we construct an authentic multi-turn dialogue simulation mechanism. With carefully selected evaluation dimensions and LLM-based scoring, this mechanism captures the complex intention of conversations between the user and the character. By shifting focus from character background to user intention fulfillment, RMTBench bridges the gap between academic evaluation and practical deployment requirements, offering a more effective framework for assessing role-playing capabilities in LLMs. All code and datasets will be released soon.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns
View PDF HTML (experimental)Abstract:We present DYNARTmo, a dynamic articulatory model designed to visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon the UK-DYNAMO framework and integrates principles of articulatory underspecification, segmental and gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous and six discrete control parameters, allowing for the generation of both vocalic and consonantal articulatory configurations. The current implementation is embedded in a web-based application (SpeechArticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for use in phonetics education and speech therapy. While this paper focuses on the static modeling aspects, future work will address dynamic movement generation and integration with articulatory-acoustic modules.
Submission history
From: Bernd-Joachim Kröger [view email][v1] Sun, 27 Jul 2025 16:19:46 UTC (5,070 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computers and Society
[Submitted on 27 Jul 2025]
Title:ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios
View PDFAbstract:The emergence of Large Language Models (LLMs) presents transformative opportunities for education, generating numerous novel application scenarios. However, significant challenges remain: evaluation metrics vary substantially across different educational scenarios, while many emerging scenarios lack appropriate assessment metrics. Current benchmarks predominantly measure general intelligence rather than pedagogical capabilities. To address this gap, we introduce ELMES, an open-source automated evaluation framework specifically designed for assessing LLMs in educational settings. ELMES features a modular architecture that enables researchers to create dynamic, multi-agent dialogues through simple configuration files, facilitating flexible scenario design without requiring extensive programming expertise. The framework incorporates a hybrid evaluation engine that objectively quantifies traditionally subjective pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic benchmarking of state-of-the-art LLMs across four critical educational scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching, Interdisciplinary Lesson Plan Generation, and Contextualized Question Generation, employing fine-grained metrics developed in collaboration with education specialists. Our results demonstrate distinct capability distributions among models, revealing context-specific strengths and limitations. ELMES provides educators and researchers with an accessible evaluation framework that significantly reduces adaptation barriers for diverse educational applications while advancing the practical implementation of LLMs in pedagogy. The framework is publicly available at \emph{this https URL}.
Current browse context:
cs.CY
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation
View PDF HTML (experimental)Abstract:Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday communication in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies.
Submission history
From: Abdullah Alabdullah [view email][v1] Sun, 27 Jul 2025 14:37:53 UTC (1,168 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Artificial Intelligence
[Submitted on 27 Jul 2025]
Title:SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration
View PDF HTML (experimental)Abstract:Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations
View PDF HTML (experimental)Abstract:Large language models (LLMs) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only LLM trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron overlap analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes LLM internals and inform future cross-lingual transfer research.
Submission history
From: Katharina Trinley [view email][v1] Sun, 27 Jul 2025 13:53:45 UTC (12,000 KB)
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning
View PDF HTML (experimental)Abstract:Large language models (LLMs) face significant challenges in effectively leveraging sequential environmental feedback (EF) signals, such as natural language evaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either convert EF into scalar rewards, losing rich contextual information, or employ refinement datasets, failing to exploit the multi-step and discrete nature of EF interactions. To address these limitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into LLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses) continual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and general language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-training to distill sequential EF interactions into single-step inferences. This synergy enables robust feedback-independent reasoning without relying on external feedback loops. Experimental results on mathematical reasoning (MATH-500, AIME24/AIME25) and code generation (CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art performance with the Qwen3-8B model, while maintaining strong generalization across model scales (Qwen3-4B). This work provides a promising approach for leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities in diverse domains.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms
View PDF HTML (experimental)Abstract:Shaping inclusive representations that embrace diversity and ensure fair participation and reflections of values is at the core of many conversation-based models. However, many existing methods rely on surface inclusion using mention of user demographics or behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a step back and recognized that equitable inclusion needs to account for the implicit expression of opinion and use the stance of responses to validate the normative alignment. This study aims to evaluate how opinions are represented in NLP or computational models by introducing an alignment evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the normative social views and discourse. Our approach models the stance of responses as a proxy for the underlying opinion, enabling a considerate and reflective representation of diverse social viewpoints. We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through this, we provide a lens on how implicit opinions are (mis)represented and offer a pathway toward more inclusive model behavior.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Post-Completion Learning for Language Models
View PDF HTML (experimental)Abstract:Current language model training paradigms typically terminate learning upon reaching the end-of-sequence (}) token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.
To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.
Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Modeling Professionalism in Expert Questioning through Linguistic Differentiation
View PDF HTML (experimental)Abstract:Professionalism is a crucial yet underexplored dimension of expert communication, particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model (LLM)-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers
View PDF HTML (experimental)Abstract:Transformers have profoundly influenced AI research, but explaining their decisions remains challenging -- even for relatively simpler tasks such as classification -- which hinders trust and safe deployment in real-world applications. Although activation-based attribution methods effectively explain transformer-based text classification models, our findings reveal that these methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. To address this limitation, we propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features. By contrasting the activations of an input sequence with reference activations, Contrast-CAT generates clearer and more faithful attribution maps. Experimental results across various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art methods. Notably, under the MoRF setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds over the most competing methods, demonstrating its effectiveness in enhancing interpretability for transformer-based text classification.
Current browse context:
cs.CL
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models
View PDF HTML (experimental)Abstract:Recent progress in large language models (LLMs) has opened new possibilities for mental health support, yet current approaches lack realism in simulating specialized psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps individuals transform problematic life stories into empowering alternatives, remains underutilized due to limited access and social stigma. We address these limitations through a comprehensive framework with two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative therapists by planning therapeutic stages, guiding reflection levels, and generating contextually appropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-centric evaluation method that quantifies effectiveness by tracking "Innovative Moments" (IMs), critical narrative shifts in client speech signaling therapy progress. Experimental results on 260 simulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-quality support conversations to facilitate social applications.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation
View PDF HTML (experimental)Abstract:News recommendation systems play a vital role in mitigating information overload by delivering personalized news content. A central challenge is to effectively model both multi-view news representations and the dynamic nature of user interests, which often span both short- and long-term preferences. Existing methods typically rely on single-view features of news articles (e.g., titles or categories) or fail to comprehensively capture user preferences across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user representations. Our model also incorporates BERT-based word embeddings to enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Experimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness of combining multi-view news representations with dual-scale user modeling. The implementation of our model is publicly available at this https URL.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

Computer Science > Computation and Language
[Submitted on 27 Jul 2025]
Title:IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs
View PDF HTML (experimental)Abstract:Current evaluations of large language models (LLMs) rely on benchmark scores, but it is difficult to interpret what these individual scores reveal about a model's overall skills. Specifically, as a community we lack understanding of how tasks relate to one another, what they measure in common, how they differ, or which ones are redundant. As a result, models are often assessed via a single score averaged across benchmarks, an approach that fails to capture the models' wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses factor analysis to identify latent skills driving performance across benchmarks. We apply this method to a comprehensive new leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a small set of latent skills that largely explain performance. Finally, we turn these insights into practical tools that identify redundant tasks, aid in model selection, and profile models along each latent skill.
References & Citations
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Connected Papers (What is Connected Papers?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
alphaXiv (What is alphaXiv?)
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
DagsHub (What is DagsHub?)
Gotit.pub (What is GotitPub?)
Hugging Face (What is Huggingface?)
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Recommenders and Search Tools
Influence Flower (What are Influence Flowers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.

--- Page 1 ---
2507.23726v1 [cs.AI] 31 Jul 2025

arXiv

ht! ByteDance | Seed

Seed-Prover: Deep and Broad Reasoning for
Automated Theorem Proving

ByteDance Seed Al4Math

Abstract

LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement
learning with long chain-of-thought, yet they continue to struggle with theorem proving due
to the lack of clear supervision signals when solely using natural language. Dedicated domain-
specific languages like Lean provide clear supervision via formal verification of proofs, enabling
effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-
style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean
feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design
three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover
proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on
PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the
lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry,
which outperforms previous formal geometry engines. We use these two systems to participate in
IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in
automated mathematical reasoning, demonstrating the effectiveness of formal verification with
long chain-of-thought reasoning.

Project Page: https: //github.com/ByteDance-Seed/Seed-Prover

Seed-Prover

100 -
Goedel-Prover-V2 Delta-Proyfr
Kimina-Prover 7@
90 - <
DeepSeek-ProverV2
= DSP+—@
S
= 80 - Kimina-Prover-Preview
i HunyuanProver _,/BFS-Prover
uw 70 - InternLV.5-StepProver
Me |
€ DeepSeek-Prover-VI.59?~ a1P
eepSeek-Prover-V1.

E 60 - P t L@an@bell-Prover
° InternLM2-StepProver ® Goedel-Prover
3% DeepSeek-Prover-V1
re 50 - ===—@ Lean-STaR
4 Hypertree Proof ee ©
o Search eee e InternLM2-Math-Plus

40 - Cursiciium Learning ABEL®

2
Proof Artifact__--- aad COPRA e Alchemy
Co-training” P4 TheoremMama
30 - e-- ReProver @ 30-Prover
e
 2025

Publication Date

Figure 1 Growth in MiniF2F-Test performance over time.

1


--- Page 2 ---
1 Introduction

Recent advances in large language models (LLMs) have shown extending reasoning lengths through natural
language can significantly boost performance on math benchmarks such as MATH and AIME [4, 11]. Training
such models requires reinforcement learning (RL) on verifiable answers. It is extremely difficult to automatically,
or even manually, verify a proof in natural language since each step must be carefully checked for correctness
[13]. This poses significant challenges for applying reinforcement learning to the training of large language
models to prove mathematical statements. Unlike natural language, formal languages such as Lean can provide
a clear and automatic signal on the correctness of a formalized proof. A noteworthy work from AlphaProof
[3] uses Lean to successfully solve 3 problems from the 2024 International Mathematical Olympaid (IMO).
AlphaProof demonstrates that LLMs using formal language are capable of proving very challenging problems
that LLMs using natural language fail to prove.

There are two types of LLM formal provers, step-level provers [3, 14, 22, 23, 25] and whole-proof generation
provers|5, 24]. Step-level provers incrementally generate Lean code line-by-line. While this enables close
interaction with the Lean environment, it requires special scaffolding to generate a complete Lean proof, and
the interaction is often too granular to allow high-level reasoning. In contrast, whole-proof models generate
an entire Lean proof at once, but typically lack interaction with the Lean compiler. Recent work has shown
that combining whole-proof models with long chain-of-thought reasoning [9, 15, 21] substantially outperforms
step-level provers. In this work, we propose Seed-Prover, a whole-proof model with following features:

e Lemma-Style Proving: Seed-Prover tries to generate useful intermediate lemmas before proving the
main statement. These lemmas serve as shared knowledge across different inference paths.

e Iterative Proof Refinement: Seed-Prover iteratively refines its proof based on Lean compiler feedback,
previous proved lemmas, and self-summarization.

e Test-Time Scaling: We implement a three-tiered inference strategy that enables Seed-Prover to think
both deeply and broadly—allocating thinking budget to fine details while exploring interesting properties.

e SOTA Performance: Seed-Prover proves 5 out of 6 problems in IMO 2025, saturates MiniF2F [31]
(shown in Figure 1), and outperforms prior work by up to 3x on multiple formal benchmarks.

Due to the lack of sufficient geometry support in Lean, Seed-Prover incorporates a dedicated geometry reasoning
engine Seed-Geometry. Similar to existing line of efforts in AlphaGeometry [2, 19] and TongGeometry [30],
Seed-Geometry follows the forward-chaining design in the reasoning engine implementation, where the
system derives all known facts by checking applicable rules until closure is reached. By backward-tracing
fact dependencies, Seed-Geometry identifies the minimum dependency relations in a geometry problem’s
configuration, seperating the problem context from the auxiliary constructions necessary to prove a problem.
Using statistics derived from more than past 20 years of math olympiad competitions, Seed-Geometry performs
extensive search in the geometry space defined by its dedicated domain-specific language and establishes
a repository of 230 million unique geometry problems requiring auxiliaries. A Seed model trained on such
dedicated geometry data becomes an exceptionally effective neuro-symbolic geometry prover, where it fills in
the missing auxiliary geometry elements and the geometry reasoning engine performs step-by-step forward-
chaining, completing the final proof of a problem. In experiments, Seed-Geometry solves 43 of the IMO-AG-50
(us. 42 by AlphaGeometry 2), a benchmark that curates geometry problems of IMO from 2000 to 2024. It
also sets a new state-of-the-art on the IMO shortlist geometry problems from 2000 to 2022, and notably solves
the geometry problem of IMO 2025 under just 2 seconds.

2 Approach

Here we introduce the two systems we used in IMO 2025, Seed-Geometry and Seed-Prover.

2.1 Seed-Geometry

Seed-Geometry builds on the success of TongGeometry [30] and performs a major redesign. From a global
perspective, both systems leverage trained neural models to complete missing auxiliary constructions and

--- Page 3 ---
specialized reasoning engines to forward-chain derivation. However, Seed-Geometry is a substantial upgrade
over TongGeometry in the following aspects.

2.1.1 Extended Domain-Specific Language

Seed-Geometry constructs geometric diagrams in the principle of ruler-and-compass construction. However,
plain ruler-and-compass construction steps can be long and cumbersome, making the language representation of
the constructions overly verbose, introducing unnecessary burden on both the neural processing of Transformer-
based large language model and the symbolic derivation of the backend engine. To mitigate these issues,
Seed-Geometry groups particular action sequences into specific actions, making the representation of the
problem concise enough. Of particular note, Seed-Geometry has several such composite actions: isogonal
conjugate with respect to a triangle and a point, exsimilitude center of two circles, insimilitude center of two
circles. All three actions can be represented with primitive ruler-and-compass actions; yet the construction
sequence in itself is non-trivial and unnecessarily clumsy.

2.1.2 Extremely Fast Reasoning Engine

Seed-Geometry improved its reasoning engine’s performance by rewriting its backend in C++ and making it
accessible to Python users through Pybind11. This change led to roughly 100-fold speed increase compared to
the Python implementation in TongGeometry. The C++ implementation handles memory more efficiently
and benefits from compiler optimizations, allowing for much faster deep searches within the reasoning engine.
This is particularly crucial because the engine’s forward-chaining design typically slows down considerably
when the search tree expands widely at deeper levels.

2.1.3 Exceptional Large Language Model

Seed-Geometry utilizes a high-performing large language model from the Seed family [17]. This particular
Seed model has undergone extensive pre-training on vast datasets of coding and mathematics, granting it a
wide array of specialized skills. The specific model size was chosen considering the number of data tokens. We
considered training two models in an actor-—critic setup initially: the policy model that proposed possible
next auxiliary element to construct and a value model that predicted the number of steps to go from the
state. However, we note from preliminary experiments that a single Seed model serving as the policy would
suffice, contradicting the design of both a policy model and a value model in existing work [30]. We also note
that a policy model unspecialized to the specific goal makes both training and solving more manageable and
therefore, we only trained the model on pairs of problem context and auxiliaries, without the target fact goal
in the prompt.

2.1.4 Extensive Search

When presented with a new problem, Seed-Geometry first transforms the representation into a canonical
form. If the reasoning backend successfully finds the goal fact to prove in the reasoning process, the problem
is considered immediately solved. Otherwise, Seed-Geometry initiates a search process. In particular, Seed-
Geometry employs beam search, with the policy model generating proposals for each beam in the buffer.
With the extremely fast reasoning engine, Seed-Geometry supports running each new generated proposal in
time. If any one of the proposal leads to the proof, the problem is considered proven; otherwise we select
the top few proposals for the next step of expansion in the search tree based on each proposal’s cumulative
negative log likelihood. The search process terminates until a fixed number of steps have been consumed.
Seed-Geometry’s search process is made efficient and scalable. Compared to TongGeometry, Seed-Geometry’s
solving process supports a distributed setup where each GPU process communicates with each other and
blocked only at the beam selection point. Each GPU process is also equipped with a CPU thread pool that
asynchronously executes the reasoning step during language model inference such that the reasoning cost can
be overlapped with language model inference.

--- Page 4 ---
Whole proof Lemma-style

theorem eg : 1 + 2 = 3 := by lemma roundi_h@ : 1+ 1 = 2 := by
have h@ : 1+ 1 = 2 := by ring
ring
lemma round1_hi : 1 + (1 + 1) = 3 := by
have hi : 1 + (1 + 1) = 3 := by ring
ring
theorem eg : 1+ 2 = 3 := by
linarith [h@, h1] linarith [round1_h@, round1_h1]
#print axioms eg #print axioms eg

Figure 2 An example of whole proof and lemma-style proof in Lean 4.

2.2 Seed-Prover

Seed-Prover is a large language model specialized in formal reasoning in Lean 4. Its most significant distinction
from prior work lies in its adoption of lemma-style proving as the proof paradigm, which places lemmas at
the center of the reasoning process. This approach offers several key advantages: it enables clear identification
of lemmas that have and have not been proved, indicating the progress made in solving the main problem;
lemmas can be processed independently and combined freely; lemmas from different inference trajectories can
be combined to address more challenging problems. Both the training and inference procedures of Seed-Prover
are designed around lemmas.

2.2.1 Lemma-Style Proving

Previous works [9, 15, 21] trained the model to generate whole proofs starting with the keyword theorem.
In contrast, we first require the model to generate some useful leommas—each introduced by the keyword
lemma—before generating the main proof using theorem by applying the generated lemmas. An example
is shown in Figure 2. This leomma-style proof provides following merits. First, it allows clear identification
of the lemmas that have been successfully proved, and those that need further refinement. Second, lemmas
are modular—they can be compiled independently, stored independently, and combined freely. Additionally,
proofs of lemmas may provide inspiration to the model for proving unproved lemmas and the main problem.
To enable this workflow, we establish a lemma pool for each difficult problem, which stores comprehensive
data from all our inference runs, including lemma statements, lemma names, complete proofs, proof difficulties,
and dependency relations. The lemma pool is typically used to (1) retrieve the most relevant lemmas by name
or formal statement; (2) sample the most difficult lemmas according to their proof difficulty.

2.2.2 Conjecture Proposing

When tackling challenging contest-level math problems, human contestants often identify interesting properties
of the problem and use them to guide their reasoning. Seed-Prover is trained to propose such potentially useful
properties by chain-of-thought reasoning. Notably, this process differs from chain-of-thought reasoning to
solve the problem directly; rather, it emphasizes broad exploration of the problem space without committing
to a particular approach. Take functional equations as an example: one might conjecture that the function is
injective, surjective, bijective, monotonic, or periodic—all without engaging in deep, targeted reasoning about
the problem itself. In a sense, many useful properties of a problem may be enumerated before full problem
resolution. The proposer module accepts an unsolved problem and, optionally, some already proved lemmas
as input, and generates 10-50 candidate conjectures about properties of the problem. Proposing multiple
conjectures in parallel significantly increases diversity and the likelihood of covering the valuable properties.
For each problem, we may repeat this process multiple times to create a large conjecture pool.

This approach differs from Draft, Sketch, Prove [7], which presumes the ability to fully solve the problem
upfront. In contrast, our method performs broad exploratory searches over the problem space enabling

--- Page 5 ---
Problem

NL Whole Proof Whole Proof Fail lemma 1 Light Inference

Lean Compiler X Lean Compiler X Fail lemma 2 Light Inference
Lemma 1 7 Y.
Light Inference

Lemma 2 Whole Proof Problem with
New Context

Lean Compiler X Fail lemma 4 Light Inference
Lemma .. Whole Proof - ; a!)
——+ Light Inference
Lean Compiler

Problem with Inner Refinement

Lean Compiler *
Lemma 3 p Whole Proof Fail lemma 2 Light Inference

Lemma n
New Context

/ Whole Proof Whole Proof Outer Refinement
Lean Compiler & Lean Compiler © Lean Compiler &

Whole Proof Light Medium
Generation Inference Setting Inference Setting

Figure 3 The workflows of single-pass whole proof generation, light, and medium inference settings.

discovery of useful properties, which makes it possible to solve problems that the model cannot solve directly
in natural language.

2.2.3. Training

To enable seamless interaction between Seed-Prover and Lean, we adopt multi-stage, multi-task reinforcement
learning (RL) based on VAPO [29]. The RL reward is 1 if the formal statement is successfully proven, and 0
otherwise. Additionally, a formatting penalty is applied to encourage the model to generate lemmas before
attempting the main theorem. As training progresses, problem diffculty, problem quality, and maximum
output length are progressively increased. The training dataset comprises a combination of open-source
datasets [1, 8, 12, 27, 28] and in-house formalized problems. For problems that are too difficult for single-pass
generation, we use our proposer to generate easier problem variants and put these into the training dataset.
We also exclude problems which are too easy (i.e. proof rate above 1/4) from RL training.

Unlike prior work [15, 21] that only utilize formal statements as prompts for RL training, our approach
randomly incorporates natural language hints, natural language proofs, similar lemmas, proved lemmas, failed
lemmas, failed attempts, summaries of previous attempts, and Lean compiler feedback into the prompt. This
diverse prompting strategy enhances the model’s adaptability within our inference pipeline by enabling it to
understand and utilize various types of input.

2.2.4 Test-Time Scaling

Here, we introduce our approach to test-time scaling of Seed-Prover. Depending on available inference budgets
and problem difficulties, we developed three levels of strategies, which are illustrated in Figure 3 and Figure 4.

Light Previous work evaluated LLM theorem provers by Pass@k. We find that iteratively refining proofs
using Lean compiler feedback [32] in conjunction with self-summarization can surpass the limits of single-pass
inference token budgets and improve proof ability significantly. In the light setting, each proof attempt is
refined up to 8-16 times and evaluated under Pass@8-16. We denote the sample budget of Pass@n and up
to m refinements as n x m, so the sample budget of the light setting is equivalent to generating the whole
proof at Pass@64—256. This setting completes in 1-2 hours. Under this setting, Seed-Prover proves IMO 2022
P2 (MOHS = 15'), whereas without refinement, the same problem can only be proved in Pass@8192. We

1MOHS is Math Olympiad Hardness Scale: https: //web.evanchen.cc/upload/MOHS-hardness.pdf. We note that the hardness
scale for human contestants may not be well-aligned with the difficulty of proving it in Lean using an LLM.

--- Page 6 ---
Problem Heavy Inference Setting

Proposer

Conjecture 0 Conjecture 1 Conjecture 2 Conjecture 3 Conjecture .. Conjecture n

a or ‘ ’
Light Inference Light Inference Light Inference Light Inference Light Inference Light Inference

Select based on proof difficulty

Problem Lemma i Lemma j Lemma k

Medium Inference

Proposer

Conjecture 10 Conjecture 11 Conjecture 12 Conjecture 13 Conjecture .. Conjecture 1n

+ +, Y
Light Inference Light Inference Light Inference Light Inference Light Inference Light Inference

Select based on proof difficulty

Problem Lemma 1i Lemma 1j Lemma 1k

Medium Inference

Figure 4 The workflow of heavy inference setting.

observed two dominant behaviors of Seed-Prover in the light inference setting. First, it fixes Lean syntax
errors in response to Lean compiler feedback. Second, it refines initial proof sketches, a process that might
entirely alter the reasoning trajectory.

Medium Proofs for challenging competition problems are often lengthy and structurally complex. The
medium test-time setting introduces both outer and inner refinement processes. The outer refinement process
mirrors the light setting which refines the proof of the original main problem. The inner refinement process
targets difficult lemmas that the outer refinement process generates but fails to prove, using a light setting
with an 8 x 8 budget to handle finer details. If any inner refinement successfully proves at least one lemma
(indicating meaningful progress), the outer refinement process will update this information into the prompt
and continue its refinement. Under this setting, Seed-Prover solves harder problems like IMO 2003 P6
(MOHS = 35), IMO 2020 P5 (MOHS = 20), IMO 2024 P1 (MOHS = 5) and IMO 2025 P5 (MOHS = 15),
and final proofs potentially exceeding 1000 lines of code.

Heavy While the medium setting encourages reasoning in depth over proof details, it lacks the breadth
needed to explore diverse properties of the given problem. Under the heavy inference setting, Seed-Prover
begins with a conjecture pool and an empty lemma pool for the given problem. Initially, the proposer generates
thousands of conjectures (by default 5000) to populate the conjecture pool. During inference, Seed-Prover
tries to prove or disprove every conjecture in the conjecture pool using the light setting. Successfully proved
conjectures are moved into the lemma pool. Seed-Prover leverages the lemma pool and Lean compiler feedback
to refine the proof attempts. Additional conjectures are proposed based on proved lemmas. After days of
thinking, the lemma pool accumulates several thousand nontrivial math facts. Each lemma is scored based on
its proof rate, semantic relevance, and proof length. The proof rate serves as a strong indicator of lemma value;
empirically, lemmas with low proof rate are often crucial to the final proof. Semantic relevance is assessed by
an LLM judge. Unrelated lemmas or lemmas with short proof lengths are removed. We take hundreds of
the top-ranked lemmas to help Seed-Prover finish the proof of the main problem using the medium inference
setting. Seed-Prover has been trained to select and integrate these lemmas into a complete proof during RL.
IMO 2017 P2 (MOHS = 40), IMO 2025 P3 (MOHS' = 25) and IMO 2025 P4 (MOHS = 15) are proved under
the heavy inference setting.

3. Evaluation

--- Page 7 ---
3.1 Seed-Geometry

Using Seed-Geometry’s backend, we performed large-scale problem generation. In particular, we took a similar
approach to TongGeometry [30] by collecting data statistics of geometry problems over more than the past 20
years, and ran the problem generation program on the trees. In more than 7 days, the problem generation
program found over 230 million unique problems, reaching 8x the search efficiency compared to the Python
implementation. After necessary preprocessing using a pretrained byte-pair encoding [18], the dataset totaled
38B tokens. We trained both a policy model to complete the auxiliary objects given the context and a value
model to estimate the number of steps remaining under the current state. The policy model was initialized
from a pretrained Seed model and the value model was initialized with the trained policy model. However, in
the experiments, we found that under extensive search, the value model could harm the general performance
due to large errors in value estimation. We also compared generating auxiliary actions step-by-step with
beam search and generating the whole auxiliary sequence in one go. Results from the latter were significantly
inferior to the former. Therefore, in the final evaluation, we used step-by-step generation with beam search in
a distributed setup, with each GPU process hosting a policy model for proposal generation.

Table 1 lists the performance of Seed-Geometry in IMO geometry problems from 2000 to 2024. Using the
accounting method in IMO-AG-50, Seed-Geometry achieves 43 problem solves compared to AlphaGeometry 2,
reaching 1 more solution. The problems that Seed-Geometry cannot solve but AlphaGeometry 2 does are
not proof-based problems but rather computation-based problems, which AlphaGeometry 2 could potentially
address using its algebraic engine. Table 2 lists the performance of our model in the much harder IMO
shortlist problems from 2000 to 2022. The original benchmark of IMOSL-AG-30 was claimed to be established
from the problems in the benchmark, but omitted many of the hard problems. Here, we include all hard-level
problems in the shortlist and present the full results. As shown in the tables, AlphaGeometry 2 solved 19 of
the 39 problems and our Seed-Geometry solved 22 of them. In addition, Seed-Geometry solved IMO 2025 P2
in 2 seconds after it received the human-provided problem formulation.

Based on the results provided, Seed-Geometry has established a new state-of-the-art in automated geometry
problem solving, surpassing the performance of its predecessor, AlphaGeometry 2. In summary, by solving
more problems in both the standard IMO and the more difficult IMO shortlist benchmarks, Seed-Geometry
demonstrates a superior capability in handling complex, proof-based geometry challenges, setting a new
performance standard in the field.

Table 1 Performance comparison of AlphaGeometry 2 (AG2) and Seed-Geometry (SG) in IMO geometry problems
from 2000 to 2024. Note that in IMO-AG-50, the 2002 P2, 2003 P4, 2004 P5, 2008 P1, and 2009 P4 are separated into
two, whereas we merge each of them into one.

ID AG2 SG ID AG2 SG ID AG2 SG
2000 P1 Vv 2007 P4 Vv 2015 P4 Vv
2000 P6 Vv 2008 P1 Vv 2016 Pl Vv
2001 Pl Xx 2008 P6 Vv 2017 P4 Vv
2001 P5 Vv Xx 2009 P2 Vv 2018 Pl Vv
2002 P2 Vv 2009 P4 Vv Xx 2018 P6 X Vv
2002 P6 Xx 2010 P2 Vv 2019 P2 Vv
2003 P3 Xx 2010 P4 Vv 2019 P6 Vv
2003 P4 Vv 2011 P6 Vv 2020 Pl Vv
2004 P1 Vv 2012 Pl Vv 2020 P6 X
2004 P5 Vv 2012 P5 Vv 2021 P3 Vv
2005 P1 Vv 2013 P3 Vv 2021 P4 Vv
2005 P5 Vv 2013 P4 Vv 2022 P4 Vv
2006 P1 Xx Vv 2014 P3 Vv 2023 P2 Vv
2006 P6 Xx 2014 P4 Vv 2023 P6 X Vv
2007 P2 Vv 2015 P3 Vv 2024 P4 Vv

--- Page 8 ---
Table 2 Performance comparison of AlphaGeometry 2 (AG2) and Seed-Geometry (SG) in IMO shortlist geometry
problems from 2000 to 2022. Note that in IMOSL-AG-30, many geometry problems are ignored and here we fill in
those missing geometry problems for comparison, with 2016 G7 merged into one.

ID AG2 SG ID AG2 SG ID AG2 SG
2002 G7 Vv 2011 G3 X Xx 2016 G6 Vv
2002 G8 Vv 2011 G4 X Vv 2016 G7 Vv
2003 G5 Vv 2011 G5 X Vv 2016 G8 X
2004 G7 Xx Vv 2011 G6 Vv Xx 2017 G7 X
2004 G8 Xx Vv 2011 G7 Vv Xx 2017 G8 X
2005 G5 Vv 2012 G6 X Vv 2018 G7 Vv
2005 G6 Xx Vv 2012 G7 X Xx 2019 G6 Vv
2006 G9 Vv 2012 G8 X Vv 2019 G8 X
2007 G8 Xx 2014 G7 Vv 2020 G8 Vv
2009 G6 Vv 2015 G5 Vv 2021 G8 X
2009 G7 Vv Xx 2015 G7 X Xx 2022 G6 X
2009 G8 Vv Xx 2015 G8 X Xx 2022 G7 X
2010 G5 Vv 2016 G5 Vv 2022 G8 X

Table 3 Performance comparison of Seed-Prover against previous systems across formal math tasks. The performance
on PutnamBench is using the number of proved statements instead of percentage following previous works [20].

Metric Seed-Prover Previous SOTA

IMO 2025 4/6 (Heavy, 5/6 post-competition) 5/6 (Natural language, Gemini)
Past IMO 78.1% (Heavy) —

MiniF2F-valid 100.0% (Medium!) 90.6% (DeepSeek-Prover-V2 [15])
MiniF2F-test 99.6% (Medium?) 92.2% (Kimina-Prover [21])
PutnamBench 331/657 (Medium) 86/657 (Goedel-Prover-V2 [9])
CombiBench 30.0% (Medium) 10.0% (Deepseek-Prover-V2 [15])
MiniCTX-v2 81.8% (Light) 44.3% (o4-mini [16])

1One problem used Heavy. ?One problem failed under Heavy.

3.2 Seed-Prover

To evaluate the performance of Seed-Prover, we tested it on IMO 20257, past IMO problems, MiniF2F [31],
PutnamBench [20], CombiBench [10], and MiniCTX-v2 [6] covering a wide range of mathematical domains.
For PutnamBench and CombiBench, we first evaluate using the light setting and use the medium setting
for unsolved problems. For IMO problems and MiniF2F, we also use the heavy setting for the remaining
unsolved problems. The results are shown in Table 3. Unless otherwise specified, we use Lean v4.14.0 with its
corresponding Mathlib version.

IMO 2025 During the IMO 2025 contest, all problems were translated into formal statements by human
experts. For fill-in-the-blank problems (“determine x such that ...”), initial solution candidates were generated
by Seed1.6-Thinking before translation. We conducted searches for IMO 2025 Problems 1, 3, 4, and 5 using
both the medium and heavy inference settings in parallel. As required by the IMO committee, all proof
submissions were due by July 18th. Seed-Geometry solved Problem 2 instantly, and Seed-Prover derived the
proof for Problem 5 under the medium inference setting, while proofs for the other three problems required
the heavy inference setting. Notably, the proof for Problem 1 was finished after the deadline.

?ByteDance was officially invited to participate in IMO 2025.

--- Page 9 ---
Past IMO_ To evaluate our system’s performance on past IMO problems, we curated a dataset consisting
of 155 past IMO problems. Most problems were adapted from Compfiles® and MiniF2F [31]. Additionally,
a subset of problems was manually added or corrected by human experts. For problems prior to 2017, we
used light and medium settings. For problems after 2017, we use the heavy inference setting if the medium
inference setting failed. Seed-Prover successfully proves 121/155 problems, achieving an overall success rate of
78.1%. By difficulty, Seed-Prover proves 47/55 easy problems (P1 or P4), 47/56 medium problems (P2 or
P5), and 27/44 hard problems (P3 or P6). By subject area, it proves 72/85 algebra problems, 42/55 number
theory problems, and 7/14 combinatorics problems. This demonstrates that the Seed-Prover’s performance at
IMO 2025 reflects consistent capability on IMO problems across all years.

MiniF 2F [31] Under the medium setting, we prove 99.6% problems on both MiniF2F-valid and MiniF2F-test.
We used the heavy inference setting to tackle the last problem in both splits (ie. IMO 1990 P3 and IMOSL
2007 Algebra P6). Seed-Prover successfully proved IMO 1990 P3 and failed on IMOSL 2007 Algebra P6.
Interestingly, among the most challenging problems solved in MiniF2F are ones such as AMC12A 2021 P12,
AMC12A 2021 P25, and AMC12A 2020 P9, which are relatively straightforward to reason about in natural
language, but pose significant challenges when formalized in Lean. This difficulty arises primarily from
obstacles in applying Vieta’s formulas or the non-triviality of counting roots.

PutnamBench [20] We evaluated Seed-Prover on PutnamBench using the light and medium inference settings.
Under the light inference setting only, Seed-Prover proved 201/657 problems from PutnamBench. Using the
medium inference setting improved this performance to 331/657 problems. This result shows a significant
performance jump compared to previous works on undergraduate math problems.

CombiBench [10] Previous benchmarks have primarily focused on number theory and algebra problems.
CombiBench is a benchmark specifically centered on combinatorial problems, where the problems often involve
newly-defined concepts. Here, we evaluate Seed-Prover on CombiBench using the medium inference setting.
Our model proves 30 out of 100 problems from CombiBench, outperforming previous work. Nonetheless,
relative to other benchmarks, our model still struggles with proving combinatorics problems.

MiniCTX-v2_ [6] To test our system on a broader range of mathematical subjects from real-world formalization
projects—including the ability to understand new definitions, notations, and lemmas—we evaluated Seed-
Prover on MiniCTX-v2. This dataset includes context-rich problems from formalization repositories in subjects
such as axiomatic systems, high-energy physics, analysis, and research-level number theory, all of which were
written after Nov. 2024 to prevent data contamination. For evaluation purposes, we used the light inference
setting under Lean v4.16.0. Our system successfully achieved 81.8% of MiniCTX-v2, which demonstrates
its strong potential in real-world automated theorem proving, generalizing beyond standalone competition
problems. For comparison, the baseline 04-mini solved 44.3% statements at Pass@8 [16].

4 Conclusion

In this work, we presented Seed-Geometry and Seed-Prover—two formal reasoning frameworks that integrate
the capabilities of large language models. Both systems substantially outperform previous formal reasoning
frameworks. Seed-Geometry accelerates verification and scales the search mechanism. Seed-Prover leverages
iterative refinement and a three-tiered test-time inference strategy to achieve state-of-the-art. Notably, we
successfully proved 5 out of 6 problems in IMO 2025, demonstrating the efficacy of these formal systems.
Formal languages like Lean offer rapid proof verification, making them far more cost-effective than human
experts and more reliable than LLM judges. Our future work will focus on combining formal systems with
large language models to tackle open conjectures.

Shttps://github.com/dwrensha/compfiles

--- Page 10 ---
References

[1]

[2|

[3]

[4]

[5]

[6]

[7|

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17|

[18]

Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase
Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: A large-scale, high-quality math dataset for
reinforcement learning in language models, 2025. URL https: //arxiv.org/abs/2502. 17387.

Yuri Chervonyi, Trieu H Trinh, Miroslav Ol84k, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk
Jung, Vikas Verma, Quoc V Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry
with alphageometry2. arXiv preprint arXiv:2502.03544, 2025.

DeepMind. Ai solves imo problems at silver medal level. https://deepmind.google/discover/blog/
ai-solves-imo-problems-at-silver-medal-level/, 2024.

DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL
https: //arxiv.org/abs/2501.12948.

Kefan Dong and Tengyu Ma. Stp: Self-play llm theorem provers with iterative conjecturing and proving, 2025.
URL https: //arxiv.org/abs/2502.00212.

Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX: Neural theorem proving with (long-)contexts. In The
Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?
id=KI gaAqEFHW.

Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik,
Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem provers with informal
proofs. In The Eleventh International Conference on Learning Representations, 2023.

Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul,
Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and
Stanislas Polu. Numinamath. [https://github.com/project-numina/aimo-progress-prize] (https://github.
com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024.

Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen,
Sanjeev Arora, et al. Goedel-prover: A frontier model for open-source automated theorem proving. arXiv preprint
arXiv:2502.07640, 2025.

Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang,
Yunzhou Xie, Beibei Xiong, et al. Combibench: Benchmarking Ilm capability for combinatorial mathematics.
arXiv preprint arXiv:2505.03171, 2025.

OpenAI. Openai ol system card, 2024. URL https: //arxiv.org/abs/2412.16720.

Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang,
Zhouliang Yu, Luming Li, et al. Criticlean: Critic-guided reinforcement learning for mathematical formalization.
arXiv preprint arXiv:2507.06181, 2025.

Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic¢,
Nikola Jovanovié, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025. URL
https: //arxiv.org/abs/2503.21934.

Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal
mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344, 2022.

ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao
Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning
for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025.

Tate Rowney, Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX leaderboard. https: //cmu-13.github.io/
minictx/leaderboard.html, 2025. Accessed July 28, 2025.

ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang,
Xiangpeng Wei, Wenyuan Xu, et al. Seed1.5-thinking: Advancing superb reasoning models with reinforcement
learning. arXiv preprint arXiv:2504.13914, 2025.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword
units. arXiv preprint arXiv:1508.07909, 2015.

10

--- Page 11 ---
[19]

[20]

[21]

[22|

[23]

[24]

[25]

[26]

[27|

[28]

[29]

[30]

[31]

[32|

Trieu H Trinh, Yuhuai Wu, Quoc V Le, He, and Thang Luong. Solving olympiad geometry without human
demonstrations. Nature, 625(7995):476—482, 2024.

George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and
Swarat Chaudhuri. Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition.
In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track.

Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina
Vinyes, Zhenzhe Ying, Zekai Zhu, Jiangiao Lu, Hugues de Saxcé, Bolton Bailey, Chendong Song, Chenjun Xiao,
Dehao Zhang, Ebony Zhang, Frederick Pu, Han Zhu, Jiawei Liu, Jonas Bayer, Julien Michel, Longhui Yu, Léo
Dreyfus-Schmidt, Lewis Tunstall, Luigi Pagani, Moreira Machado, Pauline Bourigault, Ran Wang, Stanislas
Polu, Thibaut Barroyer, Wen-Ding Li, Yazhe Niu, Yann Fleureau, Yangyang Hu, Zhouliang Yu, Zihan Wang,
Zhilin Yang, Zhengying Liu, and Jia Li. Kimina-prover preview: Towards large formal reasoning models with
reinforcement learning. 2025. URL http://arxiv.org/abs/2504. 11354.

Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2.5-
stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems, 2024. URL
https: //arxiv.org/abs/2410.15700.

Zijian Wu, Jiayu Wang, Dahua Lin, and Kai Chen. Lean-github: Compiling github lean repositories for a versatile
lean prover, 2024. URL https: //arxiv.org/abs/2407 .17227.

Huajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang,
Xuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo,
and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and
monte-carlo tree search. In The Thirteenth International Conference on Learning Representations, 2025. URL
https: //openreview.net/forum?id=14YAIwrskXa.

Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen.
Bfs-prover: Scalable best-first tree search for Ilm-based automatic theorem proving, 2025. URL https: //arxiv.
org/abs/2502.03438.

Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger,
and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances
in Neural Information Processing Systems, 36:21573-21612, 2023.

Huaiyuan Ying, Zijian Wu, Yihan Geng, Jlayu Wang, Dahua Lin, and Kai Chen. Lean workbook: A large-scale
lean problem set formalized from natural language math problems. In The Thirty-eight Conference on Neural
Information Processing Systems Datasets and Benchmarks Track.

Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong
Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint
arXiv:2503.14476, 2025.

Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian
Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin,
Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui
Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL
https: //arxiv.org/abs/2504.05118.

Chi Zhang, Jiajun Song, Siyu Li, Yitao Liang, Yuxi Ma, Wei Wang, Yixin Zhu, and Song-Chun Zhu. Proposing
and solving olympiad geometry with guided tree search. arXiv preprint arXiv:2412.10673, 2024.

Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: a cross-system benchmark for formal olympiad-level
mathematics. In International Conference on Learning Representations.

Yichi Zhou, Jianqiu Zhao, Yongxin Zhang, Bohan Wang, Siran Wang, Luoxin Chen, Jiahui Wang, Haowei Chen,
Allan Jie, Xinbo Zhang, et al. Solving formal math problems by decomposition and iterative reflection. arXiv
preprint arXiv:2507.15225, 2025.

11

--- Page 12 ---
Appendix

A Contributors

The names are sorted alphabetically. An asterisk * indicates a member who left Seed.

Algorithm Luoxin Chen, Liankai Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li,
Wenlei Shi, Jiahui Wang, Siran Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Huajian Xin, Fan Yang, Hongyi
Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang*, Yichi Zhou, Thomas Hanwen Zhu

Data Jinming Gu, Wenhao Huang, Zhicheng Jiang, Xiaoran Jin, Kaijing Ma, Jiawei Shen, Tong Sun, Chenrui
Wei, Shufa Wei, Yuchen Wu, Yihang Xia, Huaiyuan Ying*, Zheng Yuan, Ge Zhang

Infra Cheng Ren, He Sun, Zhihong Wang, Tianyun Zhao*, Jiangiu Zhao, Thomas Hanwen Zhu

B LooKeng: An Easy-to-Use and Effective Python Interface for Lean

Interacting with Lean poses significant challenges that limit the flexibility of Lean-based workflows. The most
popular interface, LeanDojo [26], only supports earlier versions of Lean 4, restricting users from accessing
Lean’s newest updates. Furthermore, LeanDojo requires creating a Lean repository for interaction, which
makes it impractical to use considering the massive scale of Lean interaction during model development
and inference. To address these issues, we introduce LooKeng, a REPL*-based Python interface designed
to simplify and accelerate the interaction process. LooKeng offers powerful features for developers while
providing a user-friendly interface for end-users. The core functionality of LooKeng includes ‘init state’,
‘run_ tac’, and ‘verify_ proof’. One can use LooKeng to interact with Lean step-by-step or verify an entire
proof directly. The key features of LooKeng are summarized as follows:

e Stateless Design: A Lean state can be simultaneously processed using different LooKeng instances,
enabling effortless scaling and sharing.

e Complex Tactics: Complex tactics such as apply? and all_goals are fully supported, with enhanced
infotree integration to prevent false positive proofs.

e Version-Free: The LooKeng CLI allows users to manage and switch between different Lean versions.

e Memory Control: Users can easily track the memory consumption of the Lean backend, set custom
thresholds, and automatically terminate processes when memory usage exceeds the limit.

e Proof Verification: LooKeng provides a straightforward method, ‘verify_ proof’, to rigorously verify the
final proof using the native Lean interface, ensuring correctness and reliability.

e Proof Simplification: LooKeng can remove useless tactics and hypothesis in the proof to obtain a simpler
proof.

e Statement Negation: LooKeng is able to generate the negated statement of a statement.

e Multi-Concurrency Support: LooKeng can run as a service, handling thousands of concurrent requests
via async architecture and resource isolation.

“https: //github.com /leanprover-community /repl

12

--- Page 1 ---
2507.23740v1 [cs.CL] 31 Jul 2025

arXiv

Rule2Text: Natural Language Explanation of
Logical Rules in Knowledge Graphs

Nasim Shirvani-Mahdavyi!0009—0006— 2733-2242] Devin Wingfield, Amin Ghasemi,
and Chengkai L_jl0000—0002—1724—8278]

University of Texas at Arlington, 701 S Nedderman Dr, Arlington, TX, 76019, USA
{nasim.shirvanimahdavi2,dtw8917 ,mxg6185}@mavs.uta.edu, 

Abstract. Knowledge graphs (KGs) often contain sufficient information
to support the inference of new facts. Identifying logical rules not only
improves the completeness of a knowledge graph but also enables the de-
tection of potential errors, reveals subtle data patterns, and enhances the
overall capacity for reasoning and interpretation. However, the complex-
ity of such rules, combined with the unique labeling conventions of each
KG, can make them difficult for humans to understand. In this paper,
we explore the potential of large language models to generate natural
language explanations for logical rules. Specifically, we extract logical
rules using the AMIE 3.5.1 rule discovery algorithm from the bench-
mark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV
and FB+CVT-REV. We examine various prompting strategies, includ-
ing zero- and few-shot prompting, including variable entity types, and
chain-of-thought reasoning. We conduct a comprehensive human eval-
uation of the generated explanations based on correctness, clarity, and
hallucination, and also assess the use of large language models as auto-
matic judges. Our results demonstrate promising performance in terms
of explanation correctness and clarity, although several challenges remain
for future research. All scripts and data used in this study are publicly

available at https: //github.com/idirlab/KGRule2NL

Keywords: Knowledge graphs, Logical rules, Natural language expla-
nation, Large language models, Interpretability

1 Introduction

Knowledge graphs (KGs) encode factual information as triples of the form (sub-
ject s, predicate p, object o). They are integral to a wide range of artificial
intelligence tasks and applications [9/11]. Although large-scale KGs (e.g., Free-
base [3] and Wikidata [I9]) contain a vast number of triples, they are often
incomplete, which adversely affects their usefulness in downstream applications.
However, KGs often hold sufficient information to infer new facts |/7/17|. For
example, if a KG indicates that a certain woman is the mother of a child, it is
quite likely that her husband is the child’s father. Identifying such rules can help
infer highly probable missing facts which can be further verified by human data
workers or experts. In addition to enhancing the completeness of KGs, such rules
can also aid in detecting potential errors, deepening our understanding of the

--- Page 2 ---
2 N. Shirvani-Mahdavi et al.

data’s inherent patterns, and facilitating reasoning and interpretability [13/7].
Rule learning systems, such as AMIE [8]2] and AnyBURL [12], derive Horn
rules for symbolic reasoning and link prediction in KGs. These rules can serve as
explanations for specific predictions; for instance, such rules can assist domain
scientists in uncovering underlying missing relationships within their data.
However, rules are often challenging to comprehend for humans, espe-
cially for non-experts. The difficulty arises from the abstract logical struc-
ture and the complexity of the rules; the number of logical components, re-
ferred to as atoms, as well as the nuanced nature of entity and relation la-
bels within each KG. For instance, as explained in [18], label of predicates in
the Freebase dataset follow the format /[domain]/|type]/[label] (e.g., /ameri-
can_football/player_rushing_statistics/team). Without proper background knowl-
edge about such differences in KG labels, evaluating logical rules can become

cumbersome.
One way to address this challenge is by providing natural language explana-

tions for logical rules, which enhance accessibility and usability, aid KG man-
agement in cross-disciplinary contexts, and improve transparency for researchers
and practitioners. Pre-defined templates can generate such explanations, but this
approach is not scalable, as it is impractical to manually extract all logical rules
from a large KG and define templates for each. To handle unseen rules, solutions
leveraging large language models (LLMs) are promising due to LLMs’ genera-
tive abilities and generalization capability. Related work has focused on natural
language generation from logical forms [21J5], natural language generation from
KGs [16], encoding and translating natural rules [6[1], and rule-based reasoning

with LLMs [15}22).
To the best of our knowledge, this is the first work to examine the effec-

tiveness of LLMs in generating natural language explanations for logical rules.
We mined the rules by the AMIE 3.5.1 algorithm, the latest version released in
2024, using the widely used cross-domain benchmark dataset FB15k-237 [4] and
two properly preprocessed large-scale variants of the Freebase dataset, FB-CVT-
REV and FB+CVT-REV (Section|2). We investigated a range of prompting
strategies, such as zero and few-shot prompting [10], incorporating an instance
of the rule, including variable entity types and Chain-of-Thought (CoT) rea-
soning [20] (Section [3). To evaluate the quality of the generated explanations,
we conducted detailed human evaluations based on criteria such as correctness,
clarity, and hallucination. Additionally, we explored the potential of LLM-as-a-
judge for this task (Section (4p. Our findings indicate that combining CoT
prompting with variable type information yields the most accurate and readable
explanations. Overall, our findings highlight a promising direction for this task.
We conclude the work and outline potential avenues for future research in Sec-
tion |5}| All the scripts and data produced from this work are available from our

GitHub repository at https: //github.com/idirlab/KGRule2NL
2 Rule Selection from Knowledge Graphs

2.1 Rule Mining Algorithm
We employed AMIE 3.5.1, a well-established rule learning system in its latest
version, due to its comprehensive metrics for rule evaluation as well as its proven

--- Page 3 ---
Rule2Text: NL Explanation of Logical Rules in KGs 3

compatibility with our chosen benchmark datasets. In AMIE, a rule has a body
(antecedent) and a head (consequent), represented as B; A By A...\ B, > H
, or in simplified form B = H. The body consists of multiple atoms Bj, ...,
B,, and the head H itself is also an atom. In an atom r(h,t), which is another
representation of a triple (h, r, t), the subject and/or the object are variables
to be instantiated. The prediction of the head can be carried out when all the
body atoms can be instantiated in the KG. In AMIE, the concept of support
quantifies the amount of evidence (i.e., correct predictions) for each rule in the
data. It is defined as the number of distinct (subject, object) pairs in the head
of all valid instantiations of the rule in the KG. The concept of head coverage,
a proportional version of support, is the fraction of support over the number
of facts in relation r, where r is the relation in the head atom. The standard
confidence of a rule is the fraction of support over the number of instantiations
of the rule body. To mine the rules, we used the default settings of AMIE for
optimized performance, with minimum thresholds of 0.1 for head coverage and
standard confidence, and a maximum threshold of 3 for the number of atoms.
2.2 Datasets

For our experiments, we leveraged three datasets. FB15k-237, a small subset of
the Freebase dataset, was selected as it is a widely used benchmark for KG com-
pletion, recognized for avoiding the data leakage issues of FB15k [4]. Its multi-
domain coverage makes it well-suited for extracting logical rules with diverse
relations. FB-CVT-REV and FB+CVT-REV [18] datasets (Statistics shown in
Table[1} are large-scale variants of the Freebase dataset designed to eliminate the
data leakage issue previously identified in FB15k. FB+CVT-REV includes me-
diator entities (ie., Compound Value Type nodes) originally present in Freebase
to represent n-ary relations. In contrast, FB-CVT-REV converts n-ary relation-
ships centered on a CVT node into binary relations by concatenating the edges
that connect entities through the CVT node, a method also used in FB15k-237.
As shown in Table |1| the conversion process has resulted in a higher number
of rules in these two datasets compared to those in FB+CVT-REV. Including
these datasets facilitates the analysis of large-scale data and the effects of me-
diator nodes and concatenated relationships on the derived rules and generated
explanations.

The label of a concatenated relation is formed by merging the labels of two
underlying relations. As a result, the label becomes lengthy, taking the format
of domain1/typel/labell./domain2/type2/label2. Notably, the domains and even
types can sometimes be identical in concatenated labels, but labell and label2
are always distinct. This format differs from the simpler structure of standard
relations, which follow the format of domain/type/label. Thus, this added com-
plexity can pose a greater challenge for LLMs in generating natural language
explanations.

3 Methodology
3.1 Prompting Strategies

To generate natural language explanations for logical rules, we conducted our
experiments in three phases. All scripts and prompts, rules, generated explana-

--- Page 4 ---
4 N. Shirvani-Mahdavi et al.

tions, and annotated data are available at our GitHub repository. In all prompts
used in our experiments, we provided background knowledge to the models to en-
hance their understanding of the syntax and labels of the datasets. This content
includes the format of predicates in the datasets as mentioned in Section [2| This
background knowledge is particularly important because, in rules that involve
concatenated relations, the resulting lengthy labels with multiple components
can easily confuse the model.

Phase 1: Zero-Shot vs. Few-Shot Prompting In the first phase, we
compared zero-shot and few-shot prompting strategies using rules from a small
subset of the Freebase dataset, specifically FB15k-237. The objective was to
assess the impact of in-context examples on explanation quality and establish a
baseline. The few-shot prompt includes two (rule, explanation) pairs as examples.
In this phase and phase 2, we employed OpenAI’s GPT-3.5 Turbo model for its
balance of performance, efficiency, and cost-effectiveness. A total of 100 rules
with the highest head coverage were selected for human evaluation, covering a
broad range of domains, from music and media to medicine and space. To ensure
the quality of the annotations and mitigate potential subjectivity, we tasked
three individuals with annotating the data. For each rule, annotators were shown
both the rule and a concrete instantiation to aid understanding, along with two
generated explanations, one from zero-shot prompting and one from few-shot.
Their task was to identify which explanation better captured the semantics of
the rule. In cases of comparable semantic accuracy, the more naturally worded
explanation was preferred. After selecting the better explanation, annotators
rated it using the evaluation metrics described in Section As discussed in
Section|4] the few-shot prompting strategy did not yield significant improvements
over the zero-shot baseline.

Phase 2: Utilizing Variable Entity Types in The Prompt This
phase initially incorporated rule instantiations into the prompt design.
However, analysis of the generated explanations revealed persistent limita-
tions in the model’s ability to identify variable entity types, leading us
to adopt integration of these types in the prompt. For instance, in the
rule ?b /time/event/instance_of recurring event World Series => World Series
/sports/sports_championship/events 7b, World Series is a constant entity and 7b
is a variable entity. In Freebase datasets, entities can belong to multiple types.
Consequently, each variable entity is associated with a list of types. Given an
edge type and its edge instances, there is almost a function that maps from the
edge type to a type that all subjects in the edge instances belong to, and simi-
larly, almost such a function for objects[18]. For the example above, the variable
?b’s types are either /time/event or /sports/sports_championship_event. For this
phase, three annotators annotated 100 rules, rules with the highest head cov-
erage, 50 top rules from FB-CVT-REV, and 50 from FB+CVT-REV. Unlike
the previous phase, the annotators were asked to complete metric evaluations
for explanations from both prompts, the zero-shot prompt as our baseline, and
the prompt incorporating variable type. As discussed in Section [4] our findings
show that providing variable type information significantly improved the model’s
performance in generating accurate explanations.

--- Page 5 ---
Rule2Text: NL Explanation of Logical Rules in KGs 5

Phase 3: Comparing Models & Chain-of-Thought Prompting
Building on the strong impact of incorporating variable entity types into the
prompts, we further leveraged the reasoning capabilities enabled by CoT prompt-
ing. This prompt guides the model through five reasoning steps. First, it parses
the rule and identifies its components, including constant entities, variable en-
tities, and relations. Second, for each variable entity, it determines the most
contextually relevant type. Third, it interprets each atom in the rule using the
selected types. Fourth, it synthesizes the information to infer the rule’s overall
implication. Finally, it generates a concise natural language explanation. The
prompt also includes two illustrative examples with CoT reasoning to support
the model’s understanding. In this phase, we expanded our evaluation to include
two additional models, GPT-40 Mini and Gemini 2.0 Flash, alongside GPT-3.5
Turbo. These models were selected to provide a balanced comparison in terms of
performance, efficiency, and cost-effectiveness. Three annotators evaluated new
explanations, generated via CoT prompting by the three models, for the same
set of rules used in phase 2. As discussed in Section |4| GPT-3.5 Turbo shows
improved performance compared to phase 2, while Gemini 2.0 Flash achieves
the highest overall performance, followed by GPT-4o Mini.

3.2 Evaluation Metrics for Generated Explanations
To evaluate the generated explanations, we used the following metrics for human
and automatic evaluation.

Correctness: Evaluation of the explanation’s accuracy on a scale from 1 (com-
pletely incorrect) to 5 (fully correct). Correctness refers to the explanation’s in-
clusion of all components of the rule, presented in the exact logical order specified
by the rule.

Clarity: Evaluation of the explanation on a scale from 1 (very unclear) to
5 (very clear). Clarity refers to the ease with which the explanation can be
understood and how naturally it reads. This metric exclusively assesses the ex-
planation, independent of the correctness of the underlying rule.

# of missed entities: The number of entities present in the rule but not stated
in the explanation.

# of missed relations: The number of relations (i.e., predicates) present in
the rule but not stated in the explanation.

# of hallucinated entities: The number of entities absent from the rule but
incorrectly stated in the explanation.

# of hallucinated relations: The number of relations absent from the rule but
incorrectly stated in the explanation.

Rule logicalness: Although the meaningfulness of a rule is not directly related
to the generation of explanations, we asked the annotators to rate the rules on
a scale from 1 (not logically sound) to 2 (moderately logical), and 3 (logically
sound). This metric exclusively evaluates the rule itself, without considering the
explanation.

Perplexity: Given the absence of reference sentences for comparison with the
explanations, as our automatic evaluation metric, we computed perplexity using
GPT-2. While it is a useful measure of the model’s fluency and coherence, it is
not an indication of the correctness of the explanations.


--- Page 6 ---
6 N. Shirvani-Mahdavi et al.

4 Experiments & Results

Phase 1 The annotated data, available on our GitHub repository, represents
an aggregation of input from three annotators. For each rule, we select the ex-
planation receiving the majority vote and calculate the average of the measures
for that chosen explanation only. For instance, if annotators 1 and 2 selected
the explanation generated from the zero-shot prompt for a particular rule, while
the third annotator chose the explanation from the few-shot prompt, we only
averaged the measures provided by annotators 1 and 2 for that rule.

Table [2] presents the average of all measures for all annotated rules (denoted
as all), as well as for explanations generated from zero and few-shot prompts
separately (denoted as prompt 1 and prompt 2, respectively). Specifically, for
rules where the explanation generated from prompt 1 was favored by the major-
ity of annotators, the table reports the average measures for those explanations.
The same approach is applied to rules where explanations from prompt 2 were
preferred. Additionally, we provide these measures for the subset of rules where
annotators unanimously selected the same explanation (denoted as unanimous),
comparing them to the remaining rules with majority voting (denoted as major-
ity). The measures # missed entities, # missed relations, # hallucinated entities,
# hallucinated relations, correctness, clarity, logicalness, and perplexity are de-
noted as m ent, m rel, h ent, h rel, correctness, clarity, logical, and perplexity in
Table [2| (and |3) respectively.

These results demonstrate the model, overall, generates relatively accurate
and clear explanations with low perplexity. Among the 100 sentences selected for
human annotation, 49 were assigned to explanation 2, derived from the few-shot
prompt, while the remaining sentences were assigned to explanation 1. Notably,
annotators reached unanimous agreement on 48% of the rules. Furthermore, the
number of missed or hallucinated elements is negligible. Our observations in-
dicate that most hallucinations stem from the labels of relations, particularly
concatenated relations. The model tends to generate additional entities or rela-
tions to explain the complex labels associated with concatenated relations.

Table 1. Statistics of Table 2. Evaluation results on the annotated data in

the datasets phase 1 _
dataset # of triples # of rules m ent! m rel” h ent! h rel‘|correctness' clarity’ logical" |perplexity’
FBI5k-237 310,116 6,320 all 0.10 0.04 0.29 0.10 4.36 4.67 2.36.14
FB-CVEREV | 195,124,274 14.355 «Prompt 1 | 0.14 0.05 0.25 0.07 4.40 4.69 2.29 37.85
FB+CVT-REV| 134.213,735 2,965 Prompt 2 | 0.06 0.03 0.34 0.12 4.32 464 2.44 34.36
unanimous} 0.13 0.03 0.35 0.12 4.34 4.68 2.29 33.80
majority 0.08 0.05 0.24 0.07 4.37 4.66 2.43 38.30

Phase 2 Table [3] presents the results for this phase, averaged across all
annotators. Explanation 2, generated using the variable type prompt, consis-
tently shows higher correctness and clarity across all categories, highlighting
the importance of type information for model comprehension. Both explanation
types have minimal missing entities and relations. However, Explanation 2 also
shows slightly higher hallucination rates and increased perplexity. Rules with
three atoms and those involving concatenated relations generally receive lower
correctness and clarity scores, likely due to their increased complexity. Inter-


--- Page 7 ---
Rule2Text: NL Explanation of Logical Rules in KGs 7

estingly, despite these lower scores, annotators rated the rules from these two
categories as more logically coherent.

Table 3. Evaluation results on the annotated data in phase 2

explanation from zero-shot prompt explanation from variable type prompt
logical"|m ent’ m rel’ h ent* h rel"|correct’ clarity’ |perplexity*|m ent’ m rel’ h ent! h rel'|correct’ clarity" |perplexity*
all 2.58 0.06 0.10 0.22 0.09 3.94 4.12 29.05 0.05 0.07 0.21 0.13 4.21 4.19 33.07
2 atoms 2.50 0.03 0.04 0.08 0.05 4.22 4.35 34.10 0.31 0.41 0.15 0.16 4.25 4.30 38.59
3 atoms 2.62 0.08 0.13 0.31 0.12 3.78 3.99 26.21 0.07 0.08 0.24 0.11 4.18 4.12 29.97
binary 2.59 0.08 0.10 0.18 0.08 4.04 4.22 31.02 0.06 0.03 0.20 0.11 4.32 4.28 34.11
mediator 2.51 0.08 0.13 0.16 0.06 4.15 4.13 24.22 0.01 0.11 0.16 0.06 4.36 4.2 5
concatenated| 2.60 0.02 0.08 0.35 0.15 3.63 3.91 27.63 0.05, 0.11 0.25 0.20 3.88, 3.99 33.33,

Phase 3. Given the negligible number of hallucinated and missing entities
and relations, we evaluated the explanations in phase 3 using only correctness,
clarity, and perplexity. Table [4] presents the results. Overall, the models exhibit
trends similar to those observed in Phase 2. For example, all models perform bet-
ter on shorter rules, particularly those with only two atoms, and achieve higher
performance on rules involving only binary relations compared to those with con-
catenated ones. GPT-3.5 Turbo shows improved performance with CoT prompt-
ing compared to its performance using only variable entities. This improvement
is consistent across all categories except for rules that include mediator nodes.
GPT-4o Mini is the second-best performing model and demonstrates relatively
strong performance on rules containing at least one concatenated relation. Gem-
ini 2.0 Flash demonstrates the best overall performance. Its explanations are
the most concise, though in rare instances, it includes remarks such as, “Note:
This rule is likely flawed.” Notably, the lowest clarity scores across all models
are observed for rules involving mediator nodes. Additionally, most models ex-
hibit their highest perplexity on rules with only two atoms, which is somewhat

unexpected given the simplicity of these rules.
Table 4. Evaluation results on the annotated data in phase 3

GPT-3.5 Turbo GPT-4o mini Gemini 2.0 Flash
correct’ clarity’ perplexity'|correct’ clarity’ |perplexity’|correct’ clarity’ |perplexity”
all 4.28 4.26 32.40 4.45 4.53 31.57 4.67 4.70 27.19
2 atoms 4.38 4.43 34.08 4.52 4.62 40.96 4.80 4.76 29.98
3 atoms 4.22 AAT 31.46 4.42 4.51 26.26 4.61 4.68 25.62
binary 4.40 4.42 34.58 4.50 4.58 33.52 4.70 4.71 27.77
mediator 4.13 4.07 26.26 4.24 4.49 26.82 4.69 4.63 26.92
concatenated] 4.10 4.07 31.57 4.50 4.51 30.38 4.63 4.75 26.19

LLM-as-a-Judge One of the limitations of this work is the absence of
ground truth data, which restricts our ability to fine-tune models effectively. A
potential solution lies in leveraging the LLM-as-a-judge approach. If a reliably
fair and consistent judge model can be designed, it becomes possible to use a
strong model, such as Gemini 2.0 Flash, to generate (rule, explanation) pairs.
The judge can then evaluate these pairs, and those receiving high scores can
be treated as pseudo-ground truth for fine-tuning smaller open-source models.
Additionally, low-scoring examples can be analyzed to identify patterns and
better understand the types of explanations or rules that pose challenges for
the model.

To explore this direction, we developed an LLM-as-a-judge prompt. LLM-
based judges often exhibit bias toward models from their family [14], for ex-
ample, GPT models tend to favor responses generated by other GPT models.


--- Page 8 ---
8 N. Shirvani-Mahdavi et al.

To account for this potential bias, we evaluated the performance of the two best
models, GPT-4o0 Mini and Gemini 2.0 Flash, using both GPT-4o0 Mini and Gem-
ini 2.0 Flash as judges. This resulted in a total of four evaluation settings for a
more balanced comparison. Since clarity can be a highly subjective metric, we
focused our analysis on correctness. The information provided to the LLM judges
was identical to that given to human annotators: the rule, an instance of the rule,
the list of variable types, and the explanation. Table [5] presents the correlation
between LLM judges and human annotators. Because annotator scores are aver-
aged across multiple individuals, they are represented as floating-point numbers,
whereas LLM judge scores are integers. To ensure a fair comparison, we rounded
the annotator scores to the nearest whole number before computing correlation
coefficients. We used Spearman correlation to measure rank-order agreement, as-
sessing how similarly the judges and annotators rank the explanations. Pearson
correlation was used to evaluate the strength of the linear relationship between
their actual scores. Both LLM judges show moderate agreement with annota-
tors on the correctness of explanations generated by GPT-40 Mini. Gemini 2.0
Flash also aligned reasonably well with annotators when evaluating its own out-
puts, whereas GPT-4o Mini showed weak agreement in that setting. Although
these results are not ideal, they point to a promising direction for future work
in leveraging LLMs for scalable evaluation and dataset generation.

Table 5. Correlation between LLM judges and annotators for correctness

Judge Explanation generated by|Spearman|Pearson
GPT-4o0 mini GPT-4o0 mini 0.528 0.595
Gemini 2.0 Flash}|GPT-4o0 mini 0.498 0.603
GPT-4o0 mini Gemini 2.0 Flash 0.221 0.208
Gemini 2.0 Flash|Gemini 2.0 Flash 0.429 0.527

5 Conclusion & Future Work

We employed three LLMs with multiple prompting strategies to generate natural
language explanations for logical rules extracted by the AMIE algorithm from
three datasets at varying scales. Human evaluation indicated encouraging results
regarding accuracy and clarity, although rule complexity presents challenges for
future research. Our findings indicate that the combination of Chain-of-Thought
prompting and variable type information yields the most accurate and readable
explanations. Future research can extend this work by evaluating LLM perfor-
mance on more complex rules beyond AMIE’s extraction capabilities, exploring
additional knowledge bases such as Wikidata, which encode facts differently, and
constructing reference explanations to fine-tune LLMs for improved generation
quality.

Acknowledgments. This material is based upon work supported by the National
Science Foundation under Grants TIP-2333834. We also extend our gratitude to the
Texas Advanced Computing Center (TACC) for providing computing resources for this
work’s experimentation.

References

1. Hisgy, K., Ozaki, A.: Rule Learning as Machine Translation using the Atomic
Knowledge Bank. arXiv:2311.02765 (2023)

--- Page 9 ---
10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

Rule2Text: NL Explanation of Logical Rules in KGs 9

. Betz, P., Galdrraga, L., Ott, S., Meilicke, C., Suchanek, F., Stuckenschmidt, H.:

PyClause-Simple and efficient rule handling for knowledge graphs. In: IJCAI. pp.
8610-8613 (2023)

Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J.: Freebase: a collabo-
ratively created graph database for structuring human knowledge. In: SIGMOD.
pp. 1247-1250 (2008)

Bordes, A., Usunier, N., Garcia-Duraén, A., Weston, J., Yakhnenko, O.: Translating
embeddings for modeling multi-relational data. In: NeurIPS. pp. 2787-2795 (2013)
Chen, Z., Chen, W., Zha, H., Zhou, X., Zhang, Y., Sundaresan, S., Wang,
W.Y.: Logic2Text: High-fidelity natural language generation from logical forms.
arXiv:2004.14579 (2020)

Clark, P., Tafjord, O., Richardson, K.: Transformers as soft reasoners over lan-
guage. arXiv preprint arXiv:2002.05867 (2020)

Galarraga, L., Teflioudi, C., Hose, K., Suchanek, F.M.: Fast Rule Mining in Onto-
logical Knowledge Bases with AMIE++. VLDB 24(6), 707-730 (Dec 2015)
Galdrraga, L.A., Teflioudi, C., Hose, K., Suchanek, F.: AMIE: association rule
mining under incomplete evidence in ontological knowledge bases. In: WWW. pp.
413-422 (2013)

Ji, S., Pan, S., Cambria, E., Marttinen, P., Philip, S.Y.: A survey on kgs: Repre-
sentation, acquisition, and applications. TNNLS 33(2), 494-514 (2021)

Kermani, A., Perez-Rosas, V., Metsis, V.: A systematic evaluation of Ilm strategies
for mental health text analysis: Fine-tuning vs. prompt engineering vs. rag. In: The
10th Workshop on CLPsych. p. 172 (2025)

Kiafar, B., Ravva, P.U., Joy, A.A., Daher, S., Barmaki, R.L.: Mena: Multi-
modal epistemic network analysis for visualizing competencies and emotions. arXiv
preprint arXiv:2504.02794 (2025)

Meilicke, C., Chekol, M.W., Betz, P., Fink, M., Stuckeschmidt, H.: Anytime
bottom-up rule learning for large-scale KGC. VLDB 33(1), 131-161 (2024)
Nakashole, N., Sozio, M., Suchanek, F.M., Theobald, M.: Query-time reasoning in
uncertain RDF KBs with soft and hard rules. VLDS 884(6), 15-20 (2012)
Panickssery, A., Bowman, S., Feng, $.: LLM evaluators recognize and favor their
own generations. NeurIPs 37, 68772-68802 (2024)

Servantez, S., Barrow, J., Hammond, K., Jain, R.: Chain of Logic: Rule-Based
Reasoning with Large Language Models. arXiv:2402.10400 (2024)

Shi, X., Zhu, Z., Zhang, Z., Li, C.: Hallucination mitigation in natural language gen-
eration from large-scale open-domain knowledge graphs. In: EMNLP. pp. 12506-—
12521 (2023)

Shirvani-Mahdavi, N., Akrami, F., Li, C.: On Large-scale Evaluation of Embedding
Models for Knowledge Graph Completion. arXiv:2504.08970 (2025)
Shirvani-Mahdavi, N., Akrami, F., Saeef, M.S., Shi, X., Li, C.: Comprehensive
analysis of Freebase and dataset creation for robust evaluation of knowledge graph
link prediction models. In: ISWC. pp. 113-133. Springer (2023)

Vrandeéi¢é, D., Krétzsch, M.: Wikidata: a free collaborative knowledge base. CACM
57(10), 78-85 (2014)

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,
D., et al.: Chain-of-thought prompting elicits reasoning in large language models.
NeurIPS 35, 24824-24837 (2022)

Wu, X., Cai, Y., Lian, Z., Leung, H.f., Wang, T.: Generating natural language from
logic expressions with structural representation. TASLP 31, 1499-1510 (2023)
Yang, Z., Du, X., Mao, R., Ni, J., Cambria, E.: Logical reasoning over natural
language as knowledge representation: A survey. arXiv:2303.12023 (2023)

--- Page 10 ---
10 N. Shirvani-Mahdavi et al.

23. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z.,
Li, D., Xing, E., et al.: Judging Ilm-as-a-judge with mt-bench and chatbot arena.
NeurIPS 36, 46595-46623 (2023)

--- Page 1 ---
arX1v:2507.23751v1 [cs.AI] 31 Jul 2025

CoOT-SELF-INSTRUCT:
BUILDING HIGH-QUALITY SYNTHETIC PROMPTS FOR
REASONING AND NON-REASONING TASKS

Ping Yu', Jack Lanchantin', Tianlu Wang', Weizhe Yuan':*, Olga Golovneva!
Ilia Kulikov', Sainbayar Sukhbaatar!', Jason Weston!-?, Jing Xu!
TRAIR at Meta, 7NYU

ABSTRACT

We propose CoT-Self-Instruct, a synthetic data generation method that instructs
LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given
seed tasks, and then to generate a new synthetic prompt of similar quality and
complexity for use in LLM training, followed by filtering for high-quality data
with automatic metrics. In verifiable reasoning, our synthetic data significantly
outperforms existing training datasets, such as slk and OpenMathReasoning,
across MATHS500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable
instruction-following tasks, our method surpasses the performance of human or
standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard.

1 INTRODUCTION

The transformative rise of Large Language Models (LLMs) has initiated a substantial paradigm

shift in the domain of deep learning (Zhang et al.| 2023 2024). The

development of such models emphasizes scale, and relies heavily on large volumes of high-quality

data (Gandhi et al.}!2024}/Abdin et al.|/2024). However, acquiring such data from human sources can

often be challenging or even impractical due to factors such as high costs, data scarcity, and privacy
concerns (Kurakin et al.|/2023). Furthermore, several studies
2023) have pointed out that human-generated data, being inherently prone to
biases and errors, may not always be ideal for model training or evaluation. In this context, synthetic
data emerges as a viable alternative for obtaining high-quality datasets.

Synthetic data is artificially generated to replicate the characteristics and patterns of real-world data.
One innovative approach to creating such data is the Self-Instruct method [2022a),
which utilizes LLMs themselves to generate instruction-following examples. This method begins
by selecting a small set of seed instruction-following samples, which are then used to prompt LLMs
to produce additional demonstrations in a similar format. Since then a number of variants have been
introduced that increase the complexity of queries 2024), maintain
semantic diversity 2023), scale the synthetic data (Yuan et al. 2023), and use these
methods in self-improvement loops (Yuan et al. 2024). However, a significant challenge with these
approaches is to ensure the quality and effectiveness of the generated data for language model train-
ing. Overall, generating high-quality synthetic data and optimizing its use for both reasoning and
non-reasoning tasks still remains insufficiently understood.

In this paper, we present Chain-of-Thought(CoT)-Self-Instruct, a method that both (i) uses reason-
ing to help create high quality synthetic data; and (ii) self-filters the created data to only keep the
highest quality ones, see|Figure I] We show the efficacy of this approach for creating both verifiable
reasoning data and non-verifiable instruction following tasks, where in both cases using Chain-of-
Thought (CoT) to help generate the examples outperforms those generated without CoT. To curate
high quality verifiable data we introduce Answer-Consistency, which discards examples where the
CoT-Self-Instruct-generated answer does not match the majority vote solution of the LLM, with the
assumption that those examples are either incorrectly labeled or too difficult. For non-verifiable data
we use the recent Rejecting Instruction Preferences (RIP) method which measures
the quality of prompts based on the distribution of reward model scores from LLM solutions. In both

--- Page 2 ---
 Okay, let's tackle this. The user wants me to generate a new reasoning question
Analyze Seed based on the given seed questions. Let me first understand the seed questions to ensure
Instructions ~~~ my new question is inspired but original.

Answer
Prompt x i} yal
Answer

Let me try to think of a concrete example ...... But that's similar to Seed Question 2.

Seed Maybe modify it. yn |
Kal . Alternatively, maybe use different functions. For example ...... But that seems eee
Pans too simple. Maybe the user wants a more complex problem. Let me think again. Answer yO ea y*
Alternatively, think of a problem involving ...... So the answer is 1.
Brainstorm This is a good candidate. It's a novel question, not directly copying the seeds, inconsistent consistent
candidates: ~ and the answer is a single scalar
& self evaluate once
Let me check if it meets the requirements....... Reject x Accept x
Yes, this seems to fit. The question is self-contained, not copying the seeds, and the answer > 8

Final self A is a single scalar. So this should be the new question.
evaluation [New Question Begin]Find the value of $ k $ such that the system of equations $ kx + yF 1 Answer-Consistency Filter
$and $x + ky =1 $ has infinitely many solutions.[New Question End]
[Final Answer to New Question Begin]\\boxed{1}[Final Answer to New Question End]

Find Common

Elements ——(_ - Step 1 #Common Elements List#:

cess Generate a plan

- Step 2 #Plan#: —
The plan is to create a synthetic prompt that combines the elements of .... This prompt

will explore a scenario where ....

- Step 3 #Synthetic Prompt#: —p>
Develop a message for the administrators of the Meiji Shrine in Tokyo, suggesting that

they incorporate more digital and interactive elements into their traditional New Year

(Oshogatsu) festivities, such as an app that allows visitors to explore the shrine's history

MlResponse1 Mill Response 2_—«Mlll Response 3

.

Reward(x2) Reward(x3)

Response Rewards

and cultural significance, or an augmented reality feature that highlights the shrine's 2 x2 x3
traditional architecture and decorations. ...... ae N Prompts
RIP Filter
Step1. Synthetic Instruction Creation via
Chain-of-Thought (CoT) Step 2. Synthetic Instruction Curation

Figure 1: CoT-Self-Instruct. Our method first prompts LLMs to reason and generate new instruc-

tions given seed prompts, followed by automatically curating high-quality data using either Answer-
Consistency for verifiable reasoning tasks, or RIP 2025) for non-verifiable tasks.

i

cases, filtering provides further gains. For reasoning tasks, CoT-Self-Instruct generated training data
outperforms Self-Instruct and existing datasets such as slk (Muennighoff et al.|/2025) and Open-
MathReasoning (Moshkov et al.}/2025) across MATH500, AMC23, AIME24 and GPQA-Diamond.

For non-reasoning tasks, it outperforms human data from WildChat (Zhao et al.||2024) and Self-
Instruct, whether filtered or not, on both AlpacaEval 2 and ArenaHard. Overall, models trained with

CoT-Self-Instruct generated training data provided the best results of the methods we tested.

2 RELATED WORK

Synthetic Data Generation Synthetic data is produced using algorithms (Saxton et al.||2019),
generative models 2022), or simulations (Vezhnevets et al.}|/2023),
rather than being directly created by humans (Liu et al.||2024). It presents a promising solution for
training models, particularly in scenarios where real-world data is scarce, expensive, or difficult to
obtain. Self-Instruct first proposed a framework that lets a language model
be prompted with seed data as few-shot examples in order to generate new synthetic data. Such
data has been used to then self-train language models, e.g. in the Self-Rewarding framework
let al.|/2024). Evol Instruct proposed to increase prompt complexity by letting the
language model re-write the original prompts with increasing complexity. Other specific methods
of creating complex synthetic data have been proposed, for example multi-hop question answering

(Lupidi et al.|/2024) or difficult reasoning questions (Yuan et al.|/2025) both grounded on documents.
2025

Synthetic data has also been used to help train agents ( 2025) and tool-

use models (Mekala et al.||2024), and for rewriting pre-training data (Maini et al.||2024
2025).

Synthetic Data Selection Data selection is a critical component for post-training with synthetic
data (and for data in general). Previously, LLM training was regarded as largely dependent on the
size of available training data |2022b). More
recent work has revealed that training on a smaller yet higher-quality curated set of prompts tends to
be more effective in improving models’ both instruction following and reasoning capabilities

2024}/Muennighoff et al.|/2025). In addition to preprocessing

--- Page 3 ---
techniques such as deduplication of similar prompts using similarity metrics such as ROUGE-L
similarity score or clustering (2023), as language models become
more powerful, prompt curation can also be facilitated by using LLMs themselves as a quality judge.
Recent work studies employing powerful language models to measure the complexity, diversity and
quality of instructions
[2023a). The success of RLHF for post-training (Stiennon et al]
2024) has attracted more attention to collecting large scale and high quality preference data. Most
work involving preference optimization employs existing methods derived from pretraining and
instruction-tuning (Touvron et al.|[2023][Muennighoff et al.|[2025), such as deduplication, clustering,

quality classifiers or filtering heuristics. Rejecting Instruction Preferences (RIP) 2025) is
a recent method that gives strong performance by leveraging reward models based on the LLM

responses to filter prompts. For verifiable reasoning tasks, Self-Consistency filtering
2024) has also been shown to be a high quality curation method by rejecting prompts where LLM
solutions show no agreement, as the task is either incorrectly labeled or too difficult.

3. CHAIN-OF-THOUGHT(COT)-SELF-INSTRUCT

(CoT)-Self-Instruct is an approach to generate high quality synthetic data for training using rea-
soning. We first assume access to a language model, and a small amount of high quality human-
annotated seed data. We consider both verifiable reasoning domains, and non-verifiable general
instruction following. Our approach involves two stages:

1. Synthetic Instruction Creation with Chain-of-Thought (CoT): given sample human-
annotated seed instructions, we instruct the LLM to reason step by step to come up with
instructions of similar complexity and domain.

2. Synthetic Instruction Curation: we curate the generated synthetic data to keep only high-
quality instructions for self-training.

We then train LLMs using the generated high quality synthetic instructions. We describe each stage
in turn.

3.1 SYNTHETIC INSTRUCTION CREATION VIA COT

The process of CoT-Self-Instruct data creation starts with a small set of seed instructions as the
instruction pool. Multiple instructions are sampled at random from the instruction pool, and then
used to few-shot prompt a language model to generate a series of intermediate reasoning steps,
followed by a new instruction. Unlike standard Self-Instruct which directly
prompts the model to write new instructions given a list of seed instructions, each time we show the
LLM an N-shot set of sample instructions, we first ask it to carefully analyze the given instructions,
such as domain, complexity and purpose. After analyzing the seed instructions, reflecting on what
makes them high quality prompts, the LLM is prompted to reason step by step to come up with a plan
to generate a new self-contained instruction that is of similar quality and complexity as the given seed
instructions, and ultimately to output the final synthetic instruction satisfying these requirements in
a strict answer format.

Verifiable reasoning tasks For reasoning tasks where there is a deterministic answer which we
can compare against to generate verifiable rewards during training, we instruct the LLM to use
reasoning to generate both an instruction and the verifiable target. The prompt we used for CoT-

Self-Instruct on reasoning tasks is given in

General instruction following tasks For tasks involving general instruction-following with open-
ended responses, we direct the LLM to use reasoning to generate only the instruction, not the re-
sponse itself. In these instances, later during training on this synthetic data we utilize a reward
model to assess the responses, eliminating the need for a reference answer. The prompt we used for
CoT-Self-Instruct on general instruction following tasks is given in[Figure 3] Seed prompt pools for
instruction-following typically include various different domains. When selecting few-shot prompts,
for example combining prompts from storytelling and coding could result in unnatural synthetic

--- Page 4 ---
Figure 2: CoT-Self-Instruct prompt generation template for verifiable reasoning tasks.

You are a reasoning question generator assistant. Your goal is to create a novel, and challenging
reasoning question. You are provided the following seed questions:

Seed Question 1: {INSTRUCTION 1}
Seed Question 2: {INSTRUCTION 2}

Your task is to:

1. Write a brand-new, self-contained reasoning question that meets the following requirements:

(a) The question draws inspiration from the seed question without copying it verbatim, remaining novel
and of comparable difficulty.

(b) The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced
fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice
from A to D).

2. Then reason step by step, solve the new question and format your output as follows:

[New Question Begin]{your_generated_question}[New Question End]

[Final Answer to New Question Begin]\boxed{your_final_answer}[Final Answer to New Question End]

Figure 3: CoT-Self-Instruct prompt generation template for general instruction following tasks.

You are a prompt generator assistant. Your goal is to create diverse and creative synthetic prompts.
Please follow the steps below to create synthetic prompts.

Step 1: Carefully read #Prompt 1# and #Prompt 2#. Identify and list all the common elements
between these two prompts. If no common elements are found, list the main elements from each
prompt.

Step 2: Develop a comprehensive plan based on the #Common Elements List# or #Main Ele-
ments List# from Step 1. This plan will guide the generation of new synthetic prompts that are similar
to the original prompts.

Step 3: Execute the plan step by step and provide one #Synthetic Prompt#.

Please reply strictly in the following format:

- Step 1 #Common Elements List# or #Main Elements List#:
- Step 2 #Plan#:

- Step 3 #Synthetic Prompt#:

#Prompt 1#:
{INSTRUCTION 1}

#Prompt 2#:
{INSTRUCTION 2}

prompts. To address this, we propose to first label seed prompts into categories, and then sample
from the categories first.

3.2 SYNTHETIC INSTRUCTION CURATION

Even with the strongest language models, not all generated synthetic instructions are well-defined
and answerable, or are effective in base model self-training. We therefore apply a curation step to
select higher quality synthetic instructions from the pool of generated data for final post-training
with RL.

--- Page 5 ---
Verifiable reasoning tasks We propose Answer-Consistency to filter and retain only high quality
data. Given the task instruction, we first instruct the LLM to generate K responses and take the
majority response. We then reject the data example and remove it from the training pool if the
majority response does not match the target answer in the synthetic data example generated by CoT-
Self-Instruct (i.e., via [Figure 2). Because the target answer from CoT-Self-Instruct is generated
with extensive reasoning steps during instruction writing and question-answering this
differs from the way a response is generated at inference time and might be more accurate. Hence,
comparing if the two labels match gives an extra layer of filtering. We confirm in our experiments

that Answer-Consistency is superior to standard Self-Consistency filtering (Prasad et al.||2024).

General instruction following tasks For non-verifiable tasks, the Answer-Consistency method is
not applicable. Instead, we employ the Rejecting Instruction Preferences (RIP) method as proposed
by [Yu et al.|(2025). In this method, for a given task instruction, A’ responses are generated, and
each response is evaluated using a reward model (RM), resulting in a score for each response. The
filtering process is then based on the distribution of these scores. In our setting we use the lowest
score among these /C responses to represent the score of the synthetic prompt. We then filter the data
by selecting only those prompts with the higher scores. Notably, this approach can also be applied
to verifiable tasks, and we conduct experiments in that context as well.

3.3. SELF-TRAINING WITH SYNTHETIC DATA

After generating the synthetic training data, we can conduct RL training on the set of generated
instructions. We compare the performance of self-trained LLMs with models trained on human-
annotated data and on seed instructions in reasoning and non-reasoning domains respectively. For
verifiable reasoning tasks, we use GRPO (Shao et al.| /2024), and for general instruction following
we consider both offline DPO (Rafailov et al.||2024) and online DPO, which can perform much

better, see e.g. |Lanchantin et al.

4 EXPERIMENTAL SETUP

We study the effectiveness of our synthetic prompt generation approach for reasoning and non-
reasoning domains along the following two axes: synthetic prompt generation, and prompt curation.

4.1 REASONING

Seed Instructions We use s1k (Muennighoff et al.|/2025) reasoning instructions as our seed rea-

soning tasks. The s1k dataset consists of 1000 high-quality, diverse and difficult reasoning prompts.
To conduct self-training with verifiable rewards we select a subset of s1k consisting of 893 verifi-
able reasoning instructions by filtering out theorem-proving questions and only keeping those that
yield a scalar, single-valued, or simple closed-form answers that can be easily verified (such as 1, A,

False, se), We then use this subset as the seed instruction pool to generate more verifiable

reasoning instructions.

Prompt Generation The CoT-Self-Instruct template is given in [Figure 2] and the baseline Self-
Instruct method is given in Appendix|Figure 6] To evaluate how CoT-Self-Instruct compares to base-
lines for generating verifiable reasoning tasks, we apply these methods to Qwen3-4B-Base models,
Qwen3-4B model with Think mode and Qwen3-4B model with NoThink mode (2025).
We use temperature = 0.7 and top-p=0.8 for Qwen3-4B-Base and Qwen3-4B (NoThink mode), and
temperature = 0.6 and top-p=0.95 for Qwen3-4B (Think mode).

RLVR Training All our reasoning experiments use GRPO training initialized from Qwen3-4B-
Base with reinforcement learning from rule-based verifiable rewards (RLVR). For hyperparameters,
we use a cosine learning rate scheduler with a peak value of le — 6 and adopt the AdamW optimizer
for the policy model. We set the number of training epochs to 40 with a batch size of 128. For
rollouts, we sample 16 rollouts for each prompt with temperature = 0.6 and top-p=0.95, with a

--- Page 6 ---
maximum length of 4096 tokens. All GRPO experiments are conducted with VeRL(Sheng et al.

and Math-Verify|']as verifier.

Baselines & Variations We construct targets for Self-Instruct and CoT-Self-Instruct when build-
ing the generated instructions. As a baseline, we also train on the original s1k prompts rather than

synthetic data, in that case we use the public available DeepSeek R1 2025) thinking
solution from simplescaling/s1K-1.1 to build targets. We also compare to training on OpenMath-

Reasoning which consists of 10k prompts (Moshkov et al.||2025) with publicly available solutions
by DeepSeek-R1 and QwQ-32B. We also explore some alternative ways of filtering data or con-
structing targets, explored as variations on our main experiments:

¢ Self-Consistency filtering: generating responses KC times with random seeds and then se-
lecting the majority-voted answer as the target or rejecting the example if the majority
answer receives fewer votes than a given threshold (50% in our main experiments).

* RIP filtering: we use the infly/INF-ORM-Llama3.1-70B (Minghao Yang||2024) RM.

¢ Best-of-K targets: constructing targets by selecting the highest scored answer out of K
responses using INF-ORM-Llama3.1-70B RM.

Evaluation We evaluate using Math500 (Hendrycks et al.| {2021} |Lightman et al.||2023), AIME
2024, AMC 23, and GPQA Diamond (Rein et al.||2024). We use temperature 0.6 and top-p 0.95 to

generate predictions. For each problem we generate N = 16 solutions and report average accuracy.

4.2 NON-VERIFIABLE INSTRUCTION FOLLOWING

Seed Instructions We use the Wildchat-RIP-Filtered-by-8b-Llama datasef?| which includes 4k
high-quality prompts filtered from 20k raw wildchat prompts as our seed prompts. Unlike in some
reasoning tasks, this data includes a variety of different domains. Combining, for instance, prompts
from storytelling and coding as few-shot examples could result in generating unnatural synthetic
prompts. To address this, we categorized all seed data into 8 distinct categories. During sampling,
we select 2 seed prompts from the same category to serve as few-shot prompts. Our seed data spans
8 categories: Writing & Storytelling, Technical & Programming, Creative & Design, Data & Analy-
sis, Education & Research, Communication & Support, Business & Marketing, and Miscellaneous.

Prompt Generation We evaluate how CoT-Self-Instruct compares to baselines for generating
non-verifiable instruction-following tasks by applying these methods using LLama 3.1-8B-Instruct.
The CoT-Self-Instruct template is given in[Figure 3] and the baseline Self-Instruct method is given
in[Figure 4] We also experiment with a prompt that lies between the two methods by generating a
short rather than long CoT, given in|Figure 5| For RIP filtering, we use the Athene-RM-8B model"
reward model over 32 responses.

DPOTraining We train via DPO starting from LLama 3.1-8B-Instruct, leveraging the fairseq2
library (Balioglu\/2023). We use a batch size of 64 and learning rate of 1e—6 with dropout rate of 0.0
and a ( value of 0.1 throughout the experiments. For each prompt, we generate 64 responses. These
responses are then annotated with Athene-RM-8B to select pairs. Compared to human prompts, our
synthetic prompts tend to be more complex, resulting in longer average response lengths, which can
lead to length explosion. During DPO training, the evaluation judge often favors longer responses,
potentially causing response lengths to increase over time (Yuan et al.| et al. (2024) ). To mitigate this
issue, we adopted the approach outlined by[Wu et al.|(2024), which ee pining the reward
score with length information to determine the preferred response. This method ensures that shorter
responses are selected when scores are similar. We applied a length normalization coefficient of 0.2
for the length-normalized reward. This is applied for all methods, in each case sampling 5k DPO
pairs.

https://huggingface.co/Nexusflow/Athene-RM- 8B

--- Page 7 ---
Table 1: CoT-Self-Instruct results on reasoning tasks, comparing to baselines, fine-tuning Qwen3-
4B-Base with GRPO. For Self-Instruct and CoT-Self-Instruct the synthetic data (including targets)
is constructed with Qwen3-4B. We report pass@1 averaged over 16 seeds. CoT-Self-Instruct gen-
erates synthetic data that outperforms existing prompt training sets and the Self-Instruct method,
particularly when applying our data filtering methods.

MATH AIME AMC GPQA

# Train 500 24 23 Diamond Avg. t
Qwen3-4B-Base (Zero-Shot) - 67.4 10.6 42.0 24.2 36.1
sIk Prompts + (R1) Gold Label 893 68.6 18.5 51.3 40.1 44.6
OpenMathReasoning Prompts + Gold Label 10,000 79.0 13.3 62.5 35.4 47.5
Self-Instruct 5000 81.1 16.3 58.1 42.5 49.5
+ Self-Consistency Filter 3467 83.6 18.5 68.5 44.1 53.6
+ RIP Filter 2254 84.5 21.2 65.9 45.5 54.5
CoT-Self-Instruct 5000 84.9 20.4 62.2 44.4 53.0
+ Self-Consistency Filter 4034 85.2 22.5 67.8 44.9 55.1
+ RIP Filter 2419 85.7 24.4 70.5 44.4 56.2
+ Answer-Consistency Filter 2926 86.5 24.6 72.3 45.5 57.2
+ Answer-Consistency Filter (more data) 10,000 86.7 26.7 73.8 47.4 58.7

Online DPO training We also assess online DPO by following the training settings described by
[Lanchantin et al.|(2025). We use the default sampling parameters (temperature=1.0, top-p=1.0) to
generate exploration rollouts. We train models using the £airseq2 library (Balioglu\/2023), where
model inference is performed with the v11m library (Kwon et al.|/2023).

Evaluation To evaluate the helpfulness and quality of responses, we employ AlpacaEval 2.0
and Arena-Hard (Li et al.|/2024bJa). These are robust instruction-
following benchmarks that show a strong correlation with user preferences. Originally, AlpacaEval
used GPT-4 Preview (11/06) as the judge, while Arena-Hard utilized GPT-4.1 for its leaderboard.
However, since we do not have access to these specific OpenAI API versions, we conduct our tests
using two alternative (and newer) judges: GPT-4-turbo and GPT-4o. For generating predictions, we
set the decoding temperature to 0.6 and the top-p to 0.9, aligning with the commonly used values of
the seed model in our study. Our validation set used for checkpoint selection is based on a held-out
set of 470 examples, comprising 253 validation examples from|Li et al.|(2023a) and 218 Evol-Test

set examples from|{Xu et al.| (2023).
5 EXPERIMENTAL RESULTS

Our main results are given in|Table 1| for reasoning tasks and |Table 2| for non-reasoning tasks.
Various other variations and ablations are given in the Appendix.

5.1 REASONING TASKS

Synthetic instructions generated by CoT-Self-Instruct outperform Self-Instruct In
where Qwen3-4B-Base models are GRPO trained on Qwen3-4B generated prompts and responses,
CoT-Self-Instruct achieves an average accuracy of 53.0%, outperforming Self-Instruct which yields
49.5% (both without data filtering). As shown in Appendix[Table 8]similar trends are observed when
training on Qwen3-4B-base, rather than Qwen3-4B, generated targets.

Filtered CoT-Self-Instruct outperforms filtered Self-Instruct Applying filtering methods to
both CoT-Self-Instruct and Self-Instruct improves both methods, despite the overall amount of train-
ing data decreasing, see [Table 1] That is, it is better to have high quality synthetic data than more
data that is lower quality. However, we find that CoT-Self-Instruct still maintains its advantage over
Self-Instruct, whichever filtering method is used. E.g. with Self-Consistency Filtering Self-Instruct
improves from 49.5% — 53.6%, while CoT-Self-Instruct improves from 53.0% — 55.1%. Similar
findings are observed with RIP filtering as well. We find empirically that the optimal filtering criteria
is to filter out prompts with lower than 50% RIP-Scores.

--- Page 8 ---
Table 2: CoT-Self-Instruct results on general instruction following tasks, comparing to base-
lines, fine-tuning LLama 3.1-8B-Instruct with offline and online DPO. CoT-Self-Instruct generates
synthetic data that outperforms human-written prompt training sets and the Self-Instruct method,
particularly when applying our data filtering methods. Both AlpacaEval 2 and ArenaHard are eval-
uated with two kinds of judge: GPT-4 Turbo and GPT-4o0, with similar conclusions.

Training AlpacaEval LC Winrate ArenaHard Score

Method GPT-4 Turbo GPT-4o GPT-4 Turbo GPT-4o0 Avg. T
LLama 3.1-8B-Instruct DPO 27.3 21.3 32.0 27.8 27.1
Human prompts (WildChat) DPO 49.1 43.0 52.7 42.6 46.8
+ RIP Filter DPO 57.6 44.5 59.1 41.7 50.7
Self-Instruct DPO 52.9 46.0 51.8 39.2 47.4
+ RIP Filter DPO 55.2 46.1 55.6 39.5 49.1
CoT-Self-Instruct DPO 58.5 48.6 62.0 46.7 53.9
+ RIP Filter DPO 63.2 49.4 60.2 45.8 54.7
Human prompts (Wildchat) Online DPO 80.1 62.7 64.4 45.5 63.1
CoT-Self-Instruct + RIP Online DPO 83.2 68.7 67.3 49.3 67.1

High quality synthetic prompts generated by CoT-Self-Instruct significantly outperform seed
instructions and other publicly available reasoning prompts CoT-Self-Instructions outperform
slk, see[Table I] where models trained on 2926 filtered CoT-Self-Instructions achieve 57.2%. This
is much higher than the 44.6% achieved with s1k prompts using R1 labels (and sik results are even
lower, 43.8%, with Qwen3-4B labels, see Appendix [Table 5). Filtering CoT-Self-Instruction to the
same training size as s1k yields 54.2%, still significantly higher, see Appendix [Table 3] These results
also outperform using 1OK OpenMath-Reasoning instructions with gold labels, which gives 47.5%.
Increasing the CoT-Self-Instruction with Answer-Consistency filtering data to 10k improves results
further with an average of 58.7%. Overall, CoT-Self-Instruct with Answer-Consistency filter gives
the best performance of all existing datasets or synthetic data construction methods tested.

5.2 NON-VERIFIABLE INSTRUCTION FOLLOWING TASKS

Synthetic instructions generated by CoT-Self-Instruct outperform Self-Instruct For non-
reasoning tasks, allowing the model to create a plan beforehand with CoT-Self-Instruct also sig-
nificantly enhances the quality of synthetic data, see Averaged over AlpacaEval 2 and
ArenaHard, CoT-Self-Instruct achieves an average of 53.9 vs. Self-Instruct’s 47.4, both without fil-
tering and training with DPO. We also observe that asking for longer CoT reasoning chains provides
more gains that shorter CoTs, see Appendix [Table 11] further emphasizing the need for reasoning
when producing synthetic data.

RIP Filtering improves CoT-Self-Instruct results further Applying the RIP filter to each
method we find it to be effective across all types of synthetic generation methods tested. This
boosts the CoT-Self-Instruct results from 53.9 — 54.7. RIP also improves Self-Instruct as well,
from 47.4 — 49.1, but still underperforming CoT-Self-Instruct. We can also apply RIP filtering to
human prompts from WildChat in a similar manner. In this case we actually see a larger boost, from
46.8 — 50.7. We attribute this to human data being relatively noisy compared to synthetic data,
which can make filtering more important.

High quality synthetic prompts generated by CoT-Instruct significantly outperform human
prompts Our best performing DPO-trained model is achieved by using CoT-Self-Instruct with
RIP data filtering, yielding 54.7. This outperforms LLama 3.1-8B-Instruct (27.1) or training on hu-
man prompts from WildChat with (46.8) or without RIP data filtering (50.7). We also performed
experiments with online DPO, which improved results further. In that setting human prompts from
WildChat obtain 63.1 while CoT-Self-Instruct+RIP obtains 67.1. Overall, we find CoT-Self-Instruct
with RIP filtering to yield the best performance over all existing datasets or synthetic data construc-
tion methods tested.

--- Page 9 ---
6 CONCLUSION

In this paper, we propose CoT-Self-Instruct, a synthetic data creation and curation pipeline that in-
structs LLMs to plan and reason to come up with new synthetic prompts given seed instructions,
and then filters them for quality, either using Answer-Consistency for verifiable tasks or RIP fil-
tering when they are not verifiable. We show that applying our method improves models’ abilities
in both the reasoning and non-reasoning domains by creating high quality synthetic instructions
for RL training, surpassing existing seed human-annotated instructions and public training sets on
challenging benchmarks.

REFERENCES

Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,
Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 techni-
cal report. arXiv preprint arXiv:2412.08905, 2024.

Can Balioglu. fairseq2, 2023. URL http://github.com/facebookresearch/

fairseq2

Vadim Borisov, Kathrin SeBler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language
models are realistic tabular data generators. arXiv preprint arXiv:2210.06280, 2022.

Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient
instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082,
2023.

Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay
Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: Training a better alpaca with fewer data.
In The Twelfth International Conference on Learning Representations, 2024. URL [ht tps:)

//openreview.net/forum?id=FdVxgSJhvz

Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong
Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional
conversations. arXiv preprint arXiv:2305.14233, 2023.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.

Yann Dubois, Bala4zs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled al-
pacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.

Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better syn-
thetic data by retrieving and transforming existing datasets. arXiv preprint arXiv:2404.14361,
2024.

Fabrizio Gilardi, Meysam Alizadeh, and Maél Kubli. Chatgpt outperforms crowd workers for text-
annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023.

Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yu-
peng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection.
arXiv preprint arXiv:2301.07597, 2023.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874, 2021.

Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. arXiv preprint
arXiv:2309. 16349, 2023.

--- Page 10 ---
Alexey Kurakin, Natalia Ponomareva, Umar Syed, Liam MacDermed, and Andreas Terzis. Harness-
ing large-language models to generate private synthetic text. arXiv preprint arXiv:2306.01684,
2023.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles, 2023.

Jack Lanchantin, Angelica Chen, Janice Lan, Xian Li, Swarnadeep Saha, Tianlu Wang, Jing Xu,
Ping Yu, Weizhe Yuan, Jason E Weston, et al. Bridging offline and online reinforcement learning
for Ilms. arXiv preprint arXiv:2506.21495, 2025.

Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gon-
zalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and
benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024a.

Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion
Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024b. URL

https://imsys.org/blog/2024-04-19-arena-hard

Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and
Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259,
2023a.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following

models. https://github.com/tatsu—-lab/alpaca_eval) 5 2023b.

Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth
International Conference on Learning Representations, 2023.

Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi
Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data. arXiv
preprint arXiv:2404.07503, 2024.

Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for align-
ment? a comprehensive study of automatic data selection in instruction tuning. arXiv preprint
arXiv:2312.15685, 2023.

Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang.
On Ilms-driven synthetic data generation, curation, and evaluation: A survey. arXiv preprint
arXiv:2406.15126, 2024.

Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and
Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language
models. In The Twelfth International Conference on Learning Representations, 2023.

Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foer-
ster, Roberta Raileanu, and Maria Lomeli. Source2synth: Synthetic data generation and curation
grounded in real data sources. arXiv preprint arXiv:2409.08239, 2024.

Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephras-
ing the web: A recipe for compute and data-efficient language modeling. arXiv preprint
arXiv:2401.16380, 2024.

Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang,
and Jane Dwivedi-Yu. Toolverifier: Generalization to new tools via self-verification. arXiv
preprint arXiv:2402.14158, 2024.

Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models:
Towards zero-shot language understanding. Advances in Neural Information Processing Systems,
35:462-477, 2022.

10

--- Page 11 ---
URL

Xiaoyu Tan Minghao Yang, Chao Qu.
ggingf /infly
co/infly/INF- ORM-Llama3.1- 70B)

Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization
via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.

Inf-orm-llama3.1-70b, 2024. [https://

Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt
Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art math-
ematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891,
2025.

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Candés, and Tatsunori Hashimoto. s1: Simple test-time
scaling. arXiv preprint arXiv:2501.19393, 2025.

Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, and
Xian Li. Recycling the web: A method to enhance pre-training data quality and quantity for
language models. arXiv preprint arXiv:2506.04689, 2025.

Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit
Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference opti-
mization. arXiv preprint arXiv:2411.04109, 2024.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in Neural Information Processing Systems, 36, 2024.

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Di-
rani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a bench-
mark. In First Conference on Language Modeling, 2024.

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea-
soning abilities of neural models. arXiv preprint arXiv: 1904.01557, 2019.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical
reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.

Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,
Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint
arXiv: 2409.19256, 2024.

Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J
Liu, James Harrison, Jaechoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training
for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances
in Neural Information Processing Systems, 33:3008—3021, 2020.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Alexander Sasha Vezhnevets, John P Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A
Duéfiez-Guzman, William A Cunningham, Simon Osindero, Danny Karmon, and Joel Z Leibo.
Generative agent-based modeling with actions grounded in physical, social, or digital space using
concordia. arXiv preprint arXiv:2312.03664, 2023.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions.
arXiv preprint arXiv:2212.10560, 2022a.

11

--- Page 12 ---
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An-
jana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.
Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv
preprint arXiv:2204.07705, 2022b.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652, 2021.

Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston,
and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with
llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.
arXiv preprint arXiv:2304.12244, 2023.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint
arXiv:2505.09388, 2025.

Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more
for reasoning. arXiv preprint arXiv:2502.03387, 2025.

Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, and
Jing Xu. Rip: Better models by survival of the fittest prompts. arXiv preprint arXiv:2501.18578,
2025.

Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason
Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 3, 2024.

Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Ilia Kulikov, Kyunghyun Cho, Dong
Wang, Yuandong Tian, Jason E Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8
m challenging questions. arXiv preprint arXiv:2502.13124, 2025.

Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,
and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language
models. arXiv preprint arXiv:2308.01825, 2023.

Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. Automatic instruction
evolving for large language models. In Proceedings of the 2024 Conference on Empirical Methods
in Natural Language Processing, pp. 6998-7018, 2024.

Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang,
Sumit Kumar Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete sur-
vey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need? = arXiv preprint
arXiv:2303.11717, 2023.

Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun
Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero
data. arXiv preprint arXiv:2505.03335, 2025.

Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat:
1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia
Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information
Processing Systems, 36, 2024.

Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging
language model agents. arXiv preprint arXiv:2506.01716, 2025.

12

--- Page 13 ---
Figure 4: Self-Instruct prompt generation template for non-verifiable instruction following tasks.

Below are sample tasks from user.
1.  {INSTRUCTION 1}
2. {INSTRUCTION 2} 

Come up with one new task, wrapped with and 

Figure 5: Short CoT prompt generation template for non-verifiable instruction following tasks.

Below are sample tasks from user.
1.  {INSTRUCTION 1}
2. {INSTRUCTION 2} 

Come up with one new task, wrapped with and . Please provide your Chain-
of-Thought first and then provide the new generated task.

7 APPENDIX

We report results when matching the training size to 893 the same as our seed tasks in|Table 3

Table 3: CoT-Self-Instruct results on reasoning tasks with same size training sets, comparing
to baselines, fine-tuning Qwen3-4B-Base with GRPO. For Self-Instruct and CoT-Self-Instruct the
synthetic data (including targets) is constructed with Qwen3-4B. We report pass@1 averaged over
16 seeds.

4#Train MATH AIME AMC — GPQA

500 24 23 Diamond Avg. fT
Qwen3-4B-Base (Zero-Shot) - 67.4 10.6 42.0 24.2 36.1
sIk Prompts + (R1) Gold Label 893 68.6 18.5 51.3 40.1 44.6
Self-Instruct 893 80.5 17.2 57.3 41.3 49.1
+ Self-Consistency Filter 893 81.9 20.0 62.8 41.5 51.5
+ RIP Filter 893 82.7 21.5 61.4 43.1 52.2
CoT-Self-Instruct 893 82.4 19.8 60.0 41.3 50.9
+ Self-Consistency Filter 893 83.2 22.7 65.1 41.6 53.1
+ RIP Filter 893 83.0 21.0 63.9 42.9 52.7
+ Answer-Consistency Filter 893 83.7 23.1 66.1 44.1 54.2

We further compare CoT-Self-Instruct with other templates on reasoning tasks:

¢ Self-Instruct-Then-Solve (NoCoT): prompting LLMs to first generate a question then an
answer to its own generated question, without any thinking or CoT, see

* CoT-Self-Instruct (NoSolve): prompting LLMs to reason step-by-step to generate a ques-
tion, without giving the “reference” answer, see

We report additional results with varying prompt templates below.

13

--- Page 14 ---
Figure 6: Self-Instruct (standard, without CoT) prompt generation template for verifiable reasoning
tasks.

You are a reasoning question generator assistant. Your goal is to create a novel, and challenging
reasoning question. You are provided the following seed questions:

Seed Question 1: {INSTRUCTION 1}
Seed Question 2: {INSTRUCTION 2}

Your task is to write a brand-new, self-contained reasoning question that meets the following require-
ments:

1. The question draws inspiration from the seed question without copying it verbatim, remaining novel
and of comparable difficulty.

2. The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced
fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice
from A to D).

3. Do not include any solution, hint, or answer-—only the question statement itself.

Please put your generated problem strictly in the format of
[New Question Begin]{your_generated_question}[New Question End]

Figure 7: CoT-Self-Instruct (No-Solve) prompt generation template for verifiable reasoning tasks
without answering (i.e., generate a question only).

You are a reasoning question generator assistant. Your goal is to create a novel, and challenging
reasoning question. You are provided the following seed questions:

Seed Question 1: {INSTRUCTION 1}
Seed Question 2: {INSTRUCTION 2}

Your task is to write a brand-new, self-contained reasoning question that meets the following require-
ments:

1. The question draws inspiration from the seed question without copying it verbatim, remaining novel
and of comparable difficulty.

2. The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced
fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice
from A to D).

3. Do not include any solution, hint, or answer-—only the question statement itself.

Please reason step by step and put your generated problem strictly in the format of
[New Question Begin]{your_generated_question}[New Question End]

14

--- Page 15 ---
Figure 8: Self-Instruct-Then-Solve (i.e. No CoT) prompt generation template for verifiable reason-
ing tasks.

You are a reasoning question generator assistant. Your goal is to create a novel, and challenging
reasoning question. You are provided the following seed questions:

Seed Question 1: {INSTRUCTION 1}
Seed Question 2: {INSTRUCTION 2}

Your task is to:

1. Write a brand-new, self-contained reasoning question that meets the following requirements:

(a) The question draws inspiration from the seed question without copying it verbatim, remaining novel
and of comparable difficulty.

(b) The question’s final answer should be a single, unambiguous scalar value (e.g., an integer, reduced
fraction, exact radical), or another answer type that can be verified in one step (e.g., ‘yes/no,’ a choice
from A to D).

2. Then solve the new question and format your output as follows:

[New Question Begin]{your_generated_question}[New Question End]

[Final Answer to New Question Begin]\boxed{your_final_answer}[Final Answer to New Question End]

Table 4: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on targets
sampled from Qwen3-4B. We conduct GRPO-training using Qwen3-4B-Base model on synthetic
prompts generated by different templates, with targets sampled from Qwen3-4B. We report pass @ |
averaged over 16 seeds. Two filter thresholds are used: SC = Self-Consistency Rate (i.e. the ratio
majority votes over total votes) and RSc = RIP score (i.e. the quantile of minimum response score.)

Filter MATH AIME AMC GPQA

# Train Thres. 500 24 23 Diamond Avg.
Self-Instruct 5000 - 81.1 16.2 58.1 42.5 49.5
+ Self-Consistency Filter 3467 sc > 0.5 83.6 18.5 68.5 44.1 53.6
+ RIP Filter 2254 RSc > 0.5 84.5 21.2 65.9 45.5 54.5
Self-Instruct-Then-Solve (NoCoT) 5000 - TAS 9.8 47.7 39.0 42.7
+ Answer-Consistency Filter 646 - 75.6 12.9 53.9 38.1 45.1
+ Self-Consistency Filter 3369 SC> 0.5 74.8 10.8 49.8 37.5 43.2
+ RIP Filter 2162 RSc > 0.5 75.0 11.0 52.3 38.0 44.1
CoT-Self-Instruct (NoSolve) 5000 - 84.3 20.2 65.5 43.7 53.4
+ Self-Consistency Filter 3972 sc > 0.5 84.7 24.8 67.5 44.9 55.5
+ RIP Filter 2431 RSc > 0.5 84.9 24.2 72.3 44.6 56.5
CoT-Self-Instruct 5000 - 84.9 20.4 62.2 44.4 53.0
+ Answer-Consistency Filter 2926 - 86.5 24.6 72.3 45.5 57.2
+ Self-Consistency filter 4034 sc > 0.5 85.2 22.5 67.8 44.9 55.1

+ RIP filter 2491 RSc > 0.5 85.7 24.4 70.5 44.4 56.2

15

--- Page 16 ---
Table 5: 893-train-size-matching results of CoT-Self-Instruct, comparing to baselines, for rea-
soning tasks on targets sampled from Qwen3-4B: We conduct GRPO-training using Qwen3-4B-
Base on selected s1k verifiable prompts and 893 synthetic prompts generated by different templates,
with targets sampled from Qwen3-4B. We report pass@1 averaged over 16 seeds on MATHSOO,
AMC23, AMIE24, GPQA-Diamond. Two filter thresholds are used: SC = Self-Consistency Rate
(i.e. the ratio majority votes over total votes) and RSc = RIP score (i.e. the quantile of minimum
response score.)

Filter MATH AIME AMC GPQA

# Train Thres. 500 24 23 Diamond Avg.
slk Prompts
+ Qwen3-4B Target 893 - 71.3 13.7 51.5 38.7 43.8
Self-Instruct 893 - 80.5 17.2 57.3 41.3 49.1
+ Self-Consistency Filter 893 SC > 0.5 81.9 20.0 62.8 41.5 51.5
+ RIP Filter 893 RSc > 0.5 82.7 21.5 61.4 43.1 52.2
CoT-Self-Instruct (NoSolve) 893 - 82.5 20.2 61.7 41.4 51.4
+ Self-Consistency Filter 893 SC > 0.5 83.6 20.6 61.7 43.0 52.2
+ RIP Filter 893 RSc > 0.5 83.4 24.8 64.1 42.8 53.8
CoT-Self-Instruct 893 - 82.4 19.8 60.0 41.3 50.9
+ Answer-Consistency Filter 893 - 83.7 23.1 66.1 44.1 54.2
+ RIP Filter 893 RSc > 0.5 83.2 22.7 65.1 41.6 53.1
+ Self-Consistency Filter 893 SC > 0.5 83.0 21.0 63.9 42.9 52.7

Table 6: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on
majority-voted targets sampled from Qwen3-4B model: We conduct GRPO-training using
Qwen3-4B-Base on synthetic prompts generated by different templates, with majority-voted tar-
gets sampled from Qwen3-4B. We report pass@1 averaged over 16 seeds. Different from [Table 4]
we use majority voted answers by Qwen3-4B model instead of single sampled responses. The con-
clusions are similar to[Table 4]

MATH AIME AMC GPQA

Majority-Voted Qwen3-4B Target # Train 500 24 23 Diamond Avg.
Self-Instruct 5000 80.8 15.6 57.2 43.7 49.3
+ Self-Consistency Filter 3467 80.9 17.7 63.9 46.3 52.2
CoT-Self-Instruct (NoSolve) 5000 82.9 21.9 65.3 44.4 53.6
+ Self-Consistency Filter 3972 83.7 21.3 68.8 44.2 54.5

Table 7: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on Best-of-K
targets sampled from Qwen3-4B model using the reward model infly INF-ORM-Llama3. 1-70B
(2024): We conduct GRPO-training using Qwen3-4B-Base on selected s1k veri-
fiable prompts and synthetic prompts generated by different templates with targets sampled from
Qwen3-4B. We report pass@1 averaged over 16 seeds on MATHS00, AMC23, AMIE24, GPQA-
Diamond.

MATH AIME AMC GPQA

Best-of-K Qwen3-4B Targets # Train 500 24 23 Diamond Avg.
Self-Instruct 5000 83.8 18.8 62.0 44.4 52.2
+ RIP Filter 2254 84.1 20.8 68.4 46.6 55.0
CoT-Self-Instruct (NoSolve) 5000 82.9 22.5 64.8 42.7 53.2

+ RIP Filter 3651 85.2 24.4 71.1 46.8 56.9

16

--- Page 17 ---
Table 8: Results of CoT-Self-Instruct, comparing to baselines, for reasoning tasks on targets
sampled from Qwen3-4B-Base model responses: We conduct GRPO-training using Qwen3-4B-
Base and report pass@1 averaged over 16 seeds on 4 benchmarks. Different from we use
answers sampled from the Qwen3-4B-Base model.

MATH AIME AMC GPQA

#Train 500 24 23 Diamond Avg.
Self-Instruct
(Qwen3-4B-Base NoCoT) 5000 75.7 13.1 51.4 28.0 42.1
+ Self-Consistency Filter 2815 75.9 11.5 54.8 29.5 42.9
+ RIP Filter 3492 75.4 12.5 51.2 28.2 41.8
Self-Instruct (Qwen3-4B NoThink) 5000 75.3 11.0 55.4 27.1 42.2
+ Self-Consistency Filter 1757 75.1 11.9 52.2 27.0 41.5
+ RIP Filter 2263 75.8 13.8 S1.1 30.6 42.8
CoT-Self-Instruct (Qwen3-4B NoSolve) 5000 75.5 11.0 52.2 31.4 42.5
+ Self-Consistency Filter 1672 77.0 15.4 50.5 35.4 44.6
+ RIP Filter 2456 76.2 14.6 53.3 30.4 43.6

Table 9: 893-train-size-matching results of CoT-Self-Instruct, comparing to baselines, for rea-
soning tasks on targets sampled from Qwen3-4B-Base model responses. These experiments are

the train-size-matching variants of [Table 8] 8

MATH AIME AMC GPQA

# Train 500 24 23 Diamond Avg.
Qwen3-4B-Base (Zero-Shot) - 67.4 10.6 42.0 24.2 36.1
slk Prmpt + Qwen3-4B-Base Label 893 75.1 10.4 47.3 28.7 40.4
Self-Instruct
(Qwen3-4B-Base NoCoT) 893 75.3 10.0 51.7 27.1 41.0
+ Self-Consistency Filter 893 75.7 11.7 51.3 28.2 41.7
+ RIP Filter 893 76.2 12.5 50.5 29.2 42.1
Self-Instruct (Qwen3-4B NoThink) 893 75.3 10.0 51.7 27.1 41.0
+ Self-Consistency Filter 893 76.2 10.2 53.3 26.6 41.6
+ RIP Filter 893 76.0 11.9 52.2 31.3 42.8
CoT-Self-Instruct (Qwen3-4B NoSolve) 893 75.9 10.2 51.6 30.1 41.9
+ Self-Consistency Filter 893 76.2 11.5 54.1 34.0 43.9
+ RIP Filter 893 771 13.1 50.0 33.9 43.5

Table 10: Results of CoT-Self-Instruct and other prompt templates for reasoning tasks on
majority-voted targets from Qwen3-4B-Base model: We conduct GRPO-training using Qwen3-
4B-Base and report pass@ 1 averaged over 16 seeds on 4 benchmarks. Different from we
use majority-voted targets sampled from the Qwen3-4B-Base model.

MATH AIME AMC GPQA

Majority- Voted Qwen3-4B-Base Target # Train 500 24 23 Diamond Avg.
Self-Instruct (Qwen3-4B-Base) 5000 76.2 11.7 51.7 30.5 42.5
+ Self-Consistency Filter 2815 771.5 13.1 54.5 29.0 43.6
CoT-Self-Instruct

(Qwen3-4B-Base, No Solve) 5000 76.3 13.1 49.7 30.2 42.3
CoT-Self-Instruct (Qwen3-4B NoSolve) 5000 76.1 12.3 54.5 31.3 43.5
+ Self-Consistency Filter 1672 771.0 13.5 55.3 31.4 44.3

17

--- Page 18 ---
Table 11: Additional comparisons for non-verifiable instruction following tasks using differ-
ent synthetic generation prompts. CoT-Self-Instruct with long CoT generates synthetic data that
outperforms short CoT and standard Self-Instruct templates. Both AlpacaEval 2 and ArenaHard are
evaluated with two kinds of judge: GPT-4 Turbo and GPT-4o0, with similar conclusions.

Training AlpacaEval LC Winrate ArenaHard Score A

Method GPT-4 Turbo GPT-40 GPT-4 Turbo —GPT-4o V8:
Self-Instruct (No CoT) DPO 52.9 46.0 51.8 39.2 47.4
+ RIP Filter DPO 55.2 46.1 55.6 39.5 49.1
CoT-Self-Instruct (Short CoT) DPO 56.5 44.3 51.6 34.1 46.6
+ RIP Filter DPO 59.0 37.7 54.3 37.5 47.1
CoT-Self-Instruct DPO 58.5 48.6 62.0 46.7 53.9
+ RIP Filter DPO 63.2 49.4 60.2 45.8 54.7

18

--- Page 1 ---
SIMURA: Towards General Goal-Oriented Agent via

Simulative Reasoning Architecture with LLM-Based
World Model

Mingkai Deng®** Jinyu Hou®** YilinShen* Hongxia Jin?
Graham Neubig' Zhiting Hu®4 Eric Xing®*

Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence
* School of Computer Science, Carnegie Mellon University
* Samsung Research
4Halicio%lu Data Science Institute, UC San Diego

{mingkaid, jinyuhou } @cs.cmu.edu, eric.xing @ mbzuai.ac.ae

Abstract

AI agents built on large language models (LLMs) hold enormous promise, but
cutrent practice focuses on a one-task-one-agent approach, which not only falls
short of scalability and generality, but also suffers from the fundamental limitations
of autoregressive LLMs. On the other hand, humans are general agents who reason
by mentally simulating the outcomes of their actions and plans. Moving towards
a more general and powerful AI agent, we introduce SIMURA, a goal-oriented
architecture for generalized agentic reasoning. Based on a principled formulation
of optimal agent in any environment, SIMURA overcomes the limitations of au-
toregressive reasoning by introducing a world model for planning via simulation.
The generalized world model is implemented using LLM, which can flexibly plan
in a wide range of environments using the concept-rich latent space of natural lan-
guage. Experiments on difficult web browsing tasks show that SIMURA improves
the success of flight search from 0% to 32.2%. World-model-based planning, in
particular, shows consistent advantage of up to 124% over autoregressive planning,
demonstrating the advantage of world model simulation as a reasoning paradigm.
We are excited about the possibility for training a single, general agent model based
on LLMs that can act superintelligently in all environments. To start, we make
REASONERAGENT- WEB, a web-browsing agent built on SIMURA with pretrained
LLMs, available as a research demo for public testing.

arX1v:2507.23773v1 [cs.AI] 31 Jul 2025

1 Introduction

AI agents powered by large language models (LLMs) hold tremendous potential for handling tasks
that require flexible decision making. Recently, there have been great advancements in agents
specialized in web and computer automation [[1] [2] [3] 4], internet research [5] [6] [7], social simulation
[8], software development [9] [IO], scientific research [12], and so on. Despite the promise,
current LLMs often prove insufficient for solving complex agentic tasks, suffering from issues such
as hallucination, repetitions, or failure at complex planning [13][14]. To address these issues, many
approaches focus on creating agents tailored to specific tasks like the above examples. However, this
strategy have some inherent drawbacks. Economically, redesigning custom agents for every task is

*Co-first author

Preprint. Under review.

--- Page 2 ---
Flight Searching: “I’d like to find a round-trip ticket from Chicago to Dubai next month, traveling in
economy class, with non-stop flights only, departing after 8 AM, and returning within two weeks.”

A t - = oe
Flights

Go to Google Flights Fill in origin and destination Fillin departure andreturndates __ Filter search result and respond

Online Shopping: “I want to buy a black mattress. Can you look at Amazon,
eBay, and Mattress Firm and give me one good option from each?”

Go to Amazon website Search for mattress on eBay Go on Mattress Firm website Search and compose response

News Research: “Find and summarize one article from each of these local news outlets
on Trump’s inauguration: Times of San Diego, The Tennessee Tribune, and MinnPost?”

Go on Times of San Diego Browse The Tennessee Tribune Search on MinnPost Review and compose response

Figure 1: Demo of tasks performed using a web-browsing agent built on SIMURA with simulative
planning using a LLM-based world model.

costly and not scalable from a business standpoint. Intellectually, narrowly focused solutions offer
no clear path towards general and transferrable intelligence. [15] Technically, autoregressive LLMs
rely on linear, step-by-step reasoning that often leads to errors that propagate through their thought
trajectories [16] [17]. Humans, in contrast, are generalist problem-solvers that can reason and plan to
achieve goals in diverse environments. Using a single cognitive system, we adapt to different tasks
not only by linear reasoning, but also by imagining potential outcomes, simulating possibilities using
a mental world model, and planning accordingly.

Moving towards a more general and powerful AI agent, we introduce SIMURA (Simulative Reasoning
Architecture), a goal-oriented architecture for generalized agentic reasoning. SIMURA mitigates
the limitations of LLM autoregressive reasoning by introducing world model as the engine for
planning via simulation. Specifically, a policy module first proposes a few potential actions, aimed at
achieving specific goals based on agent identity and environment. Then, the world model simulates
the outcomes of those proposed actions. Finally, a critic module evaluates these outcomes against the
initial goals in order to select the best action from the candidates. Because simulating the full details
of the world is infeasible and unnecessary for planning, we extract only the relevant information
using natural language as a compact but complete representation, and simulate the next world in
this latent space. To ensure robustness from observation noise and distracting execution details, we
further propose a hierarchical architecture that isolates perception, simulative planning, and action
selection which ensures adaptability and consistency across diverse tasks. Experiments on a range of
web browsing tasks show SIMURA improving substantially compared to baselines, increasing the
success rate of flight search from 0% to 32.2%, with reasoning by WM simulation outperforming
LLM autoregressive reasoning by up to 124%. Figure[I|shows examples of the agent performing
multi-website, long-range task such as flight searching, online shopping, and news research.

For evaluation and demonstration purposes, we implemented SIMURA as an open-source library
available via LLM Reasoners The resulting web agent, REASONERAGENT-WEB [20], is

//github.com/maitrix-org/1lm-reasoners/tree/main/examples/ReasonerAgent -Web

--- Page 3 ---
available as a research preview)| We are actively expanding the system to address broader challenges
and to further demonstrate its generality across a wider range of task domains.

2 Related Work

LLM-Based Agents LLM-based agents have rapidly evolved into versatile systems capable of
autonomous behavior across a range of environments. One major approach to build such systems
focuses on data collection in the targeted environment followed by model training. Notable examples
include AutoWebGLM [21], AgentQ [22], UI-TARS [23], etc. Prompt-based workflows, on the
other hand, have also shown strong potential when equipped with carefully designed modules, as
demonstrated by recent work such as AWM [24], VOYAGER [25], and so on. SIMURA is built on
prompt-based workflows but can leverage observation data for targeted improvement of its world
model [26], leading to reduced reliance on human demonstration and strong generalizability to new
tasks [18], which is an exciting next step.

World-Model-Based Agents Model-based planning for agents have long been frequently discussed
and studied. Early work demonstrated the success of this approach by testing in classic games
like go, chess, shogi and Atari. [27] [28]. Later on, world model was used for policy optimization
and experimented on control tasks. In recent years, with the boost in foundation model’s
capabilities, world-model-based planning was applied to more complex problems like math reasoning
31], playing Minecraft [32], and web browsing [33]. However, these world models typically represent
and predict the world states using holistic continuous embeddings, which suffer from noise and high
variability which detracts from robust and stable decision-making [34]. SIMURA instead adopts
natural language as a discrete, concept-based latent space for consistent representation and prediction,
which shows more general applicability across tasks in practice.

Web Browsing Agents Web browsing and navigation were chosen to evaluate SIMURA due to their
realism and the complex decision-making they demand across diverse, dynamic interfaces. Recent
years have seen the emergence of several prominent web-browsing agents, from proprietary ones such
as OpenAl’s Operator [I], Anthropic’s Computer Use [6], and Google-DeepMind’s Project Mariner
[2], and open-source ones including OpenHand’s BrowsingA gent [35], WebVoyager [36], CogAgent
and WebAgent [38]. These agents are typically built on simple ReAct-based autoregressive
reasoning which have difficulty recovering from previous mistakes; their often specialized design also
preclude these approaches from generalizing to other task domains like social interactions and the
physical world. Numerous benchmarks have been introduced to evaluate these web agents, including
WebArena [3], WebVoyager MiniWoB++ [39], Mind2Web [40], and WebShop [41]. Despite
wide adoption, these benchmarks are usually either built in simulated and simplified environments,
based on outdated questions, or lacks convincing method of measuring task completion, which detract
from the goal of evaluating practically useful web agents. To address these challenges, we build
FlightQA, an new dataset for evaluating agent ability in real-time complex website navigation. More
details are included in Section

Generalist Agents There have been various attempts of building generalist agents recently. One
major approach focuses on creating a multi-agent system that consists of a unified interface on top of
a few specialist agents that collaborates to decompose and complete complex tasks.
Although this approach could lead to impressive performance on benchmarks, it has a few inherent
limitations. First of all, tasks in reality could be versatile and may constantly require new specialist
agents to be added to the system to achieve optimal performance, which is not efficient. Moreover,
independently trained specialist agents for different domains are unable to leverage shared experience
in the way that world model training enables. Finally, error propagation along the interaction trajectory
remains an open challenge and is further complicated by the presence of multiple agents. Another
popular approach utilizes framework similar to the CodeActAgent [46]. These agents
suffer from inaccurate code plans and have limited ability to revise or correct prior errors as well.
SIMURA, on the other hand, is able to avoid these limitations by working as a monolithic architecture
in which world model act as a central planning component.

https: //easyweb.maitrix.org/


--- Page 4 ---
AG World State

Figure 2: A possible definition of an optimal agent

3 SIMURA: Generalized Architecture for Optimal Goal-Oriented Agent

3.1 Formulation of Agent-Environment Model

We first present our formulation of an optimal goal-oriented agent following the agent-environment
model presented in [49]: We consider an agent 7 with identity 7 (e.g., name, description) and goal g
acting in environment p (e.g., web browser, physical world, the entire universe) with action space
A and state space S. Formally, at each time step t, the agent 7 takes the current state s, € S and
outputs the next action a; € A following a policy distribution p, (a; | s;), while the environment ,.
takes the current state s, and action a,, and outputs the next state s;;, € S based on the distribution
Py(Se41|St, a). We can thus denote the distribution of the interaction trajectory up to timestep T, or

Qt; $t41,---;@7—1, ST) given the current state s, as below:
T-1
Pr(at,St41,---,47-1,87 | 8) = |] Px(4x | 8k) PulSk+1 | Sk, Ox) (1)
> at
k=t agent environment

In each state s;, the agent also receives a reward r(g, 5;) based on its goal g. We evaluate the agent
by its discounted cumulative reward, denoted as ry er(g, Sk) (with the discount parameter +;
decaying to zero with time, i.e., limy.. 7 = 0). Note that this reward function can be dense (e.g.,
gaming scores), but perhaps frequently sparse (e.g., curing a disease). The agent’s long-term success
can thus be measured by its expected future discounted reward, also known as value function [50],
which satisfies the following recurrence:

7

T
= lim S- So ver(9, Sx) Pi (Gt, St415---5 ST | St)
=t

V2 (se) = Exp Ss Ver (G, Sk)
kat

T0
(at,S¢41,...,87) k
T-1
= S- (x Yer (g, Sk) + ar¥2ylen) Pi (Gt, St415 +++, ST | sz), (2)
— SD
(4 :Se415--87) eet ——_$_—— trajectory

goal progress

Which indicates that the value function in state s; can be expressed in terms of the value function at
possible future states s; weighted by their probabilities.

3.2 Definition of Optimal Agent

Based on Equations {T]and we can define the optimal agent 7/, in environment j as one that
maximizes the value function, written formally as below:

T= arg max Ve ue (3)

4

--- Page 5 ---
Some simple derivation will show that the optimal agent in state s; will follow the following decision
rule Th when planning for actions a;.7_1:

T-1 T-1
*
m* (sp) = arg max s yeEr(g, 8k) + yrV 2 (sr) II Pul(Si+1 | $i, @i) (4)
Qt:T-1 = i= Y
St4u:T \k=t wt universe response
possible actions goal progress

In practice, agents often samples promising action candidates using a policy function 7 through the
distribution pz (a, | s;). Building the optimal agent thus requires capabilities for proposing possible
actions (7), predicting their outcomes (j), and evaluating goal progress (r, V), respectively. Note that
typical reactive agents that output the next action directly can be seen as taking the first sample from
i (similar to “System 1” in humans which makes fast, instinctive reactions [51]]), without simulating
and evaluating the outcomes using js and V (similar to “System 2” responsible for deliberate decision-
making). In terms of LLM-based agents, this can also be seen as the agent generating a plan using
autoregressive LLMs, which has no way of correcting errors during the sampling process.

3.3 World Model for Generalized Simulative Reasoning

Note that the optimal decision-making process defined in Equation [4] requires the agent to have
access to the ground-truth world state s and the environment pu to experience and optimize over.
However, these are often not available aside from simple scenarios like Go and Chess games
— imagine building an spacecraft to land on Mars, or simply a humanoid robot relying on noisy
sensors in daily environments. World Model (WM) thus arises as a crucial component for predicting
any environment’s response to a general agent. Specifically, a WM f operates on an internal
representation of the world state, denoted as a belief state §;, which is derived from sensory inputs
oz Via an Encoder h (unlike the optimal agent described in §3.2]which has direct access to the true
world state s;). Given proposed action a;, the WM predicts the next belief state 5:4 according to
the distribution p+ (8:41 | §:,a¢). The predicted belief state then allows the agent to propose the
next action, continuing the cycle of prediction and action up to the desired time horizon T’. Thus, a
WM here essentially functions as a generative model of possible future world states, which enables
simulative reasoning, or “thought experiments". Formally, for the optimal agent 7 equipped with
WM f in belief state 5,, we define the simulation-based decision rule in Equation 6 as follows:

T-1 T-1
K(A\) _ A g9 (4 a
m(8:) = arg max > 5 Ver (9, Se) + YeVe (Sr) Il Ps (Si41|$i, Gi) (5)
:T a : . =-V-e—"—~_—”
Oe TH1 $t41:7 k=t at : ate :
simulation with
possible actions goal progress world model

A general-purpose WM f here enables simulation of diverse possibilities across a wide range of
domains, enabling agents to reason about outcomes without direct interaction with the environment.

3.4 Design of Simulative Reasoning Agent Using LLM-Based World Model

In this subsection, we present our design of a generalizable simulative reasoning agent using large
language models (LLMs) as building blocks due to the latter’s strength in a wide range of capabilities
such as summarization, commonsense knowledge, reflection, and tool use, which are gained from
large-scale pretraining and instruction tuning. In particular, we provide detailed discussion on design
decisions that enable robust and general applicability across environments and tasks.

Discrete, Hierarchical State Representation via Natural Language The dominant approach to
encoding observation 0; (e.g., webpages, video streams) has been to directly pass all input tokens
into an LLM to form continuous embeddings s7. While technically preserving all information, real-
world sensory readings often suffer from inherent noise and high variability (e.g., ads on a webpage,
varying weather and lighting conditions in video), which can make them brittle for reasoning over.
Human cognition, on the other hand, has evolved to counter this variability by categorizing raw
perception into discrete concepts [34], which are often encoded in language, symbols or structured
thoughts. Indeed, natural language is inherently hierarchical, capable of encoding concepts from
concrete ones (e.g., apple) to highly abstract ones (e.g., religion). Discrete representations are also
complete in general [49], which ensures no information is necessarily lost in the compression process.

--- Page 6 ---
= World Model

continuous or discrete
internal representations

SS Belief (9) 2d Goal

agent estimation
of world state

Observation

sensory data contains
incomplete information

Chol
3) Encoder

Figure 3: An agent in real world where groundtruth world state and universe are unavailable to
experience or experiment, so world model is crucial for simulation. As discussed in separation
of simulated actions a’, for planning and concrete actions a; for execution facilitates transfer and
hierarchical planning, leading to more diverse and grounded actions which lead to better task success.

Implementing this form of perception, we propose to represent the world state 5; using a discrete
natural language summary s¢ generated by an encoder LLM h, formally expressed as below:

Ni
pn(S¢ | or) = | J pn(Se, | $1, =k — Action —> Eas!
aS) @) $38 (a)

Encoder Planner Actor
Pi | 0) Simulative Reasoning Pa(a | 3, a"*)
wy. Plan Next State > To
5 Leanaiates= ¢ Cs — —Predictions—> ain poss

({a} a, p> @ — ((35,55,-..)) > ; aes

Policy World Model
A at A
pz(a’ | 8) ps(S" | 3, a")

Optimize.

Figure 4: Optimal agent architecture design with conditional probability annotation

best simulated action a/* through the planner. Inside the planner, the architecture performs simulative
reasoning by proposing actions aj, using policy m and predicting the next state §,,, using the world

model f, and evaluating goal progress an i ' vnr(g, Sk) + rV2 (S77) using critic v upon reaching
state 8 at the planning horizon T’. This can repeat multiple times until the planner selects the
action sequence a/*,_, with the highest expected success and passes the first step af to actor v
which finally outputs the concrete action a,;. Formally, SIMURA can be seen as solving the following
multi-level optimization problem:

5, = argmax pp(S | 02) (Perception)

3 —SS__-’

encoder
T’-1
dir) = argmax SS? v(8r) [] pr Gr+s | Se, ap) (Planning) (8)
Qe p 1 8 . nant k=t —
Ne t+l:T’ critic world model
sampled from policy 7

ay = arg max p,(a | 4, a") (Acting)

a ——~_-__—_—"

actor

In practice, we implement each of these components by zero-shot prompting pretrained LLMs. While
these LLMs alone are often insufficient for many complex agentic tasks, SIMURA’s divide-and-
conquer approach combines existing LLM strengths like instruction-following, summarization, reflec-
tion, and tool use to allow agentic behavior to emerge. Benefiting from massive web-scale pretraining
on next-token prediction p(x; | 2! __Belief_, 850 _Simulated ~
aad State Say Action WS
User Encoder Planner Actor
"Find the cheapest "The current webpage "Select the "click('128')"

is a Google flight
interface that has

one-way flight from ‘Cheapest' tab"

Boston to Los Angeles
next Wednesday"

Figure 5: LLM-based implementation of our proposed agent model for web-related tasks (e.g. multi-
website QA, flight search, etc). Planner is where we implement our proposed world-model-based
planning. We also implement a baseline that simply samples the plan from a language model (i.e.,
autoregressive planning).

many tasks in real life (e.g., gather information, book travels, and submit applications). Whereas
many existing products do access the internet etc.], they typically use specialized tools (e.g.,
search engines and data APIs) to capture a subset of web browser capabilities (i.e., reading) while
falling short of the full functionality (e.g., access content not exposed to search engines or predefined
APIs like flight and hotel databases). We argue that an agent that takes advantage of the full browser
will push the envelope in AI’s abilities to serve human needs.

Despite the richness and flexibility, the web browser is a highly challenging environment for agentic
reasoning due to its immense complexity, long-horizon nature, partial observability, and multimodal-
ity (3) (58). We evaluate our architecture in 3 types of web browsing tasks: 1) complex website
navigation, 2) multi-hop, multi-website QA, and 3) general web automation. For the baselines, we
compare against:

1. BrowsingAgent from OpenHands [35], a representative open-web agent which generates
chain-of-thought before selecting an action

2. SIMURA (our architecture) with autoregressive planning (i.e., commit to the first sample
from our policy module) instead of our proposed simulation-based planning with world
model. Formally, the planning process is simplified to the following:

a, = arg max pi (a | $1)
at

Implementation for Web Browsing Figure [5|presents our implementation when applied to web
browsing. We use prompts tailored to the web environments in this example, but plan to extend to
other environments and move towards training a single agent model that can act optimally in all
environments, which is an exciting next step. At each step t, the agent receives the observation 0; as
the HTML-based accessibility tree visible through the browser’s viewport (an example is provided in

--- Page 9 ---
Web Task Performance vs. Agent Architecture

Ours (World Model Planning) {™§ Ours (Autoregressive Planning) {)) OpenHands BrowsingAgent
04

32.2%

0.3

0.2

0.1

Success Rate / Accuracy

0.0
Complex Website Multi-Hop, Multi-Website General Automation
(e.g., FlightQA) (e.g., FanOutQA) (e.g., WebArena)

Figure 6: Overview of performance comparison between SIMURA and baselines. The full architecture
shows clear advantage over the baseline BrowsingAgent, improving the performance on complex
website navigation from 0% to 32.2%. Our proposed world model reasoning for planning also
consistently improves over simple planning with autoregressive LLM by up to 124%.

Appendix[A). The agent then uses encoder LLM h to summarizes the observation as $; ~ pp(- | 04),
and then add it to a selective memory of past summaries and simulated actions {m(8q, aj") }/—4
to form the estimated world state §; = [m(51,a{"),...,m(S:-1, af*.,), S| for planning. During
planning, we sample / simulated actions a}, from the policy 7, cluster them into distinct actions,
and use the world model f to predict the next summary as 5:41 ~ pr(- | S¢, a) to form the next state
Se41 = [m(S1, af*),...,m(S¢, a;), 5441]; this repeats until the planning horizon T’. To evaluate the
terminal state $7 with critic v, we prompt the LLM to generate qualitative answers and convert them
into numerical scores (e.g., “success” receives a score of 1), and repeat for N times to capture the
fine-grained differences between states. Following previous work [59] [33], we set M = N = 20 and
T =t+ 1, and use DFS as the search algorithm. We implement the planning process using LLM
Reasoners [[19]], a library for LLM-based complex reasoning using advanced algorithms. After the
planner selects the simulated action a}*, we update the memory with m(5,, a/*). For the actor v, we
additionally include the observation text o, in the prompt to ensure the action grounding. All the
prompts are included in Appendix[B]

Overview of Results An overview of our results is presented in Figure|6] Across all 3 categories
of tasks, our architecture shows a clear advantage over the baseline BrowsingAgent, specifically
increase the success rate on complex website navigation from 0% to 32.2%. Our proposed world
model reasoning for planning also consistently improves over simple planning with autoregressive
LLMs by up to 124%. In the subsections below, we will introduce the evaluation settings and discuss
the results in more detail.

4.1 Complex Website Navigation

A distinguishing feature of web agents is the ability to gather live information (e.g., flights, stock
prices, social media) not present in the training data of foundation models due to its rapid update [60].
For many questions (e.g., the price of the earliest flight tomorrow), LLMs without such grounding
often result in hallucinations (see Figure [7] for an example). In practice, however, obtaining the
information is challenging, as many websites are very complex and difficult to navigate (e.g., execute
a flight search query on a travel website and filter through the results), which calls for superb reasoning
skills on the part of the agent.

Dataset Due to the lack of benchmarks for complex website navigation on the open internet, we
created our own dataset using flight search as an example, since it is an important part of travel
planning, a major use case for web agents. We formulate flight search as the user asking for a flight
satisfying a list of constraints (e.g., one-way, from New York to Los Angeles), and the agent operating
a flight search engine to find such a flight and returning it to the user. To evaluate the agent’s reasoning

--- Page 10 ---
What is the earliest-arriving flight tomorrow from One-way Pittsburgh X + © Zurich X + Wed 12/18 <> adult Economy 2 |
Pittsburgh to Zurich?

Q Save || > Share $0 go

Zurich Airport (ZRH). The earliest available flight on Wednesday, = 515 am~6:l0.am" a stops teh s5m $1,601
“ * Economy
December 18, 2024, departs Pittsburgh at 7:00 AM and arrives in & Spirit Airline Iss MCO, EWR-JFK  PIT-ZRH Mytrip

Zurich at 10:45 AM the following day, December 19, 2024, with two
iew Deal

stops en route. «vax

G There are no direct flights from Pittsburgh International Airport (PIT) to

Please note that flight schedules are subject to change. For the most

A a . , Ss Sh 1 @
accurate and up-to-date information, it's advisable to check with the ses) [Sie ae

airlines directly or use reliable flight search platforms. ‘
¥ 9 p 3:01 pm - 7:40 am” 1stop 10h 39m $1,744
inited Airlines IAD T-ZRH Economy
Sources @ United Airlines
; View Deal
Hw Ade sy

Figure 7: Faced with the question “What is the earliest-arriving flight tomorrow from Pittsburgh to
Zurich?” ChatGPT-40 browsed the frontpage of Kayak.com and hallucinated a flight that arrives at
10:45am on the following day as the answer (left). Performing the search on Kayak.com, however,
shows that the earliest-arriving flight lands in Zurich at 6:10am on the next day (right). The question
was asked on December 17th, 2024.

Number of Constraints: 3

v ean ae a
Repeat , : FlightQA Dataset \
. { +
N Times Constraint Generation (N x (K+1) Questions)
\
Y ( )
['round-trip', Could you find a round-trip |
‘San Francisco to Paris', —| [>| flight from San Francisco to |
‘departing in two weeks'] Paris departing in two weeks? |
* Questi
5 3 luestion
Repeat Constraint Extension ° 1
K Times Generation |
!
y
['round-trip', Could you find a round-trip |
‘San Francisco to Paris', flight from San Francisco to |
‘departing in two weeks', Paris departing in two weeks and |
‘returning after a 10-day stay'] ) | returning after a 10-day stay? |
\ /
N /

Figure 8: Illustration of the data generation process for the FlightQA dataset. We first prompt a LLM
to generate N lists of C' starting constraints (Constraint Generation). Then, we prompt the LLM
to iteratively add constraints to the lists one by one, repeating for At times (Constraint Extension).
Finally, we prompt the LLM to convert each constraint list into a question in natural language
(Question Generation).

ability, we further produce questions with varying number of constraints by iteratively adding to
the list, which enables a counterfactual analysis that controls for the confounding effect of specific
constraint configurations (e.g., an agent with perfect reasoning should still be able to answer the same
question with one more constraint; an agent of rote memorization will likely fail when the question
changes slightly).

We illustrate our data collection process in Figure[8] To ensure scalability and controllability, we
prompt a LLM to first generate a list of C’ starting constraints, repeating for N times. After that,
we prompt the LLM to iteratively add constraints to the lists one at a time, repeating for K times.
Finally, we prompt the LLM to convert each constraint list into a question in natural language. In
practice, we set C = 3, N = 15, and K = 5, which results in FlightQA, a dataset consisting of 90
questions with 15 sequences of constraint lists where the number of constraints increases from 3 to 8.
We use gpt-4o to perform all the data generation steps. The initial question generation and question
expansion prompts are included in Appendix |C]

Evaluation Because FlightQA involves querying live information from the open internet, it is
impossible to establish ground truth answers due to the constantly evolving flight pricing and
availability. Inspired by previous work on evaluation for generated text [61], we propose to evaluate
the agent response based on two quality aspects: groundedness for whether the response is supported
by the interaction history and relevance for whether the response satisfies user constraints to the

10

--- Page 11 ---
Performance (%) Outcomes (%)

Response Browser Max Steps Repetitive Action
Method Correct Grounded Relevant}Returned Crashed Reached Actions — Errors
OpenHands BrowsingAgent 0.0.0.0.0 3.3.3 0.0 93.3
SIMURA (Ours)
Autoregressive Planning 14.4 15.6 14.4 16.7 0.0 37.8 44.4 1.1
— with ol? 1.1.1.1.1 11.1 40.0 37.8 10.0
— with 03-mini’ 3.3 4.4 3.3 4.4 3.3 51.1 32.2 8.9
World Model Planning 32.2" 36.7 = 32.2 38.9 11 40.0 18.9 11

Table 1: Performance and outcome statistics for the FlightQA dataset. Our architecture increases the
correct rate from 0% in OpenHands BrowsingAgent to 32.2%. Reasoning by world model simulation
also clearly outperforms autoregressive reasoning by 124%. ** indicates being significantly higher
than the second-best method at the statistical significance level of 0.01 (p  and .

22

--- Page 23 ---
Prompt for Policy
{memory }

# Current State:
{state }

# Intent:

Describe the action the assistant should take next to carry out the user’s instruction. Avoid
using phrases such as "To accomplish the goal," "I will," "To proceed.". Avoid ending with
phrases like "to execute the search." Describe one action at a time and avoid combining
multiple steps. Refrain from mentioning specific element IDs as they may change during
execution. Limit your response to one phrase and include any details that help select the
correct action. Be creative and propose novel methods to achieve the goal. Avoid creating
accounts without user permission or providing personal information. Concrete example
would be "Go to the home page of Google Flights." and "Click on the ’Search’ button."

Wrap your response in the following format:


Your thoughts and reasoning process




Description of the action to perform next


Prompt for World Model

{memory }

# Current State:
{state }

# Memory Update:
{memory_update }

# Action Intent:
{plan}

# Next State:

Describe all the elements in the webpage after the agent attempts to carry out the intent.
Note that the execution may not be successful, so you will have to infer the result of the
action. Note any dialogs, progress indicators, or error messages. Include any interactive
elements and their values or if they are blank. Note any detailed information such as facts,
entities, or data that are relevant to the task. Report any error messages displayed. Try to be
as comprehensive and detailed as possible.

Wrap your response in the following format:



Follow the format of the current state description. Use present tense. Avoid starting phrases
like "Based on the interaction history, current state, and current intent”.



23

--- Page 24 ---
Prompt for Critic
{memory }

# Final State:
{state }

# Task Success and Progress:

Your task is to evaluate the performance of the agent. Given the agent’s instruction, interaction
history, the final state of the webpage, and the agent’s responses to the user if any, your goal is
to decide whether the agent’s execution is successful or not. If the current state is a failure but
it looks like the agent is on the right track towards success, you should also output as such.

Wrap your response in the following format:


Your thoughts and reasoning process



"success" or "failure"




"yes" or "no"


Prompt for Memory Update

{memory }

# State:
{state }

# Action Intent:
{plan}

# Memory Update:

Summarize the changes in the webpage observation that should be remembered for achieving
your goal and for predicting the next state. Note any new elements, any elements no longer
visible, or any changes in the content of existing elements. Also note if there is no change.
Include any other inferred information that may help you decide the next action, such as
whether an action intent is successful, or whether progress has been made or reversed. Do not
include your next planned actions. Revise your belief from previous history if the current
state contradicts it.

Wrap your response in the tag  and .

24

--- Page 25 ---
Prompt for Actor

25

--- Page 26 ---
Prompt for Action Clustering

Here is the action space for a browser agent to navigate in a webpage:
16 different types of actions are available:

noop(wait_ms: float = 1000)

send_msg_to_user(text: str)

scroll(delta_x: float, delta_y: float)

fill(bid: str, value: str)

select_option(bid: str, options: str | list[str])

click(bid: str, button: Literal[left?, ’middle’, ‘right’] = ‘left’, modifiers:
list{typing.Literal[’ Alt’, Control’, Meta’, ’Shift’]] = [])

dbiclick(bid: str, button:  Literal[left’, ’middle’, ‘right’?] = ‘left’, modifiers:
list{typing.Literal[’ Alt’, Control’, Meta’, ’Shift’]] = [])

hover(bid: str)

press(bid: str, key_comb: str)

focus(bid: str)

clear(bid: str)

drag_and_drop(from_bid: str, to_bid: str)
upload_file(bid: str, file: str | list[str])
go_back()

go_forward()

goto(url: str)

Only a single action can be provided at once. Example: fill@al2’, *example with

Below, you will find lists of intents, or natural language descriptions of actions that,
when executed, will translate to one of the function calls above. The intents will be provided
in the following JSON format:

cece

{

json
"intent_id": "intent description"

cece

Your task is to cluster list of intents into semantically equivalent groups, where each group
represents intents that lead to the same action when executed (i.e., navigating to the Google
homepage is translated to goto(’ https://www.google.com’)) and would therefore correspond
to the same API call in a Playwright browser. Intents that use different wording but convey
the same action should be grouped together. Try to minimize the number of clusters.

26

--- Page 27 ---
Prompt for Action Clustering (Continued)

Represent the clustering results using a JSON object where each cluster has a unique identifier,
and each identifier maps to a list of actions in that cluster. See below for an abstract example:

cece

{
"cluster_id": {
"intent": "representative intent name for this cluster",
"candidates": [
"

json

Concrete Example 1:

Dictionary of Intents:

*¢¢4Fson
{

"QO": "Navigate to the Google homepage by entering its URL.",

"1": "Go to the Google homepage.",

"2": "Go to the Google homepage",

"3": "Go to the Google homepage by navigating to
*https://www.google.com’",

"4": "Go to the home page of Google"

cece

["Navigate to the Google homepage by entering its URL.", "Go to the Google home-
page.”, "Go to the Google homepage", "Go to the Google homepage by navigating to
https://www.google.com", "Go to the home page of Google"]

Clustering Results:

cece

{
"cluster_1i": {

"intent": "Navigate to the Google homepage",
"candidates": [0, 1, 2, 3, 4]

json

}

cece

Concrete Example 2:
Dictionary of Intents:
{action_candidate_json}

Clustering Results:

27

--- Page 28 ---
B.1 Adaptation for WebArena Evaluation

Agent Description for WebArena Evaluation

An information and automation assistant that interacts with the browser and responds to user
instructions. The response follows the following rules: 1. When the intent is a question, and
a complete answer to the question has been found, then send the answer to the user; 2. the
intent wants to locate specific information or navigate to a particular section of a site, and

the current page satisfies, then stop and tell the user you found the required information; 3.
the intent want to conduct an operation, and has been done, then stop and tell the user the
operation has been completed.

The assistant should try to achieve the goal in the current site without navigating to sites like
Google. Be forthright when it is impossible to answer the question or carry out the task. The
assistant will end the task once it sends a message to the user.

28

--- Page 29 ---
C_ Prompts for Generating and Evaluating on the FlightQA Dataset

Prompt for Generating Initial Constraints and Questions

System:
You are a creative writer who is an expert at crafting questions to help train assistants who
answer user queries. Current date and time: {current_datetime }

Instruction:

Your task is to create a robust benchmark for evaluating an AI’s ability to search for flights
through a platform like Google Flights. To ensure the dataset effectively represents real-world
use cases. Here are some important factors to consider:

1. Diversity of Queries

- Range of Destinations: Include both common and obscure destinations to test how well the
model handles varying levels of demand.

- Dates and Duration: Include different date ranges, including last-minute flights, peak travel
dates (like holidays), and extended trips. Ensure there’s a variety in trip duration as well.

- Passenger Variability: Include solo travelers, families, and group travel (e.g., one adult vs.
two adults and two children) since these change the search parameters and price results.

- Class and Preference: Vary preferences like cabin class (economy, business, first) and filters
(non-stop, one stop, preferred airlines, etc.).

- Budget Constraints: Test price sensitivity by setting different budget limits to see how well
the AI handles trade-offs.

2. Complexity of Requirements

- Multi-Leg Flights: Add queries for multi-city trips or those requiring complex layovers.

- Dynamic Constraints: Include queries with dynamic constraints, such as “find the cheapest
flight but depart between 8-10 AM,” to see if the model can adapt its search within specific
time frames.

- Conditional Preferences: Test cases where users might want options based on multiple
conditions, like “either find the cheapest non-stop or the shortest two-stop option.”

In practice, the questions typically vary in the following dimensions:
- Ticket type (round-trip, one-way, etc.)
- Routes (origin and destination)

- Layover location(s)

- Dates (departure and/or return)

- Flight time (departure and arrival)

- Total flight time

- Airlines

- Cabin class (economy, business, etc.)
- Aircraft

- Eco-friendly options (CO2 Emissions)

Given a number of constraints, you should first provide a list of constraints, with
the number of constraints equal to the specification. After that, you will generate
a question a typical user will ask which imposes those constraints. You should re-
peat this for at least 7 times to generate a set of questions with simple language. Make
sure that the number of constraints in the question matches the number of constraints specified.

Do not include constraints about the number of passengers. If the constraint is a
date, you can use relative dates (e.g., "tomorrow", "next month", "after 8 PM", etc.). Avoid
using specific dates like "December 25th" to ensure the questions are relevant throughout the
year.

Your response should follow the JSON format below:

29

--- Page 30 ---
Prompt for Generating Initial Constraints and Questions (Continued)

Number of Constraints: 

{
"num_constraints": ,
"questions": [
{
"constraints": [],
"question": 

},

Below is a concrete example:

Number of Constraints: 3

{
"num_constraints": 3,
"questions": [

{

"constraints": ["one-way", "New York to London",
“departing next Friday"],
"question": "Can you find a one-way flight from New York
to London departing next Friday?"

30

--- Page 31 ---
Prompt for Iteratively Expanding Constraints and Questions

System:
[Same as above]

Instruction:
[Same as above until “Your response should follow]

Your response should follow the JSON format below:
Maximum number of constraints: 

Starting constraints and questions:

{
"num_constraints": ,
"constraints": [],
"question": 

}

Questions with increasing complexity:

{
"questions": [
{
"num_constraints": ,
"constraints": [],
"question": 

"num_constraints": ,
"constraints": [],
"question": 

},

(continue until reaching the maximum number of constraints)

}

Your Response:

Maximum number of constraints: {max_num_constraints }
Starting constraints and questions:
{starting_constraint_questions}

Questions with increasing complexity:

31

--- Page 32 ---
Prompt for Evaluation

# Interaction Date and Time:
{interaction_datetime }

# Interaction History:

[Concatenation of observations from all steps]

Above are the webpages an assistant interacted with while trying to answer the
user’s query.

The user is looking for flights with the following constraints:

{constraints }

Here is the exact query provided by the user:

{goal}

Here is the assistant’s response:

{message}

Your task is to evaluate two aspects of the response:

1) Whether the assistant’s response is supported by the interaction history, and

2) Whether the assistant’s response satisfies the user constraints to the extent al-
lowed by the results.

Some Context:

- To determine the seating class of a flight being returned, refer to the value of the
"Change seating class" combobox.

- It is not always possible to satisfy all the user constraints. In this case, examine whether the
response is as close to the user constraints as possible.

Answer in the following format:


Your thoughts and reasoning.



Your assessment of whether the response is supported by the interaction history. Answer
"yes" or "no"




Your assessment of whether the response satisfies the user constraints to the extent allowed by
the results. Answer "yes" or "no"



32

--- Page 1 ---
arX1v:2507.23776v1 [cs.CL] 31 Jul 2025

Cascaded Information Disclosure for Generalized Evaluation of Problem
Solving Capabilities

Yunxiang Yan

Tomohiro Sawada

Kartik Goyal

College of Computing
Georgia Institute of Technology
{ryan.yunxiang.yan, tsawada, kartikgo}@gatech. edu

Abstract

While question-answering (QA) benchmark
performance is an automatic and scalable
method to compare LLMs, it is an indirect
method of evaluating their underlying problem-
solving capabilities. Therefore, we propose a
holistic and generalizable framework based on
cascaded question disclosure that provides a
more accurate estimate of the models’ problem-
solving capabilities while maintaining the scal-
ability and automation. This approach collects
model responses in a stagewise manner with
each stage revealing partial information about
the question designed to elicit generalized rea-
soning in LLMs. We find that our approach
not only provides a better comparison between
LLMs, but also induces better intermediate
traces in models compared to the standard QA
paradigm. We empirically verify this behav-
ior on diverse reasoning and knowledge-heavy
QA datasets by comparing LLMs of varying
sizes and families. Our approach narrows the
performance gap observed in the standard QA
evaluation settings, indicating that the prevalent
indirect QA paradigm of evaluation overesti-
mates the differences in performance between
models. We further validate our findings by
extensive ablation studies.

1 Introduction

While general-purpose LLMs have become ubiqui-
tous today, evaluating them holistically remains
a massive challenge. Primarily, these models
are compared against each other by their per-
formance on a handful of benchmark tasks and
datasets (Liang et al., 2023; Srivastava et al.,
2023; Wang et al., 2019; Sarlin et al., 2020) dom-
inated by various kinds of question-answering
tasks (Hendrycks et al., 2021; Lin et al., 2022;
Rajpurkar et al., 2016; Rein et al., 2023). While
objective and scalable, to estimate the underlying
problem-solving capabilities of models, this ap-
proach is indirect — merely judging a model on its

ability to pick the correct choice among distrac-
tors or to produce a correct numerical answer to a
mathematical problem doesn’t measure the qual-
ity of strategies these models use to arrive at that
answer. There is a pressing need (Alzahrani et al.,
2024; Gan et al., 2024; Li et al., 2024b) for direct
evaluation of problem-solving capabilities as they
are better aligned to the actual use-cases of LLMs.
For example, while the GPQA dataset (Rein et al.,
2023) contains multiple-choice questions (MCQs)
that indirectly test the graduate-level knowledge of
the models, the corresponding real-world use-case
is more subjective — the model should possess the
ability to have an accurate open-ended conversa-
tion with the user about advanced concepts. This
ability is better reflected in the strategies that the
LLM uses to answer GPQA questions. Hence, the
focus of this paper is to directly evaluate the inter-
nal problem-solving capabilities of the models in a
holistic, yet scalable and generalizable manner.
We propose an evaluation framework called cas-
caded information disclosure which focuses on elic-
iting and estimating the problem-solving capabili-
ties reflected in the intermediate traces of the mod-
els. It involves modification of standard QA tasks
as follows. We first transform the question into its
more generalized form and then design a scheme
to partition this form into non-overlapping parts.
Then we disclose these parts to the model under
evaluation in a stagewise manner. While this frame-
work accommodates multiple stages/parts, the final
stage in all instantiations is a verifiable projection
stage that projects the model’s responses in the
earlier stages to an automatically evaluatable/veri-
fiable form. This progressive question disclosure
leads to a more accurate assessment of the sub-
jective problem-solving capabilities of the models
while the verifiable projection stage maintains the
scalability and automation of evaluation. We instan-
tiate this framework for two distinct QA modes: 1)
multiple-choice QA, and ii) reasoning-based math

--- Page 2 ---
Q Stem: A small iron block is

dropped from a height of 30m.

What is its velocity at 10m?

Options:

A) (> 10 m/s) and ( 19m/s) and ( 49 km/hr) and (... 

... 

 | need to first convert all
options to m/s for ease of comparison...Option A
satisfies stage-1 answer.  B 

think>

Answer>

A | will use Maxwell’s electromagnetic equations and terminal velocity... 

mass and options to answer. 

Subjective eval: LLM-judge
Q Stem...

.. 

... 

..Stage-1 answer is correct
because you need the knowledge of the options
to answer.

Correct

vv

Figure 1: Demonstration of MCQA instantiation of our Cascaded Information Disclosure evaluation framework
against Standard evaluation and LLM-based Subjective evaluation of problem-solving capabilities. The standard
evaluation paradigm picks the correct option but has poor justification for the answer. Cascaded Information
Disclosure strategically generalizes the question and exposes parts of it to the model in a stagewise manner. The
free-form response generated during ideation phase reflects model’s poor problem-solving ability. LLM-based
subjective evaluation is an appealing alternative to judge the ideated response but fails in the example shown —
it supports the trivial conclusion in the incorrect ideation. Verifiable Projection while objective and automatic,
correctly penalizes the incorrect ideated response by picking the wrong option.

word problems.

Comparing a wide variety of models of different
sizes and families (§ 3.1), our approach narrows the
performance gap observed in the standard evalua-
tion settings, indicating that the prevalent indirect
evaluation paradigm overestimates the differences
in performance between models. Overall, we find
that our approach not only provides a better esti-
mate of LLMs’ underlying reasoning capabilities,
but also elicits better performance and intermediate
traces from the models compared to the standard
QA paradigm — an effect primarily driven by the de-
composition of the questions into separate foci. We
further validate the soundness of our observations
by extensive ablation studies.

To summarize, we i) propose a novel generalized
framework to test the problem-solving capabilities
of LLMs that supports automation and extensibility,
ii) empirically study our framework via concrete
automatic instantiations across multiple QA tasks
and LLMs, and iii) create manual instantiations of
this framework by annotating two datasets that will
be publicly available.

2 Cascaded Information Disclosure

The central guiding conjecture (Wood et al., 1976;
Collins and Stevens, 1983) behind our approach

is that an answerer (human or machine) will em-
ploy more detailed reasoning and knowledge-based
strategies and reveal more information in their an-
swers if the question is broken down into a series
of leading questions. Therefore, our approach in-
volves exposing partial information about the ques-
tion to the answerer model in a cascade of stages
and eliciting potentially detailed and meaningful re-
sponses from it at each stage. More concretely, the
input question Q is first converted into an abstract
form Q. The abstract question is then decomposed
into n non-overlapping subsets Q = LJ;_, Q;. Our
approach then solicits responses R; from the model
by presenting these subsets in a predetermined or-
der and conditioning each response on the corre-
sponding subset and the response in the previous
stage: R; + Q;, Rj_1. The response of the final
stage is considered to be the model’s answer to
the full question: A(Q) = Rn | Qn, Rn—1. Cru-
cially, the response at any stage doesn’t explicitly
depend on the subparts of the question at the ear-
lier stages — the only information the model can
gather about the previous stages is through the re-
sponse generated one stage prior. This promotes
detailed and informative responses from the model
at each stage which, as we show in our results,
is the primary driver for eliciting better problem-
solving strategies from the models. An appropriate

--- Page 3 ---
decomposition procedure underpins the success of
our approach and needs to be custom-designed for
every instantiation of this abstract framework.

In our experiments, we instantiate this frame-
work for two different types of questions: multiple-
choice questions, and math word problems. While
the instantiations are procedurally different, there
are some commonalities between the two instantia-
tions. Both of them decompose the question into
two stages (n=2) that we call generalized ideation
and verifiable projection stages. The generalized
ideation stage presents a partial generalized view
of the original question to the model such that the
model generates a broad and general response Rj.
The verifiable projection stage takes this response
along with the rest of the information in the orig-
inal question (question residue) to project R; to
an objectively measurable answer format. There-
fore, the generalized ideation stage produces the
subjective response detailing the problem-solving
strategies employed by the models, and the veri-
fiable projection stage ensures that the evaluation
remains automatic and scalable by using standard
evaluation metrics for the projected response.

2.1 MCOQA Instantiation

We break the MCQ into two parts: the question
stem, and the options containing the answer. There-
fore, the generalized ideation stage only gets to
look at the general question without the knowledge
of the answer candidate and induces the model
to produce a detailed constructed response (Liv-
ingston, 2009) to answer the question in a free-
form subjective manner. This response is then
passed, along with only the candidate options,
to the verifiable projection stage. This stage as-
sesses the model’s free-form response and picks
the option that most appropriately matches the
response. We conduct experiments on different
MCQA datasets designed to test for reasoning and
advanced graduate-level knowledge.

2.2 Math Word Problem Instantiation

The original math word problems are rewritten into
a templated form that is more general and specific
entities are replaced by abstract variables. This
generalized question is presented to the model in
ideation stage in which the only correct way to
answer the question is to reason about the prob-
lem and unknown variables, and produce a general
mathematical formula for the generalized question.
The verifiable projection stage takes the ideation

response and the variable assignment as input. The
mathematical formula is turned into executable
code that accepts the variable assignments as ar-
guments to yield a numeric answer to the origi-
nal question. This instantiation requires consider-
able manual intervention. Therefore, we use the
GSMS8K (Cobbe et al., 2021) dataset for original
questions whose generalized form is constructed us-
ing templates in prior work (Mirzadeh et al., 2024).

3 Experimental Setup

We empirically compare the effectiveness of the
cascaded information disclosure evaluation frame-
work against other prevalent frameworks by eval-
uating a variety of open-weight models span-
ning various sizes (7B to 32B) and model families
(Llama (Grattafiori et al., 2024), Qwen (Qwen et al.,
2025), Gemma (Team et al., 2024), and Phi (Abdin
et al., 2024)). Specifically, we use the instruction-
finetuned checkpoints for the models that are capa-
ble of following instruction prompts in a zero-shot
QA setting. (Details in Appendix)

Datasets: As described in§ 2, we instantiate our
cascaded information disclosure framework for
problem-solving evaluation for two types of ques-
tions: MCQs and math word problems. For the
MCQA format, we picked a) the test split of ARC-
Challenge (Clark et al., 2018) dataset which con-
sists of grade school level science questions for
MCQA format and, b) the GPQA dataset which
consists of challenging graduate-level science ques-
tions written by domain experts. Specifically,
we use the high-quality subset of the full GPQA
dataset: GPQA-Main (448 questions), and its more
challenging subset GPQA-Diamond (198 ques-
tions). It is reported in the GPQA paper (Rein et al.,
2023) that highly skilled non-expert human valida-
tors only reach 30.4% and 22.1% accuracies on
GPQA-Main and GPQA-Diamond despite spend-
ing on average over 30 minutes with unrestricted
access to the internet. For math word problems,
we use GSM8K to obtain original questions. Previ-
ous work Mirzadeh et al. (2024) created 100 sym-
bolic templates from 100 questions in the original
GSM8K dataset by manually annotating the enti-
ties (numeric values and categorical values) that
can be abstracted into variables. We extend these
templates by creating consistent assignments to the
variables in the templates, resulting in a derivative
dataset we call GSM-General' consisting of 100

'We will make this converted symbolic version of GSM8K

--- Page 4 ---
template-variable assignment pairs that map to the
100 original questions in GSM8K dataset.
Inference Details: For prompt engineering, we
adopt zero-shot, chain-of-thought prompting (Wei
et al., 2023) to mimic the most common real-world
LLM use case. Prompt templates are available in
appendix 7.2. We specifically crafted the prompts
for all the settings including the baseline methods
so as to achieve maximal performance on the tasks
under each method. Specifically, we found that
describing our cascaded setup for question disclo-
sure to the models via the prompt resulted in stable
behavior under our approach. We use XML tags
based answer format in both prompting and answer
extraction. We use greedy decoding which is the de-
fault setting for all three selected datasets. For each
model, we set the context length to the maximum
supported by the model and we set a uniform cap
of 8192 for maximum number of generated tokens
to facilitate a fair comparison across models.

3.1 Evaluation Strategies Compared
3.1.1 Standard Evaluation

This evaluation setting is the most widely used
evaluation method for the datasets we work with.
We further enhanced the standard evaluation by
developing custom prompts and answer extrac-
tion methods to ensure maximal performance on
the datasets that would serve as difficult baselines
for our approach to beat. For MCQA type, the
models take in the full original question: stem
(What is Newton’s Third Law of Motion
about?) and candidate options (A: Inertia, B:
Force and acceleration, C: Momentum, D:
Action-Reaction) as inputs, and provide a re-
sponse from which the answer index (A, B, C,
or D) is extracted.”

A math word problem is a mathematical exer-
cise presented as a scenario, requiring the applica-
tion of mathematical reasoning and computation
to solve a real-world or hypothetical situation and
provide a numerical answer. For evaluation, the
prompt contains requirements on the format of the
final answer. We use Im-evaluation-harness’s im-
plementation to evaluate models’ performance on

dataset, GSM-General publicly available.

"We use two implementations of standard MCQA base-
lines: LM Evaluation Harness ((Gao et al., 2024); denoted
“LMH”) and a custom implementation. We found that LMH’s
simple prompts and answer extractions led to high parsing
failure rate. To provide a fairer comparison of model perfor-
mance, we develop an advanced MCQA baseline that greatly
improves model’s instruction following on formatting.

the 100 original GSM8K questions.

3.1.2 Cascaded Information Disclosure

As described above, we instantiate both the ques-
tion types with two stages: generalized ideation and
verifiable projection. In the generalized ideation
stage, we break the original question (or its trans-
formed abstract form) into a generalized question
and question residue. In the MCQA setting, we
construct the generalized question by concealing
the options, which will then be used as question
residue in the subsequent stage. As discussed in
§ 4.5, we also experiment with obtaining the gener-
alized question by concealing the options and man-
ually rewriting the question stem to make it option-
independent and general. In math word problem
setting, we construct the generalized question by
converting the original question into its symbolic
form that replaces entities and quantities in the orig-
inal question with abstract variables, leaving the
value assignment to the variables involved as the
question residue. In the prompt for generalized
ideation stage, we explicitly specify that reiteration
of the question is prohibited to restrict its leakage
into the next stage.

In the verifiable projection stage, we project the
generalized reasoning traces in the ideation stage
to either an option in MCQA or a numeric result
in math word problem through a projector. The
projector for MCQA tasks is an LLM whose task
is to pick the most relevant option based solely on
evaluation subject’s ideation stage traces (no ac-
cess to generalized question). We also explicitly
instruct the projector LLM to abstain from solving
the original problem and only focus on matching to
the correct option to ensure that the accuracy on an-
swers predicted by the projector provides a reliable
estimate of the quality of the open-ended reason-
ing trace generated in the ideation stage. Similarly,
the projector for the math word problem is only
provided the trace from the ideation stage and is
tasked with producing an executable Pythonic ex-
pression of the logic and formula expressed in the
ideation stage. This code is then executed with
the variable values to verify the correctness of the
answer produced. We experiment with different
kinds of projectors: a) the LLM being evaluated
as the projector (self), b) an external performant
open-weight projector (model Phi), c) a blackbox
frontier projector model GPT-40 (OpenATI et al.,
2024), and d) a rule-based projector that doesn’t
use any LLMs. For rule-based substitute of LLM

--- Page 5 ---
projector in MCQA, we implement a sentence-level
BLEU score metric (Post, 2018) for matching the
first stage traces to the ground-truth answer. In the
math word problem setting, we implement a func-
tion that extracts the expression stated in answer
tags from the ideation stage output. If executable,
this expression is used to produce the answer by
filling variable values from the question residue.

3.1.3 Subjective evaluation of Generalized
Ideation Traces

While verifiable projection stage makes our ap-
proach automatic and reliable, it is not the only pos-
sible automatic approach to evaluate the problem-
solving capabilities of the model elicited in the gen-
eralized ideation stage. We also compare our setup
against strategies for automatic subjective evalua-
tion of the first stage traces. We experiment with
LLM-as-Judge projector (Zheng et al., 2023) for
both MCQA and math word problem settings. The
LLM judge, armed with the ground truth answer,
is tasked to project the ideation trace to a binary
verdict "Correct" and "Incorrect".

3.2 Evaluation Metrics

Following the description above, we use the term
accuracy to refer to two kinds of metrics whose
usage is differentiated by context: a) objective ac-
curacy, which is a measure of match between the
ground truth answer and the projected answer, and
b) subjective accuracy, which is the rate of correct-
ness as judged by an LLM. Thus, the standard
baselines and systems employing the verifiable
projector use objective scoring, while LLM-as-a-
judge methods use subjective scoring that decides
whether the ideation trace is deemed correct or
not. Subjective accuracy is heavily biased towards
judge’s style and hence is not directly comparable
to objective accuracy. We also report the parsing
failure rate (in Appendix) of answer extraction. The
failure is caused by the answering model’s inability
to follow answer formatting instructions.

4 Empirical Analysis

4.1 Cascaded Information Disclosure
Narrows the Gap between Models

In Table 1, we present accuracies of LLMs eval-
uated on four different benchmark datasets using
various evaluation paradigms. We report three dif-
ferent LLM projectors for the verifiable projection
stage: Phi-4, GPT-40, and ideation model itself.

For each evaluation method, we measure perfor-
mance gap as the difference between the highest
and the lowest scoring model. We observe that
our approach results in a significantly narrower gap
between the models compared to the standard eval-
uation. For example, the gap under standard eval-
uation for GPQA-Diamond (37.4) is much higher
than the gap (12.1) when GPT-4o is used as the
verifying projector. Moreover, the more powerful
the verifying projector, the lower the performance
gap. This suggests that the standard evaluation
scheme prevalent today overestimates the differ-
ence between language model’s capabilities.

4.2 Cascaded Information Disclosure
Estimates Problem-solving Capability

Generalized questions make QA tasks harder:
The generalized question used in our framework is
more difficult than the original questions, by their
nature. For MCQA, the answer choices are omit-
ted in the generalized questions, thereby providing
the evaluation subjects with strictly less informa-
tion compared to standard evaluation methods. For
GSM-General, the question is transformed into an
abstract symbolic question, forcing the models to
utilize their reasoning capabilities.

The increase in difficulty is naturally reflected
by decrease in performance of larger models (14B+
parameters) on MCQA tasks in Table 1 under our
evaluation paradigm. A qualitative example demon-
strating this is provided as Phi-4’s raw reasoning
traces under standard MCQA 2 and cascaded in-
formation disclosure 3 for question #996 in ARC-
Challenge dataset. Under standard MCQA setting
where options are provided, Phi-4 only analyzes the
four options to get the correct answer whereas in
our evaluation setting, Phi-4 falsely identified "nu-
cleus" as not being part of the atom during ideation
and its ideation trace is projected to the Incorrect
answer by our verifiable projector correctly. The in-
crease in difficulty posed by our framework forces
models to engage with the task at a deeper level,
thereby contributing to superior evaluation of the
model’s problem-solving capabilities.
Generalized questions elicit better traces from
the models: By contrast, we observe the oppo-
site trend for the smaller models ( @\n\n",
"answer_symbolic” : "agelxmult-years”,
"ideation_traces": "\n1l. Let's denote the current age of {name2} as {

age2}.\n2. In {years} years, {name1} will be {agel} + {years} years old.\n3.

In {years} years, {name2} will be {age2} + {years} years old.\n4. We know that
in {years} years, {name2} will be {mult} times as old as {name1} is now, so we

can write the equation: {age2} + {years} = {mult} * {agel}\n5. We need to solve
this equation for {age2}.\n\n\n{age2} = {mult} * {agel} - {

years}",

"raw_symbolic_answer"”: "{age2} = {mult} * {agel} - {years}",
"cleaned_symbolic_answer”: "(mult * agel) - years”,

"projected_numeric_result”: 38,

"Imh_answer_trace”: "We need to figure out how old Angela is right now. We know

that in four years her sister Angela will be three times as old as he is now.

We also know that Angela is right now. We need to figure out how old Angela is

right now.\n\n\nLet me know if you have any other questions.”,
"Imh_filtered_answer"”: "[Linvalid]”

Listing 1: GSM8K Example. Evaluation subject is Gemma-2-9B

7.1 Qualitative Examples
7.1.1 Example from GSM8K Dataset

13

--- Page 14 ---
"id": "996",
"question”: "Which of these is not part of an atom?”,
"options”: [

"isotope”,

"proton”,

"nucleus”,

"electron”
J ’
"answer_index”: @,
"Phi-4_raw_output”: "\nTo determine which option is not part of an
atom, let's analyze each choice:\n\nA) Isotope: An isotope refers to variants of

a particular chemical element that have the same number of protons but
different numbers of neutrons. Isotopes are not a part of an atom itself; rather
they are different forms of atoms of the same element. Therefore, isotopes are
not a component of a single atom.\n\nB) Proton: Protons are positively charged
particles found in the nucleus of an atom. They are a fundamental part of the
atom's structure.\n\nC) Nucleus: The nucleus is the central part of an atom,
containing protons and neutrons. It is a crucial component of the atom.\n\nD)
Electron: Electrons are negatively charged particles that orbit the nucleus of
an atom. They are essential parts of the atom's structure.\n\nBased on this
analysis, the option that is not a part of an atom is A) isotope, as it refers
to different forms of atoms rather than a component of a single atom.\n\nA”,
"is_correct”: "Yes”

Listing 2: ARC-Challenge Example: Phi-4 Correctly Answers Question in Standard MCQA Setting

7.1.2 Example from ARC-Challenge Dataset

14

--- Page 15 ---
"id": "996",

"phi-4_ideation_traces”: "To determine which component is not part of an atom, we
need to understand the basic structure of an atom. An atom is the smallest unit
of ordinary matter that forms a chemical element. It consists of three primary

subatomic particles:\n\n1l. **Protons**: These are positively charged particles

found in the nucleus of an atom. The number of protons in the nucleus defines

the atomic number of an element and determines the element's identity.\n\n2. xx

Neutronsx*x: These are neutral particles, meaning they have no charge, and are

also located in the nucleus alongside protons. Neutrons contribute to the mass

of the atom and can influence the stability of the nucleus.\n\n3. **Electrons*x:
These are negatively charged particles that orbit the nucleus in various energy
levels or electron shells. Electrons are much smaller in mass compared to
protons and neutrons and are responsible for the chemical properties of an atom
-\n\nGiven this understanding, any component not fitting into these categories (
protons, neutrons, or electrons) would not be considered part of an atom.

Commonly, other particles or structures that are not part of an atom include:\n\

n- *xMolecules**: These are groups of two or more atoms bonded together.

Molecules are not part of a single atom but rather a collection of atoms.\n\n-

*xkIons**: These are atoms or molecules that have gained or lost electrons,

resulting in a net charge. While ions are related to atoms, they are not a

fundamental part of an atom's structure.\n\n- *xNucleus**x: While the nucleus is

a part of an atom, it is not a subatomic particle itself but rather a structure

within the atom that contains protons and neutrons.\n\n- **Quarks**: These are

fundamental particles that make up protons and neutrons. While quarks are part
of the subatomic particles, they are not considered part of the atom itself in
the context of basic atomic structure.\n\n- **xPhotons**: These are particles of
light and do not form part of an atom. They are involved in electromagnetic
interactions but are not components of atomic structure.\n\nBased on this
analysis, the correct answer would be a component that does not fit into the
categories of protons, neutrons, or electrons, such as a molecule, ion, nucleus,
quark, or photon.”,

"phi-4_ideation_stage_answer”: "A molecule, ion, nucleus, quark, or photon is not
part of an atom.”,
"verifiable_projector_raw_output”: "\nThe key conclusions from the '

Reasoning Trace' are that an atom consists of three primary subatomic particles:
protons, neutrons, and electrons. Any component that does not fit into these
categories is not considered part of an atom. The trace specifically identifies
molecules, ions, the nucleus, quarks, and photons as examples of components that
are not part of an atom.\n\nNow, let's evaluate each answer option:\n\n- «x
Option A: protonx*x\n - Protons are explicitly mentioned in the reasoning trace
as a fundamental component of an atom. Therefore, this option does not match the
conclusion that the component is not part of an atom.\n\n- *xOption B: electron
xx\n - Electrons are also explicitly mentioned in the reasoning trace as a
fundamental component of an atom. Thus, this option does not match the
conclusion that the component is not part of an atom.\n\n- *xOption C: nucleus
xx\n - The reasoning trace states that the nucleus is not a subatomic particle
itself but a structure within the atom that contains protons and neutrons.
Therefore, the nucleus is not considered a fundamental component of an atom in
the context of subatomic particles. This aligns with the conclusion that the
component is not part of an atom.\n\n- **xOption D: isotopex*x\n - Isotopes are
variants of a particular chemical element that differ in neutron number. While
isotopes are related to atoms, they are not a fundamental part of an atom's
structure. However, the reasoning trace does not explicitly mention isotopes as
an example of a component not part of an atom. The focus is more on subatomic
particles and structures like the nucleus.\n\nBased on the analysis, Option C (
nucleus) is the most consistent and closest match to the reasoning trace, as it
is explicitly identified as not being a subatomic particle and thus not a
fundamental component of an atom in the context of the trace.\n\nC",
"projected_answer_text”: “nucleus”,
"is_anwer_correct”: "No”

Listing 3: ARC-Challenge Example: Phi-4 Fails to Derive the Correct Answer in Cascaded Information Disclosure
Setting

15

--- Page 16 ---
7.1.3 Example from GPQA Dataset
Qwen2.5-7B’s answer under standard MCQA Evaluation

16

--- Page 17 ---
"resps”: "\nTo solve this problem, we need to understand the
Kraus representation of a quantum channel and how it relates to the given Kraus
operators. The Kraus operators \\(A_i\\) for a depolarizing channel are given as
:\n\n\\— A_@ = \\sqrt{1 - \\frac{3p}{4}} I \\IJ\n\\E A_1 = \\sqrt{\\frac{p}{4}} xX

\\J\n\\E AL2 = \\sqrt{\\frac{p}{4}} Y \\IJ\n\\E A_3 = \\sqrt{\\frac{p}{4}} Z
\\J\n\nThe depolarizing channel can be represented as:\n\n\\[ E(\\rho) = \\sum_i
A_i \\rho A_i*\\dagger \\]\n\nGiven the Kraus operators, we can substitute them
into the equation:\n\n\\[ E(\\rho) = A_@ \\rho A_@*\\dagger + A_1 \\rho A_1%*\\
dagger + A_2 \\rho A_2%\\dagger + A_3 \\rho A_3*\\dagger \\J]\n\nSince \\(A_@ =
\\sqrt{1 - \\frac{3p}{4}} I\\), we have:\n\n\\—E A_@ \\rho A_@*\\dagger = (1 - \\
frac{3p}{4}) \\rho \\J\n\nFor the other operators, we have:\n\n\\[ A_1 \\rho A_1
“\\dagger = \\frac{p}{4} X \\rho X \\J\n\\E A_2 \\rho A_2*\\dagger = \\frac{p
3{4} Y \\rho Y \\J\n\\L A_3 \\rho A_3*\\dagger = \\frac{p}{4} Z \\rho Z \\J]\n\
nCombining these, we get:\n\n\\[ E(\\rho) = (1 - \\frac{3p}{4}) \\rho + \\frac{p
3{4} X \\rho X + \\frac{p}{4} Y \\rho Y + \\frac{p}{4} Z \\rho Z \\J\n\nWe can
rewrite this as:\n\n\\[ E(\\rho) = (1 - \\frac{3p}{4}) \\rho + \\frac{p}{4} (xX
\\rho X + Y \\rho Y + Z \\rho Z) \\J]\n\nWe know that \\(X*%2 = Y*2 = Z*2 = I\\),
so:\n\n\\C X \\rho X = \\rho \\J\n\\E Y \\rho Y = \\rho \\J\n\\E Z \\rho Z = \\
rho \\J\n\nThus, the expression simplifies to:\n\n\\[L E(\\rho) = (1 - \\frac{3p
3{4}) \\rho + \\frac{p}{4} (\\rho + \\rho + \\rho) = (1 - \\frac{3p}{4}) \\rho +
\\frac{3p}{4} \\rho = (1 - \\frac{3p}{4} + \\frac{3p}{4}) \\rho = (1 - p) \\rho
+ \\frac{p}{3} (X \\rho X + Y \\rho Y + Z \\rho Z) \\J\n\nThis matches the form
given in option A.\n\nTherefore, the correct Kraus representation of the state
\\CEQ\\rho)\\) is:\n\n\\E EC\\rho) = (1 - p) \\rho + \\frac{p}{3} (X \\rho X + Y
\\rho Y + Z \\rho Z) \\J]\n\n\nA”

Listing 4: Qwen Custom Example

17

--- Page 18 ---
"reasoning”: "To derive the Kraus representation of the depolarizing channel
operation \( E(\rho) \), we need to ensure that the sum of the squares of the
Kraus operators equals the identity matrix. The given Kraus operators are:\n\n\L

A_®@ = \sqrt{1 - \frac{3p}{4}} I \J\n\E A_1 = \sqrt{\frac{p}{4}} X \IJ\n\E A_2 =
\sqrt{\frac{p}{4}} Y \J\n\[— A_3 = \sqrt{\frac{p}{4}} Z \]\n\nFirst, let's verify
that the sum of the squares of these operators equals the identity matrix \( I

\)e\n\n\— A_@*2 + A_142 + A_2%2 + A_342 = \left( \sqrt{1 - \frac{3p}{4}} I \
right)*2 + \left( \sqrt{\frac{p}{4}} X \right)*2 + \left( \sqrt{\frac{p}{4}} Y \
right)*2 + \left( \sqrt{\frac{p}{4}} Z \right)*2 \]\n\nCalculating each term
individually:\n\n\[L A_0*2 = \left( \sqrt{1 - \frac{3p}{4}} I \right)*2 = \left(
1 - \frac{3p}{4} \right) I \J\n\[E A_1%2 = \left( \sqrt{\frac{p}{4}} X \right)%*2
= \frac{p}{4} X*2 = \frac{p}{4} I \J\n\E A_2*2 = \left( \sqrt{\frac{p}{4}} Y \
right)*2 = \frac{p}{4} Y*%2 = \frac{p}{4} I \]\n\[E A_3%2 = \left( \sqrt{\frac{p
3{4}} Z \right)*2 = \frac{p}{4} Z*2 = \frac{p}{4} I \J]\n\nSumming these up:\n\n
\D A_@*2 + A_1%2 + A_2%2 + A_3%2 = \left( 1 - \frac{3p}{4} \right) I + \frac{p
3{4} I + \frac{p}{4} I + \frac{p}{4} I \J\n\0E = \left¢ 1 - \frac{3p}{4} + \fracf{
p}{4} + \frac{p}{4} + \frac{p}{4} \right) I \J\n\C = \left( 1 - \frac{3p}{4} + \
frac{3p}{4} \right) I \J\n\C = 1 I \J\n\C = I \J\n\nSince the sum of the squares
of the Kraus operators equals the identity matrix, the given Kraus operators
correctly represent the depolarizing channel operation \( E(\rho) \).\n\
nTherefore, the correct Kraus representation of the state \( E(\rho) \) is:\n\n
\C E(\rho) = \sum_{i=0}*{3} A_i \rho A_i*\dagger \]\n\nWhere the Kraus operators
are:\n\n\[L A_® = \sqrt{1 - \frac{3p}{4}} I \J\n\E A_1 = \sqrt{\frac{p}{4}} xX
\J\n\0C A_2 = \sqrt{\frac{p}{4}} Y \J\n\0 A_3 = \sqrt{\frac{p}{4}} Z \]",

"answer”: "\[ E(\rho) = A_@ \rho A_@*\dagger + A_1 \rho A_1*\dagger + A_2 \rho A_2*\
dagger + A_3 \rho A_3*\dagger \]\nWhere \( A_® = \sqrt{1 - \frac{3p}{4}} I \),
\C A_1 = \sqrt{\frac{p}{4}} X \), \C A_2 = \sqrt{\frac{p}{4}} Y \), and \( A_3 =

\sqrt{\frac{p}{4}} Z \).”

Listing 5: Qwen Ideation Traces

Qwen2.5-7B’s ideation traces under our framework:

18

--- Page 19 ---
"ideation_traces”: "\nThe reasoning trace provides a detailed derivation of
the Kraus representation for a depolarizing channel. The key conclusions from
the reasoning trace are:\n\n1l. The Kraus operators for the depolarizing channel
are:\n - \( A_@ = \saqrt{1 - \frac{3p}{4}} I \)\n - \C A_1 = \sqrt{\frac{p
3{4}} X \)A\n - \( A_2 = \saqrt{\frac{p}{4}} Y \)\n - \( A_3 = \saqrt{\frac{p
3{4}} Z \)\n\n2. The sum of the squares of these Kraus operators equals the
identity matrix, confirming they form a valid representation.\n\n3. The
depolarizing channel operation \( E(\rho) \) is expressed as:\n \O\n E(\rho)

= A_®@ \rho A_@*\dagger + A_1 \rho A_1*\dagger + A_2 \rho A_2*\dagger + A_3 \rho
A_3*\dagger\n \]\n\n4. Substituting the Kraus operators into the expression
for \( E(\rho) \), we get:\n \OE\n E(\rho) = \left(1 - \frac{3p}{4}\right) \
rho + \frac{p}{4} X \rho X + \frac{p}{4} Y \rho Y + \frac{p}{4} Z \rho Z\n \J\
n\nNow, let's evaluate each answer option:\n\n- *xOption A:**\n \C[\n E(\rho) =
(1-p)\rho + \frac{p}{3}X\rho*{2}X + \frac{p}{3}Y¥\rho*{2}Y + \frac{p}{3}Z\rho
“{2}Z\n \]\n This option uses \(\rho*2\) instead of \(\rho\) and has
coefficients \(\frac{p}{3}\) instead of \(\frac{p}{4}\). It does not match the
derived expression.\n\n- **Option B:**\n \[\n E(\rho) = (1-p)\rho + \fracf{p
3{4}X\rho X + \frac{p}{4}Y¥\rho Y + \frac{p}{4}Z\rho Z\n \]\n This option has
the correct form and coefficients \(\frac{p}{4}\) for the terms involving \(X\),
\CY\), and \(Z\). However, the coefficient for the identity term is \((1-p)\)
instead of \(\left(1 - \frac{3p}{4}\right)\). Despite this discrepancy, it is
the closest match in terms of structure and coefficients.\n\n- *xOption C:*x*\n
\0\n E(\rho) = (1-p)\rho + \frac{p}{3}X\rho X + \frac{p}{3}Y¥\rho Y + \fracf{p
3{3}Z\rho Z\n \J\n_ This option uses \(\frac{p}{3}\) instead of \(\frac{p}{4}\)
and \((1-p)\) instead of \(\left(1 - \frac{3p}{4}\right)\). It does not match
the derived expression.\n\n- **xOption D:**\n \[\n E(\rho) = (1-p)\rho*{2} + \
frac{p}{3}X\rho*{2}X + \frac{p}{3}Y¥\rho*{2}¥ + \frac{p}{3}Z\rho*{2}Z\n \I\n
This option uses \(\rho*2\) instead of \(\rho\) and has incorrect coefficients
\(\frac{p}{3}\). It does not match the derived expression.\n\nOverall, Option B
is the closest match to the reasoning trace in terms of structure and
coefficients, despite the slight discrepancy in the coefficient of the identity
term. \n\nB"

Listing 6: projector=phi4

The projection traces in stage 2 - Phi-4 as projector:

7.2 Prompt Templates and XML Tag Based Answer Extraction

7.2.1 Prompt Templates

Prompt used in our custom implementation of MCQA Evaluation

Prompt used for self-reflection

Prompt used in ideation stage Prompt used by verifiable projector Prompt used by Subjective
LLM Projector

7.2.2 XML Tag Based Answer Extraction

As we shown above, we adopt a XML tag based prompting strategies where each functional components
are wrapped with XML tag that summarizes its function. We also include clear instructions and a
demonstration for the desired output XML format at the end. Then in the answer extraction stage,
we use regular expression to search for the corresponding pattern (e.g. for answers, we are searching
for  and ). In our experiments, we found that this method greatly
improves the stability of models instruction following performance in answer format and results in a near
zero parsing failure rates.

7.3. Additional Experimental Results

19

--- Page 20 ---
You are a helpful assistant. You will be provided with a challenging science domain
question (e.g., biology, physics, chemistry) and a set of multiple-choice
options. Your task is to reason through the question step-by-step to arrive at a

detailed answer and select the correct option.

Follow these steps:

1. Think step-by-step to analyze the question and derive a solution. Detail your
reasoning process.

2. After your reasoning, clearly state your final answer by picking the correct
option letter.

Output your response using the following XML format ONLY:



Your detailed step-by-step thinking process here.



X (where X is the letter of the correct option)

Listing 7: Custom Implementation of Standard MCQA Evaluation Prompt

You are an expert evaluator. You will be provided with a ‘Reasoning Trace' from a
language model that attempted to answer a question and multiple ‘Answer Options'
(labeled as A, B, C, etc.). The original question will also be provided.

Your task is to critically re-evaluate the entire problem to find the correct answer
You must perform your own independent, step-by-step analysis to arrive at the
most accurate solution, using the provided ‘Reasoning Trace' as a reference but
not assuming it is correct.

Consider the following when making your decision:

S *xxNumerical Closeness:** If the options and your reasoning involve numerical
values, convert all relevant numbers to scientific notation (e.g., expressing
numbers as a coefficient multiplied by 10 raised to an exponent, like 1.23 x
10*°4 or 5.6 x 10*-2). Ensure all numbers are in this consistent format, and in
the same unit if applicable, before comparison. Then, identify the option whose
scientific notation value is closest to the scientific notation value derived
from your reasoning.

S *kConceptual Closeness:** If the options involve concepts, equations,
definitions, or qualitative explanations, identify the option that best reflects

the core ideas in your reasoning or is logically inferred from your reasoning.

Follow these steps:
1. Carefully read and understand the original question, all options, and the entire
"Reasoning Trace'.
2. Conduct your own detailed, step-by-step reasoning process to solve the problem.
You can reference, correct, or build upon the provided trace.
3. Based on your independent analysis, decide which option is the most consistent
and closest match.

Output your evaluation using the following XML format ONLY:



Your detailed step-by-step reasoning here.



X (where X is the letter corresponding to your chosen
option)

Listing 8: Self Reflection Prompt

20

--- Page 21 ---
You are a helpful assistant. You will receive a challenging science-domain question,
but the multiple-choice options are hidden from you. Your step-by-step
scientific reasoning will be passed to a Stage-2 AI that can see the options and
will rely on your analysis to choose the correct one.

Therefore:

1. Do NOT refuse, disclaim, or say that the task is impossible because the options
are missing.

2. Apply rigorous scientific reasoning: cite relevant concepts, principles,
equations, variables, and alternative scenarios; deduce the most plausible
answer or the exact characteristics the correct answer must have.

3. Do NOT quote, paraphrase, or repeat the question text in your response.

OUTPUT FORMAT --- STRICT
Respond using EXACTLY the XML structure shown below and nothing else.
Formatting rules:

- Put your full, detailed reasoning inside the  ...  tags.
- Put your concise final answer (or the uniquely identifying properties of the
correct answer) inside the  ...  tags.

- Do not include any text before  or after .
- Do not add extra tags, attributes, or wrapper elements.

Your step-by-step thinking process goes here.
Your final derived answer goes here.

Listing 9: Ideation Stage Prompt

You are an expert evaluator whose ONLY goal is to select the best answer OPTION (A,
B, C, ...) based on a provided Reasoning Trace.

What you receive

- The Reasoning Trace: a step-by-step analysis already produced by another model.

- The Answer Options: labelled A, B, C, ... (the original question text may be
omitted).

Your responsibilities

1. DO NOT solve the original question again. Judge options solely by how well they
align with the Reasoning Trace's conclusions.

2. DO NOT refuse, claim uncertainty, or state that none of the options match. You

MUST pick the single closest option even if the match is imperfect.

3. Use two dimensions of closeness:

- Numerical closeness: convert every relevant number in both trace and options to
scientific-notation in the SAME UNITS, then choose the option with the smallest
numerical difference (consider both coefficient and exponent).

- Conceptual closeness: match the key ideas, equations, or qualitative statements
in the trace.

4. If several options tie, pick the first among the tied ones in alphabetical order
and explain the tie-break.

OUTPUT FORMAT --- STRICT (XML)

You MUST comply with this exact structure; any deviation will cause automatic
failure.

Rules

- Put your comparative analysis inside  ... .

- End with a single uppercase letter (A, B, C, ...}) inside  ... .
NEVER leave  blank, use words, or add extra tags/attributes.
- Do NOT output anything before  or after .

Your detailed comparison of each option to the Reasoning Trace goes here.
Begin with a one-sentence summary of the trace's conclusion, then analyse each
option (A, B, C...) explaining its fit or mismatch. Finish by justifying the
option you will pick.

X

Listing 10: Verifiable Projector Prompt

21

--- Page 22 ---
You are an expert evaluator. You will be provided with a ‘Reasoning Trace' from a
language model that attempted to answer a question. The original question may be
provided, and in some cases, the correct answer may also be provided for
reference.

Your task is to meticulously analyze the 'Reasoning Trace' and evaluate whether the
reasoning process and conclusions are correct. You must not try to solve the
original question yourself; your evaluation should be based solely on the
logical soundness and accuracy of the reasoning presented.

Consider the following when making your decision:

S *xxkLogical Soundness:** Evaluate if the reasoning follows a clear, logical
progression and if each step is properly justified.

= xxNumerical Accuracy:**x If the reasoning involves numerical calculations, verify

if the math is correct and if the units are properly handled.

S xxkConceptual Accuracy:** If the reasoning involves scientific concepts,
equations, or definitions, verify if they are correctly applied and interpreted.

S *xkCompleteness:**x Check if the reasoning addresses all aspects of the question
and if any assumptions are clearly stated and justified.

Follow these steps:

1. Carefully read and understand the entire ‘Reasoning Trace’.

2. If a reference answer is provided, use it to verify the accuracy of the
reasoning.

3. Identify any logical flaws, incorrect assumptions, or mathematical errors.

4. Based on your analysis, determine if the reasoning is Correct or Incorrect.

Output your evaluation using the following XML format ONLY:



Your detailed step-by-step analysis here. First, summarize the key points in the
reasoning trace. Then, explain any flaws or strengths you identified. Finally,
state your overall justification for your judgment.



Correct  or Incorrect 

Listing 11: LLM Judge Projector Prompt

22

--- Page 23 ---
(a) Model accuracy on the GPQA Diamond dataset when the correct answer is swapped out with “None of the other choices.” For
“Phi (No swap)”, we report accuracies for the case when no changes are made to the answer choices. Higher is better. The A row
reports the difference between minimum and maximum for each column. Parsing failure rates are 0% for the projector methods.

Projector

Standard Self | Phi (No swap)

GPQA Diamond - Accuracy (%)

Llama3.1-8B 15.66 28.28 37.37 32.83
Gemma2-9B 13.13. 27.78 37.37 31.82
Qwen?2.5-7B 27.78 31.82 41.41 34.85
Qwen?2.5-14B 40.40 34.34 41.92 41.92
Phi-4 (14B) 3.03 45.96 45.96 51.52
Qwen?2.5-32B 35.35 38.89 43.43 44.95

A 37.37 18.18 8.59 19.7

(b) Parsing failure rates for the ablation studies on the GPQA Diamond dataset. The table compares failure rates when the correct
answer is swapped with a "None" option versus when it is swapped with a random incorrect option.

Projector (Ours)

Model Standard Self Phi (No swap)
GPQA Diamond - Parsing Failure (%)

Llama3.1-8B 29.29 0.00 0.00 0.00
Gemma2-9B 47.47 0.00 0.00 0.00
Qwen?2.5-7B 5.56 0.00 0.00 0.00
Qwen2.5-14B 0.51 0.00 0.00 0.00
Phi-4 (14B) 91.41 0.00 0.00 0.00

Qwen?2.5-32B 5.56 0.00 0.00 0.00

23

--- Page 24 ---
Table 8: Evaluation of projector models. Models were provided with the ground truth solutions, and were asked to
evaluate them as if they were generated from evaluation subject models. Higher is better. Projector consistency
performs nearly perfectly, far outperforming LLM-as-a-judge. For “Judge w/ Answer", we additionally provided
the projector model with the ground truth answer, to see if it would help improve the judge’s performance. Even in
this case, the projector outperforms it.

Projector Model Judge w/ answer Projector (Ours)

GPQA Diamond - Accuracy of projector (%)

Llama3.1-8B 18.69 64.14 92.93
Gemma2-9B 57.07 95.96 97.47
Qwen2.5-7B 44.95 68.18 98.48
Qwen2.5-14B 38.89 91.92 98.99
Phi-4 (14B) 79.80 94.95 98.48
Qwen2.5-32B 62.12 93.94 99.49
GPQA Main - Accuracy of projector (%)

Llama3.1-8B 25.00 65.63 95.76
Gemma2-9B 60.49 95.31 95.54
Qwen2.5-7B 45.31 70.09 97.77
Qwen2.5-14B 39.06 92.41 99.11
Phi-4 (14B) 83.48 97.32 98.21
Qwen2.5-32B 65.85 93.08 99.33

Table 9: Parsing failure rates (%) for models on GPQA datasets under standard MCQA evaluation and our two stage
instantiation of proposed framework. We observe a complete elimination of parsing failures when Phi-4 is chosen
as projector in verifiable projection stage. LMH+ and Custom are both variants of standard MCQA evaluation with
prompt engineering improvements. * - Phi-4’s extreme parsing failure rate in LMH based evaluation is the result of
missing chat template in LMH framework and Phi-4’s high sensitivity to input format.

Standard MCQA Evaluation Our Method

LMH+ Custom Projector - Phi

GPQA Diamond

Llama3.1-8B 34.34 16.16 29.29 0.00
Gemma2-9B 44.95 19.19 5.56 0.00
Qwen2.5-7B 4.55 0.51 3.54 0.00
Qwen2.5-14B 0.00 0.00 0.00 0.00
Phi-4 (14B) 89.39* 100.00* 0.51 0.00
Qwen2.5-32B 3.54 0.51 1.52 0.00
GPQA Main

Llama3.1-8B 29.02 13.84 26.79 0.00
Gemma2-9B 38.62 19.20 7.37 0.00
Qwen2.5-7B 3.138 0.45 2.01 0.00
Qwen2.5-14B 1.79 0.45 0.67 0.00
Phi-4 (14B) 91.29* 100.00* 0.67 0.00
Qwen2.5-32B 2.68 0.67 0.67 0.00

24

--- Page 25 ---
Table 10: Parsing failure rate for LMH, projector, and LLM-as-a-judge. Lower is better.

* - The projector threw an error because the model’s outputs could not be converted to a valid Python expression.
The other three evaluation methods did not throw an error because they simply marked these outputs as incorrect.
Llama seemed to struggle in this scenario due to its shorter context length.

{ - For a few examples, the projector refused to choose an answer based on the model’s outputs. (See section 4.3.)

Judge
Model LMH Projector (Phi) Self Phi-4
ARC Challenge - Parsing Failure (%)
Llama3.1-8B NA 0.097 1.37 0.00
Gemma2-9B NA 0.00 0.09 0.09
Qwen2.5-7B NA 0.00 3.92 0.09
Qwen2.5-14B NA 0.00 0.00 0.00
Phi-4 (14B) NA 0.00 0.00 0.00
Qwen2.5-32B NA 0.00 0.00 0.00
GPQA Diamond - Parsing Failure (%)
Llama3.1-8B 34.34 0.00 11.62 1.01
Gemma2-9B = 44.95 0.00 0.00 0.00
Qwen2.5-7B 4.55 0.00 2.02 0.00
Qwen2.5-14B 0.00 0.00 1.01 0.00
Phi-4 (14B) 89.39 0.00 0.00 0.00
Qwen2.5-32B 3.54 0.00 0.51 0.00
GPQA Main - Parsing Failure (%)
Llama3.1-8B 29.02 0.00 7.81 0.00
Gemma2-9B 38.62 0.00 0.22 0.00
Qwen2.5-7B 3.13 0.00 2.46 0.00
Qwen2.5-14B 1.79 0.00 0.00 0.00
Phi-4 (14B) 91.29 0.00 0.00 0.00
Qwen2.5-32B 2.68 0.00 0.45 0.00
GSM8K-100 - Parsing Failure (%)
Llama3.1-8B 0.00 10.00* 4.00 1.00
Gemma2-9B 2.55 0.00 0.00 0.00
Qwen2.5-7B 0.00 0.00 0.00 0.00
Qwen2.5-14B 0.00 0.00 1.00 0.00
Phi-4 (14B) 0.00 0.00 0.00 0.00
Qwen?2.5-32B 0.00 0.00 0.00 0.00

25

--- Page 26 ---
Table 11: Ablation study on the importance of the projector. Higher is better.

Projector
Model LMH Rule-based Phi-4

GPQA Diamond - Accuracy (%)

Llama3.1-8B 18.69 20.71 32.83
Gemma2-9B_ 14.65 32.83 31.82
Qwen2.5-7B 29.29 28.79 34.85
Qwen2.5-14B 42.42 35.35 41.92
Phi-4 (14B) 4.04 34.34 51.52
Qwen2.5-32B 41.41 33.33 44.95
GPQA Main - Accuracy (%)

Llama3.1-8B 22.76 22.54 33.04
Gemma2-9B 18.30 33.04 32.59
Qwen2.5-7B 33.48 28.57 30.13
Qwen2.5-14B 38.17 32.37.72
Phi-4 (14B) 2.90 35.49 45.31
Qwen2.5-32B 41.74 34.60 40.85
GSM8K-100 - Accuracy (%)

Llama3.1-8B 32.00 30.00 42.00
Gemma2-9B 21.00 59.00 74.00
Qwen2.5-7B 76.00 48.00 70.00
Qwen2.5-14B 80.00 75.00 84.00
Phi-4(14B) 52.00 91.00 91.00
Qwen?2.5-32B 87.00 95.00 95.00

26

--- Page 27 ---
Table 12: Ablation study on the importance of the projector. Failure rates are reported across various models on
three different benchmarks. The methods compared are a baseline (LMH) and two projector-based approaches.

Projector

Model LMH Rule-based Phi-4

GPQA Diamond - Parsing Failure (%)

Llama3.1-8B 34.34 38.89 0.00
Gemma2-9B 44.95 2.02 0.00
Qwen2.5-7B 4.55 13.13 0.00
Qwen?2.5-14B 0.00 2.02 0.00
Phi-4 (14B) 89.39 1.52 0.00
Qwen?2.5-32B 3.54 0.51 0.00
GPQA Main - Parsing Failure (%)

Llama3.1-8B 29.02 31.47 0.00
Gemma2-9B 38.62 1.56 0.00
Qwen2.5-7B 3.13 12.95 0.00
Qwen2.5-14B 1.79 1.12 0.00
Phi-4(14B) 91.29 2.01 0.00
Qwen?2.5-32B 2.68 1.56 0.00
GSM8K-100 - Parsing Failure (%)

Llama3.1-8B 0.00 47.00 10.00
Gemma2-9B 2.55 33.00 0.00
Qwen2.5-7B (0.00 40.00 0.00
Qwen?2.5-14B 0.00 16.00 0.00
Phi-4 (14B) 0.00 0.00 0.00
Qwen?2.5-32B 0.00 0.00 0.00

27

--- Page 28 ---
Table 13: Gap in model performance depending on the choice of projector model. Reported numbers are calculated
by taking the difference of the max and min of the accuracies over all of the open-source projector models. Judge
results vary drastically depending on what model is being used as a projector. Smaller is better.

Projector Judge

ARC Challenge - A Accuracy (%)

Llama3.1-8B 0.07 0.18
Gemma2-9B 0.08 0.32
Qwen2.5-7B 0.07 0.16
Qwen2.5-14B 0.07 0.12
Phi-4 (14B) 0.07 0.08
Qwen2.5-32B 0.07 0.10
GPQA Diamond - A Accuracy (%)
Llama3.1-8B 0.04 0.12
Gemma2-9B 0.07 0.24
Qwen2.5-7B 0.08 0.12
Qwen2.5-14B 0.10 0.19
Phi-4 (14B) 0.07 0.15
Qwen2.5-32B 0.08 0.18
GPQA Main - A Accuracy (%)
Llama3.1-8B 0.05 0.10
Gemma2-9B 0.04 0.27
Qwen2.5-7B 0.03 0.13
Qwen2.5-14B 0.04 0.20
Phi-4 (14B) 0.06 0.17
Qwen2.5-32B 0.05 0.18

28

A great life is built not by revolution. A great life is built by evolution. Small and steady wins the race.

What you do every day is far more important than what you do once every decade.

I want you to really think about that idea. What you do every day is simply your life in miniature.

And as you live every single day so you're crafting your life, what you do over the next hours is really building your future.

And if you can just get every single pocket of 24 hours right as best as we humanly can.

The rest of our life is going to take care of itself. So small wins matter.

The scripture says we will eat the fruit of our words. You are planning seeds when you talk.

At some point you're going to eat that fruit. My challenge is make sure you're planning the right kind of seeds.

If you want apples you have to sow apple seeds. If you want oranges you can't plant cactus seeds, poison ivy seeds, mushroom seeds.

You're going to reap fruit from the exact seeds that you've been selling.

In other words you can't talk negative and expect to live a positive life.

You can't talk defeat and expect to have victory. You can't talk lack.

Not enough. Can't afford it. Never get ahead and expect to have abundance.

If you have a poor mouth you're going to have a poor life.

We may not realize it but we're always feeding ourselves.

What we watch, listen to, the people we're around, the thoughts we're dwelling on, that's feeding our inner man.

If you go to lunch with people that talk bad about the boss, make fun of a coworker, put down a friend, they're feeding you gossip, feeding you jealousy, feeding you disrespect.

Here's the key, whatever you feed is going to grow.

You may not like what they're feeding you, you're a nice person but if you continue to hang around them before long you'll be gossiping.

You'll be critical, you'll be disrespectful, why? Because you kept feeding the wrong things.

If you're always feeding negative thoughts thinking I'll never get well, I'll never meet the right person, you're feeding down.

Feeding mediocrity, feeding fear.

If you know exactly what you want to be, you need to spend as much time with people that are actually that already.

The word count of my three minute thesis.

How did I manage to put three and a half years of work into such a short script?

Well apparently I didn't say anything about the experiment that had failed, which was like three years of work.

In 2013 I heard news that Chris West, a colleague from the Scottish Centre for Generative Medicine, had won the University Final.

That was the first time I heard about this competition.

I thought that one day I might enter it, and I'm so glad that I did,

because my three minute thesis experience was a turning point in my PhD.

Before entering this competition, the question that I was most scared of was, what is your research about?

Why is it important?

I didn't know what to say, or maximum two to three sentences.

And very often I heard people responding, hmm, your research sounds interesting.

And I know it's so well that interesting is a word that people often use when they don't know how to continue this conversation.

By preparing my three minute thesis, I challenged myself to come up with ideas about how to explain terms that may be so familiar to me, but may not make any sense to others.

It also forced me to take one step back from my daily detailed experiments to look at the big picture.

Now I so much enjoy discussing my research with others, and very often I use chunks of my three minute thesis in these conversations.

Some people may think that I entered this competition because I was good at public speaking. That's absolutely not the case.

English is not my first language, and even after four years of study in the UK, I was still feared to talk to native speakers.

Because I was concerned that they must be judging every single word coming out of my mouth, whether the grammar or pronunciation is correct.

I was also shy when talking to strangers, at networking events, I never introduced myself, but always waited to be introduced.

I have got several opportunities to give public speaking, to give talks, to big audiences, like the Senate meeting, post-graded inductions, the IAD forum, the scrimary treat, and including this event.

Because of these experiences, they have made me less and less nervous about public speaking and become more confident.

My view about communication has also changed. I start to introduce myself and got to know so many people. I start to enjoy exchanging ideas and thoughts and learning from each others.

I think communication is the first step towards collaboration and innovation.

My 3MT experience has also brought me great opportunities. Last year, I went to Chicago to attend the biggest neuroscience conference annually.

And for the past four months, I had been interning with the World Health Organization in Geneva.

I was picked purely because my supervisor got interested in my 3MT experience in my CV, and she watched my video on YouTube.

This internship has been an eye-opening experience. I now have a clearer idea about how I can combine my clinical background, my research skills, and my interests in public health, all these three together to hopefully in the future make a contribution to improving people's health.

Last but not least, I would like to take this opportunity to say thank you to all those people who had helped me with my 3MT, my supervisor, my colleagues, my landlords, and their friends.

All of them had given me great suggestions and support. I would also like to say congratulations to the nine finalists on reaching today.

Because I know exactly how much effort you have put on preparing your 3MT, and I know how nervous you are at the moment.

But I want to say that good luck, believe in yourself, and enjoy it. Thank you all very much.

Whether it's a business pitch, a wedding toast or even a public speaking competition,

three minute speeches can be tricky to craft because they're long enough for us to have

some juicy content in them but they're not so long that we can actually do justice to all that

content. So how do we tackle this issue and go about crafting a compelling three minute speech?

Let's break it down into three very simple parts. You have the introduction, the body and the

conclusion. Part of almost any speech of any length. In a three minute speech, the introduction

should take about 30 seconds. The body should take about two minutes and the remaining 30 seconds

go into the conclusion. But before we get into all of that, we need to understand why a lot of

short speeches, whether they're three, five or even one minute long tend to fall apart. And it's

many times because the speaker is confusing the audience with too many messages. So before we

sit down to actually write our speech, it's important for us to take a step back and ask ourselves

two pretty important things. Number one, what is our main center? What is that one thing that we

want to communicate to the audience? Just one because anything more than that tends to be too much

for a three minute speech. And secondly, what is the takeaway for the audience? Do we want them to take

some sort of action? Do we want them to adopt an idea? What do we want them to do at the end of the

speech? Once we have both these things in place, crafting the rest of the speech will become a lot

easier. Okay, so getting back to our structure, let's start with the introduction. We can break

this down into two parts. If you're giving a speech for entertainment or for competition purposes,

then the introduction needs to be captivating, needs a hooker audience from the very beginning.

If you want to be outlandish, we can also use some sort of unusual prop to capture that attention

from the very beginning. Look how this speaker dare and take does it when he delivers his presentation.

Or if he don't want to use something like a prop, we can also use the Imagination scenario. This

simply means making the audience shut their eyes and making them imagine something. Check out this

example by Rick Elias in his step-doc. Imagine a big explosion as you climb through 3000 feet.

Imagine a plane full of smoke. Imagine an engine going clack

Sounds scary. Why are they unique seats that day? I was sitting in one D.

And now, if we're giving them more formal three minutes speech or presentation, we don't necessarily

need to have some sort of hook. We can simply start off with telling the audience why we're here

by getting straight to the point and telling them why is it important that they need to listen to

this topic. We're simply starting off with, I'm here to talk to you about whatever your topic is

and this is important for you whatever the reason is. That's good enough for a short speech

informal settings. Next comes the body. The main part of your speech we're going to be spending

approximately two minutes. The best way to communicate a lot chunk of your speech even if it's

just two minutes is to convey it in the form of a story. Now a lot of us know the importance of

storytelling but sometimes we may find it difficult to take a bunch of information and convert it

into a narrative. So I'm not going to get too deep into this topic because we've made a lot of

videos around storytelling. They'll all be linked in the description but to give you a small

gist of it, almost every story will be broken up into four parts. You have the desire, the conflict,

the solution and finally the climax. For example if you're pitching a particular company or idea,

the desire would be what is the vision for that particular company. The conflict will be what is

the problem or challenge that we're solving for our anti-customer. The solution will be how the

product or service works that will help solve that particular challenge and finally the climax

will be the way forward in terms of the plan that will help them achieve the overall desire or

vision of the company. We have a lot more examples in the other videos so if you're interested to

go deeper into this topic, check them out. And finally we have the conclusion. This is the remaining

30 seconds of your speech. Over here if we don't want to get too fancy, we simply need to have

our main takeaway but at the end of the speech. What is the final action we want the audience to

take or the final idea we want them to adopt? That should be the closing statement. This is because

the end is what the audience tends to remember the most. So it just makes sense to have the most

important part for our speech as one of the last lines. An alternative to this is if your speech is

very information-heavy, you can summarize it towards the end to make sure that the audience don't

miss out on anything important. Now your speech might not exactly be three minutes long, it might be

two minutes or five minutes long. There are different nuances that go even when we're playing around

such less minutes. But if you've made videos on topics with specific speech lengths, I highly recommend

you check out our two-minute and our five-minute video in case you have those particular lens of speech

just to give it as well.

Hello and welcome to this video on crafting the perfect 3 minute speech.

Following are some tips on public speaking, my name is Shriram Ayah also known as Sair.

It's worth remembering that a great speech is one that is memorable, but what makes a

great speech memorable?

If you can do at least one of these three, if not all three, then your speech will be

Make the audience think, make them feel, and make them laugh or have fun.

I'm now going to give you some specific tips on public speaking.

Number one, write, memorize and structure your speech.

Ensure that when you get to the stage you are at your confident best and that you do not

have to rely on your memory or cue cards.

Cue cards and other things relating to your speech are unnecessary and act as crutches.

So practice so hard that you remember your speech even in your sleep.

Number two, the use of your hands.

So your hands are not your enemy, they are your friends.

Use them to further illustrate and explain your point that you are making with your words.

Your hands and your body are your friends.

So use them to that effect.

Number three, movement on stage.

The stage itself can be a terrifying place for many, but I want you to understand that

the stage too is your friend.

Much like the hands.

So use the stage, especially the length of the stage.

Move to the right, look at the audience there, make eye contact.

Move back to the center, make an important point, and then move to the left.

Again, making eye contact with the audience on that side of the room.

Number four, you all have wonderful smiles.

To ensure that you smile throughout your speech, even if you forget what comes next, pause

and give them your best smile.

Number five, your voice.

Much like the stage and your hands, your voice too is your friend.

And your voice has a couple of superpowers.

The ability to increase or decrease speed.

And again, use them to your advantage.

For example, when making an important point, slow down.

And sometimes even softening your voice can work in terms of gaining their full attention.

So use your voice because it is your friend.

Also, I highly encourage the use of stories in your speeches, because people remember stories.

We want your speech to be memorable, and stories will help you do that.

You can tell a story about any one of your experiences or even an achievement, either

But remember, your story needs to have one, at least one, major emotion, humor, excitement,

You can use the story in the middle of your speech and break up the speech, using a transitionary

You can also use your story to create great interest and curiosity.

You can take them to the peak of curiosity and leave them on a cliffhanger and reveal

the end of the story at the end of your speech.

But think about your favourite story and just go for it.

Remember to keep it between 30 to 45 seconds in a 3 minute speech.

But make sure your story connects to your topic.

Also remember to structure your story using the car format, context, action, result.

Often the way you start and end your speech also makes a big difference.

You can start your speech with a story or a famous quote or questions or even an interesting

fact or a statistic.

And to end your speech, make sure that you tell the audience what you expect from them,

give them an action that they must do.

You can also use a famous quote here that makes them think.

When you end your speech, use at least 15 seconds to summarise your key message.

I hope this was useful, all the very best with your public speaking.

The idea of hardening your mind and figuring out a way to do things that most people think are impossible.

How'd you become that guy?

You know what? I grew up not that guy. So a lot of people put a title on me.

They want to, they see me now. They see me now as the guy that with his shirt off,

they can do 4,030 pull ups in 17 hours. You can run 25 miles in 39 hours.

You can do all this crazy shit. But what they don't understand is they don't understand the journey that it took me to get to this point.

And what got me to this point was I was just the opposite of what I am today.

I was that guy who ran away from absolutely everything that got in front of me.

But not many people knew that. I had two people.

Like the real me was like this very scared, insecure, stuttering, got beat up by his dad.

I was kind of stuff. And I built this fake person that walked around like my shit didn't stink.

You know? That's kind of how I did it. And I do process of time.

I realized that I was lying to myself, lying to people.

My dad beat the shit I was going up. I was the first black baby born in this hospital called Nimbus Fillmore in Ruffle, New York.

My dad on skating rinks, he on bars, he ran prostitutes from Canada to Buffalo, New York.

My dad was a big time, big time. Anything bad about a person, big time hustler.

He was American. Look, I'm with him. Danza Washington. He was that, but not that bad.

You know, he wasn't that big. But that's what it reminds me of. He was that kind of guy.

Beat the shit out of me, the shit out of my mom. He was an incident one time when my mom got knocked out top of the stairs.

He drugged down the stairs by her hair. And at six years old, I never forget this.

In my mind, I was always afraid. My whole life was afraid. I had this fucking voice, this conscious,

that would always be battling me saying, hey, you got to get up and do something.

I didn't want to do shit. You know, I was just afraid, but that voice would force me to get up.

And my dad, you know, I try to beat him up whenever I had six and I get my ass kicked.

So let's went on for several years and have a big time learning disability.

Can my dad even not go into school? I had stuff on my notebook, you know, Nigger, we're going to kill you.

I'm a Spanish notebook. They had that on my card, Nigger, we're going to kill you. This is early 90s.

Even though I showed it didn't hurt me, it was jacking me up.

So all the insecurities I have, and I was a kid with my father, I'm moving to this area here.

And it just got worse and worse and worse. And it shit haunted me.

And that voice that I talked about, it kept talking louder and louder and louder.

But I was doing nothing about it. I decided to make moves.

And I cheated all through school. And it's kind of humbling to talk about my story sometimes.

And it's also embarrassing, but it's real. It's who the fuck I am. It's what I am. It's what created me.

Get the fuck out of here! Stay home!

At the time, I think the ratio was that 26 out of 27 new companies failed.

It would really be meaningful to me and something that I would really have a lot of passion for and about.

You have to sleep the dream.

You have to touch.

You have to see it when nobody else sees it.

You have to fill it when it's not tangible.

You have to believe it when you cannot see it.

You've got to be possess with the dream.

Rob, what's up with y'all?

You don't give a place to sit.

Every dog on time you get on the field

because you ain't got a why for what you do.

You ain't got a why.

You've got to hear what I'm telling you.

If y'all gonna be great on that field,

you've got to have a why.

You have a reason for what you do what you do.

What's your why?

Why do you wake up in the morning?

Why do you put on that jersey?

Why do you go out and practice why?

You owe you an explanation.

You need to look at yourself in the mirror

and say why are you only giving 50%?

What's wrong with you?

So what?

Have you or two lions?

It does not make a difference

with the circumstances that's in.

You need to tell you that you owe you something.

What change?

I changed.

And I stopped being a victim.

I stopped saying I've got to wait for good things to happen.

Because every day is a new day.

Every moment is a new moment.

So now you're gonna go out and show them

that I'm a different creature now.

Because I'm pissed off for greatness.

That means you're okay with being mediocre.

We get one opportunity now.

One chance in life to do it.

What are the layers that you're gonna leave?

I will.

That's what I do.

Lies don't cry.

Lies on.

You know how many things that the devil has kept you from doing in your life

that God called you to do?

Because of what might happen if you did,

and a lot of us, the enemy doesn't even have to fight us with real events or circumstances.

Just even the suggestion of a disaster is enough to send some of us into retreat.

And so just even the thought that what if they don't like you is enough to keep you in the house.

Or just even a thought if what if they break my heart is enough to keep you from extending yourself in relationship.

Or just even a thought, you know, many of you are reluctant to get involved in this church

because you had a previous bad church experience.

And I understand that because once you have been through a storm, I lived through Hurricane Hugo.

I was eight years old when Hurricane Hugo hit Charleston, South Carolina. It terrified me.

I didn't want to spend the night away from home for two years after that as a little boy.

Because one thing after you have survived a storm, after you have survived bad weather,

after you have survived abuse, what people don't tell you about is that even after the storm is over,

it still rages on the inside of you.

And the saddest thing in the world is to see a Christian who has been set free by the love of Christ

to know that neither height nor depth nor anything else in all creation,

neither death nor life, neither angels nor demons,

neither the present nor the future nor any other powers shall be able to separate me

from the love of God that is in Christ Jesus, my Lord.

What I'm trying to say, some of us need to fire the devil as our weatherman.

We have been consulting the wrong source to find out what our future looks like.

And so now we stay in doors and we stay in fear and we stay in paranoia.

Solomon said because we watched the wind, what a weird image.

You really can't see the wind.

You can only see the effects of it.

And he says when you watch the wind, you know, like is this a good time?

Oh, come on.

I'll give you one. This one looks like the watch the wind.

Cussing about that phrase, watch the wind.

And I was like, this is not a literal message.

I'm not using it to talk about physical storms.

I'm using it to talk about when the sower goes to so he wants the wind to work with him.

And when the wind is blowing the opposite direction, all your seed smacks you in your face.

But you ever feel like everything you sow just, you ever feel like stuff just backfires on you.

You're trying to be nice to somebody.

That's the last one of those I'm doing.

I got a precious twice tomorrow, I need my brain cells.

But you ever just been smacked by your own seed because that's what he's saying.

When the wind is blowing against you, some of you have had the wind against you.

Your whole life.

You didn't have the right shoes.

Your parents didn't make a lot of money.

You had to learn to fit in. You had to learn how to be tough.

You had to learn how to be hard.

And you learned how to survive a storm.

But the problem with that, the only problem with that is sometimes the wind is in the wind.

Sometimes the wind isn't even against you, but you still feel like it is.

Because your mind is collecting evidence and support of a verdict that isn't a longer accurate.

Everyone I know who has had some modicum of successful film and happiness in their life,

they have conviction.

They generate conviction as more of a better way to say it.

I need all of you to do the same thing.

Your friends, your family, the world, social media,

your own mind can distract you from your path.

I can't explain to you how much of a difference you can make to the world.

It's ridiculous and trust me, lives will change.

That's who you are.

That's why we're in this community.

Because everyone has something to give.

There's three primal patterns.

Everybody goes through fight, flight or freeze.

And that part of the brain.

There is a primacy, if there's like a imagine there's a hierarchy in your in the brain,

not just your brain, survival at the very top.

And then emotion, then logic.

Now you can change that.

It takes work, but you can change that.

You already are.

You're already changing that.

Sometimes our feelings trump our logic.

That's the simple answer.

Resistance is a powerful trickster.

A powerful trickster.

So what's the next mountain?

The pro, when they see the finish line, they don't slow down, they go faster.

And they choose another finish line to keep going.

Because they are committed not to the journey, not to the finish line,

not to the time or the outcome, to the running.

They're just committed to the running.

The one thing that brings them joy, the work.

And that's why I said at the beginning, playfulness, curiosity, lightness.

It's a game.

It's fun.

How can you reduce the things that are intense?

In favor of the things that are light and bring you joy?

Because you again, I'll repeat, you are a human being or human doing.

A human being as a verb means you have to be active.

You're not putting this planet to pay bills and then die.

You're here to have joy to smell the roses and walk around like a hippie with your,

your toes in between the grass and to enjoy the warm feeling of the hot shower

and to create poetry and to help people.

Like that's what you're here for.

And to help people like that's what you're here for.

The good news is you can have control when it comes to doing media interviews.

It's kind of nice, right?

I think maybe some of you don't believe me.

I see some faces where you say, oh no, I'm a little bit nervous.

I am certainly not trying to create people who are like robotic politicians who go out

there and you see this all the time.

Tell me about the one Afghanistan.

Well, healthcare today.

How do they go there?

How do that happen?

And people see that.

And they say, oh, those people are media trained.

No, they need media training.

More often what happens is we go out there, we talk to a reporter, you know, the answer

that gets into the story is something like, and we're really on top of it.

Or I agree.

And nobody can really yell at you for giving that type of answer, but it didn't really

And what we're after is action.

Whenever we deliver an interview, whenever we talk to the media, we want action.

We want everyone who's looking at that to do something.

And we live in a great time, a time where we have the invention of YouTube.

And YouTube, you can put whatever you want that you have approved.

And it may be one of these interviews or it may be one that you've done yourself.

And you can put it out there and on your website and have the right answers and the exact

ones that you want the public to say.

And one of the big problems is that people go out there and they've been told you must

say these three messages over and over.

And then the thing is the reporter only has those messages and their force to use them.

Not in the 2000s.

Maybe when TV was starting out or radio was starting out and they said, oh, well, this is

all I have and I have to fill the time.

Now they have a new story that you look terrible.

And then you're ridiculous and you're trying to put one over on them.

What we want to do is we want to be authentic.

And we can get the right messages out.

We don't want to repeat the same thing when a reporter comes to interview you.

And you talk and talk and talk ultimately what they are going to print are quotes.

That's what they want.

You need to work within the format that they have.

And if you understand the elements, there are 14 of them.

Elements that create the perfect sound bite you can create one in advance.

I love helping people to have more control and to do it in an authentic way.

That's what we'll be doing here today.