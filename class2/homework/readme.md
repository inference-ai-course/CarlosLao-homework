# ğŸ§  Multimodal Data Extraction Pipeline with OCR, ASR & Web Scraping

![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen.svg)

A comprehensive pipeline for extracting and cleaning data from diverse sourcesâ€”web pages, PDFs, and audio transcriptsâ€”using **Tesseract OCR**, **Web Scraping**, and **Automatic Speech Recognition (ASR)**. It includes image preprocessing, layout-aware OCR, language detection, deduplication, and PII removal to produce a high-quality textual corpus.

---

## ğŸ“¦ Key Features

- âœ… Advanced image preprocessing for OCR accuracy
- âœ… Web scraping and HTML cleaning with Trafilatura
- âœ… PDF-to-text conversion via OCR
- âœ… ASR transcription from YouTube audio
- âœ… End-to-end data cleaning and deduplication

---

## ğŸ§° Technologies Used

| Tool               | Purpose                     |
| ------------------ | --------------------------- |
| `pytesseract`      | OCR engine                  |
| `pdf2image`        | Convert PDFs to images      |
| `trafilatura`      | Clean HTML content          |
| `yt-dlp`           | Download YouTube audio      |
| `langdetect`       | Language detection          |
| `datasketch`       | MinHash-based deduplication |
| `OpenCV`, `Pillow` | Image preprocessing         |

---

## ğŸ“Œ Task Breakdown

The project is modularized into four core tasks and one shared utility script. Each task handles a distinct data source or processing step.

---

### ğŸ§° Main Task: Tesseract Best Practices

ğŸ“„ [`task_main.py`](./task_main.py)

- **Purpose**: Centralize reusable functions for image preprocessing and text normalization.
- **Highlights**:
  - Resizing, binarization, and enhancement utilities
  - Shared text cleaning and formatting logic
- **Input**: All `.jpg`/`.png` files in [`task_main/`](./task_main/)
- **Output**: Corresponding `.txt` files in [`task_main/`](./task_main/)

---

### ğŸ§¾ Task 1: Web Scraping & HTML Cleaning

ğŸ“„ [`task_bonus_1.py`](./task_bonus_1.py)

- **Purpose**: Extract abstracts from arXiv paper screenshots.
- **Highlights**:
  - OCR-based text extraction
  - Abstract isolation and cleanup
- **Input**: Latest 200 arXiv papers in subcategory `cs.CL`
- **Output**: [`arxiv_clean.json`](./arxiv_clean.json)

---

### ğŸ“„ Task 2: PDF to Text OCR

ğŸ“„ [`task_bonus_2.py`](./task_bonus_2.py)

- **Purpose**: Extract full text from arXiv PDFs using OCR.
- **Highlights**:
  - PDF-to-image conversion
  - OCR-based text extraction
- **Input**: PDF links from Task 1
- **Output**: [`pdf_ocr/*.txt`](./pdf_ocr/)

---

### ğŸ—£ï¸ Task 3: Automatic Speech Recognition (ASR)

ğŸ“„ [`task_bonus_3.py`](./task_bonus_3.py)

- **Purpose**: Transcribe academic talks using OCR and ASR.
- **Highlights**:
  - Slide text extraction via OCR
  - Spoken content transcription via ASR
  - Timestamp alignment
- **Input**: Video links in [`task_bonus_3_input.txt`](./task_bonus_3_input.txt)
- **Output**: [`talks_transcripts.jsonl`](./talks_transcripts.jsonl)

---

### ğŸ§¹ Task 4: Data Cleaning & Deduplication

ğŸ“„ [`task_bonus_4.py`](./task_bonus_4.py)

- **Purpose**: Merge and refine all extracted text into a unified corpus.
- **Highlights**:
  - Duplicate removal and noise filtering
  - Token statistics for quality assessment
- **Input**: Outputs from Tasks 1â€“3
- **Output**:
  - [`clean_corpus.txt`](./clean_corpus.txt)
  - [`stats.md`](./stats.md)

---

## ğŸ“ Project Structure

```plaintext
class2/
â”œâ”€â”€ task_main.py                  # Main Task: Tesseract Best Practices
â”‚   â””â”€â”€ task_main/                # Folder for image inputs and text outputs
â”‚       â”œâ”€â”€ *.jpg / *.png         # Input: raw images
â”‚       â””â”€â”€ *.txt                 # Output: preprocessed text files
â”‚
â”œâ”€â”€ task_bonus_1.py               # Task 1: Web Scraping & HTML Cleaning
â”‚   â””â”€â”€ arxiv_clean.json          # Output: cleaned abstract data (JSON)
â”‚
â”œâ”€â”€ task_bonus_2.py               # Task 2: PDF to Text OCR
â”‚   â””â”€â”€ pdf_ocr/                  # Folder for OCR-extracted text files
â”‚       â””â”€â”€ *.txt                 # Output: text files from PDFs
â”‚
â”œâ”€â”€ task_bonus_3.py               # Task 3: Automatic Speech Recognition (ASR)
â”‚   â”œâ”€â”€ task_bonus_3_input.txt    # Input: YouTube video links
â”‚   â””â”€â”€ talks_transcripts.jsonl   # Output: transcripts with timestamps (JSONL)
â”‚
â”œâ”€â”€ task_bonus_4.py               # Task 4: Data Cleaning & Deduplication
â”‚   â”œâ”€â”€ clean_corpus.txt          # Output: final cleaned and deduplicated corpus
â”‚   â””â”€â”€ stats.md                  # Output: token statistics and removal summary
â”‚
â””â”€â”€ README.md                     # Project documentation and usage guide
```
