{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4a496c",
   "metadata": {},
   "source": [
    "**1. Setup**\n",
    "- Import all pipeline components and configure logging once for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f352c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_utils import configure_logging\n",
    "from arxiv_fetcher import ArxivFetcher\n",
    "from paper_summarizer import PaperSummarizer\n",
    "from reward_model_trainer import RewardModelTrainer\n",
    "from evaluation_runner import EvaluationRunner\n",
    "\n",
    "log = configure_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde70f33",
   "metadata": {},
   "source": [
    "**2. Fetch PDFs**\n",
    "- Use ArxivFetcher to download recent papers as PDFs into the RAW_FOLDER.\n",
    "- Each PDF will be stored locally for later summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261bf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:21:21 - INFO     - arxiv_fetcher - Output folder ready: /home/jovyan/MLE_in_Gen_AI-Course/class8/homework/data/raw\n",
      "2025-09-28 19:21:21 - INFO     - arxiv_fetcher - Fetching arXiv papers...\n",
      "2025-09-28 19:21:21 - INFO     - arxiv - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=machine+learning&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:21:22 - INFO     - arxiv - Got first page: 100 of 435760 total results\n",
      "2025-09-28 19:21:25 - INFO     - arxiv_fetcher - Stored 10 PDFs in /home/jovyan/MLE_in_Gen_AI-Course/class8/homework/data/raw\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ArxivFetcher().run()\n",
    "except Exception as e:\n",
    "    log.error(f\"ArXiv fetch failed: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2204998",
   "metadata": {},
   "source": [
    "**3. Summarize PDFs into reward dataset**\n",
    "- PaperSummarizer extracts text from PDFs (or abstracts if needed), generates both \"chosen\" and \"rejected\" summaries, and writes them into reward_data.jsonl for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88444d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:21:51 - INFO     - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bb17db914f4bab812577987de4d1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:22:07 - INFO     - paper_summarizer - Processing 10 PDF(s)...\n",
      "2025-09-28 19:22:07 - INFO     - paper_summarizer - Paper 1/10: 2509.21320v1.pdf\n",
      "2025-09-28 19:24:23 - INFO     - paper_summarizer - Paper 2/10: 2509.21319v1.pdf\n",
      "2025-09-28 19:25:54 - INFO     - paper_summarizer - Paper 3/10: 2509.21278v1.pdf\n",
      "2025-09-28 19:27:42 - INFO     - paper_summarizer - Paper 4/10: 2509.21286v1.pdf\n",
      "2025-09-28 19:29:14 - INFO     - paper_summarizer - Paper 5/10: 2509.21281v1.pdf\n",
      "2025-09-28 19:30:42 - INFO     - paper_summarizer - Paper 6/10: 2509.21309v1.pdf\n",
      "2025-09-28 19:32:02 - INFO     - paper_summarizer - Paper 7/10: 2509.21282v1.pdf\n",
      "2025-09-28 19:33:31 - INFO     - paper_summarizer - Paper 8/10: 2509.21296v1.pdf\n",
      "2025-09-28 19:35:01 - INFO     - paper_summarizer - Paper 9/10: 2509.21302v1.pdf\n",
      "2025-09-28 19:36:29 - INFO     - paper_summarizer - Paper 10/10: 2509.21293v1.pdf\n",
      "2025-09-28 19:38:07 - INFO     - paper_summarizer - Saved reward dataset to /home/jovyan/MLE_in_Gen_AI-Course/class8/homework/data/reward_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    PaperSummarizer().run()\n",
    "except Exception as e:\n",
    "    log.error(f\"Summarization failed: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d5e52",
   "metadata": {},
   "source": [
    "**4. Train reward model**\n",
    "- RewardModelTrainer loads the dataset, trains a reward model, saves it to REWARD_OUTPUT_DIR, and we also inspect the classifier head weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38f78f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/demo-venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:40:23 - INFO     - reward_model_trainer - Loading and preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702d2e571d054779b6ba99c3ff5686d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:40:23 - INFO     - reward_model_trainer - Loaded dataset with 10 entries. Keys: ['chosen', 'rejected']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079e7558534e4f6ba1ad8305fc41b5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:40:26 - INFO     - reward_model_trainer - Starting reward model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:40:48 - INFO     - reward_model_trainer - Training complete. Model + config saved to /home/jovyan/MLE_in_Gen_AI-Course/class8/homework/reward_model\n",
      "2025-09-28 19:40:48 - INFO     - reward_model_trainer - Extracting weights from /home/jovyan/MLE_in_Gen_AI-Course/class8/homework/reward_model/model.safetensors\n",
      "2025-09-28 19:40:48 - INFO     - reward_model_trainer - Classifier head weights shape: torch.Size([1, 768])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 768])\n",
      "tensor([[ 3.0603e-02,  2.3429e-02, -4.6433e-03, -1.8017e-02,  1.8536e-02,\n",
      "         -1.5608e-02, -8.6062e-03,  2.1255e-02, -3.3708e-03,  3.7745e-02,\n",
      "         -1.5757e-02,  5.3781e-02,  6.6413e-03, -6.3821e-03, -3.0791e-03,\n",
      "          2.7429e-02, -2.0517e-02, -3.7056e-03, -2.2385e-02,  2.0431e-02,\n",
      "         -2.7640e-02,  2.4315e-02, -2.8198e-02, -2.0454e-02,  5.9544e-03,\n",
      "         -5.4145e-03,  2.0578e-02,  2.2125e-03, -2.1326e-03, -4.8510e-02,\n",
      "         -2.4436e-02, -1.1499e-02,  6.9951e-03, -1.1106e-03, -4.0756e-03,\n",
      "          3.7242e-03,  8.3098e-03,  1.3221e-02, -3.5126e-03, -4.2890e-02,\n",
      "         -1.2541e-03, -4.3868e-02, -2.3864e-02, -3.2872e-02, -1.1711e-02,\n",
      "         -2.7307e-02,  1.2745e-02, -2.3027e-02, -1.4159e-02,  7.5414e-03,\n",
      "         -3.2701e-02,  1.2206e-02,  3.1845e-02, -1.1641e-02,  5.5538e-02,\n",
      "         -3.0964e-02,  5.5305e-04, -2.8148e-02,  2.2080e-02,  3.9699e-03,\n",
      "          1.8178e-02,  1.6971e-02,  1.6427e-02,  2.9342e-02, -1.7790e-02,\n",
      "         -1.2468e-04, -8.6668e-03, -1.3291e-02, -2.3291e-02, -2.8825e-04,\n",
      "         -9.1425e-03, -9.9073e-03,  1.3197e-02, -8.8784e-03,  2.2800e-02,\n",
      "          8.7310e-03, -3.5156e-03,  1.6975e-02, -2.0013e-02,  4.8282e-02,\n",
      "         -1.0208e-02,  1.4079e-02, -9.2288e-03,  2.4457e-03,  2.9345e-02,\n",
      "          2.6193e-04, -6.2229e-03, -9.0756e-03,  2.2961e-02, -2.0477e-02,\n",
      "         -2.5764e-03,  3.8735e-03,  4.3175e-02, -7.8649e-03, -4.2231e-03,\n",
      "         -5.6647e-02, -1.4682e-02, -2.6402e-03,  3.1253e-02, -3.0257e-02,\n",
      "         -1.8095e-03, -5.2127e-02,  1.2379e-02, -2.3675e-02, -4.1825e-03,\n",
      "          7.2652e-03,  1.8945e-02,  8.6678e-03, -7.4864e-03, -1.1925e-02,\n",
      "          1.4583e-02,  1.7243e-02,  2.1759e-02,  4.8492e-03,  3.2335e-03,\n",
      "          2.0931e-02,  2.5931e-03,  1.1467e-02,  5.8308e-03, -1.8383e-02,\n",
      "         -8.0678e-03, -2.0508e-02,  1.0464e-02,  1.3944e-02, -5.7743e-03,\n",
      "          1.2985e-02,  2.3641e-02,  8.7921e-03, -8.7642e-03, -6.8043e-03,\n",
      "          4.0340e-02,  2.0641e-02, -3.1069e-02,  2.8575e-02,  1.8127e-02,\n",
      "         -3.5917e-02,  1.8160e-02,  1.4134e-02,  8.1477e-03, -2.1272e-03,\n",
      "          3.3529e-04,  1.1323e-02, -3.5915e-02, -9.3150e-04,  1.3495e-02,\n",
      "          4.2007e-02, -3.8899e-03, -8.3070e-03,  8.9285e-03, -4.3839e-02,\n",
      "          2.6891e-02, -1.7964e-02,  1.7601e-02, -2.8657e-03,  1.1251e-02,\n",
      "          3.0746e-02,  1.0348e-03,  2.7388e-02,  4.5893e-02,  4.0861e-02,\n",
      "          2.0157e-02, -2.3135e-02,  3.3739e-02, -3.8266e-02, -8.6412e-03,\n",
      "         -3.1276e-03,  5.5583e-03, -8.0704e-03, -2.2478e-02,  1.2956e-02,\n",
      "         -1.0361e-03,  1.8296e-03, -3.4555e-02, -1.6070e-02,  2.7102e-03,\n",
      "         -2.8765e-03, -4.8134e-03,  5.7623e-03,  8.9455e-03, -3.4998e-02,\n",
      "          2.4234e-02,  2.8059e-02, -1.2268e-02,  3.1160e-02, -1.2110e-02,\n",
      "         -2.5634e-02, -1.0100e-03, -1.0839e-02, -1.8115e-02,  1.8011e-03,\n",
      "          4.4983e-03,  2.3679e-02,  6.4956e-03,  7.3112e-03, -2.2048e-02,\n",
      "         -4.5728e-02, -2.9022e-02,  2.8561e-02, -3.0641e-03,  8.7610e-03,\n",
      "          2.0635e-02,  1.1261e-02,  9.4196e-03, -2.9363e-02, -1.3937e-02,\n",
      "          6.3678e-03,  1.5642e-02,  3.2227e-03,  1.4486e-02,  1.7190e-02,\n",
      "          6.8754e-03, -9.2427e-04, -4.2888e-03,  1.6877e-02, -1.0478e-02,\n",
      "          1.9089e-03,  2.9486e-02,  6.8492e-03, -1.1854e-02, -2.5113e-02,\n",
      "         -1.8862e-02,  6.8698e-03,  1.8000e-02, -3.5868e-02, -1.1614e-02,\n",
      "         -2.2127e-02,  4.0500e-02, -3.0433e-03, -2.9875e-03, -2.1064e-02,\n",
      "          2.4634e-02, -1.8663e-02, -2.3502e-03, -4.7101e-03, -1.3257e-02,\n",
      "          1.3863e-03,  1.5612e-02, -6.2342e-04, -3.0183e-02, -4.1666e-02,\n",
      "          1.1704e-02,  2.7471e-02, -5.2993e-03,  3.4261e-02, -5.4555e-02,\n",
      "          2.0645e-03,  1.7780e-02, -2.2401e-02, -1.4857e-02, -5.2495e-03,\n",
      "          1.5128e-02, -2.0733e-02, -3.1803e-02,  5.5176e-03,  9.5011e-03,\n",
      "         -1.8773e-02, -1.9447e-03, -7.4683e-03,  1.4625e-02, -3.5454e-02,\n",
      "         -1.5402e-02,  4.8686e-03, -1.4942e-02,  2.9847e-02,  2.9396e-02,\n",
      "          7.6508e-03, -5.4530e-03, -1.2680e-02, -1.0170e-02, -1.9627e-02,\n",
      "          2.2954e-02, -2.8018e-04, -4.3293e-02,  3.2966e-02,  3.0549e-02,\n",
      "         -1.5956e-02, -1.4035e-02, -4.3705e-02, -8.1732e-03, -1.6784e-02,\n",
      "          8.0143e-03, -3.8460e-02,  1.6977e-02,  8.6327e-03, -1.0553e-02,\n",
      "         -7.2463e-04,  3.4718e-02, -4.4268e-02,  7.9327e-03,  1.0291e-02,\n",
      "          2.7670e-02,  1.4503e-02,  1.2568e-02, -3.5767e-03, -1.2803e-02,\n",
      "          2.8641e-02, -1.2218e-02,  4.1497e-02, -2.3419e-03,  1.6298e-02,\n",
      "         -2.0844e-02,  8.8867e-03,  8.9855e-03, -1.4719e-02, -3.0609e-02,\n",
      "          2.7382e-02, -9.7473e-03,  1.3798e-02,  3.2292e-04,  9.1391e-03,\n",
      "         -5.6011e-03, -1.7123e-02, -4.6110e-02,  3.2076e-02, -1.1388e-03,\n",
      "          2.0098e-03, -1.8149e-02,  1.9428e-02,  8.9503e-03,  1.3121e-02,\n",
      "          5.3448e-03,  5.2205e-03,  2.7819e-02,  7.4169e-03, -7.8201e-03,\n",
      "         -6.0245e-03,  1.4489e-02, -6.2738e-03,  1.5855e-02,  2.1718e-02,\n",
      "          2.7230e-02,  2.2528e-02, -6.4539e-03, -1.2495e-02, -1.3101e-02,\n",
      "          2.9288e-02, -9.5697e-03,  7.1705e-04, -1.2682e-02, -2.3468e-02,\n",
      "          1.5650e-03,  4.2882e-03,  4.2638e-02,  2.1320e-02, -1.2173e-02,\n",
      "         -2.8285e-02,  1.7277e-02,  8.2422e-03,  1.2143e-02, -2.4162e-02,\n",
      "         -1.3737e-02, -1.7045e-02, -1.5552e-02, -2.4544e-02, -4.3049e-03,\n",
      "         -2.1005e-02, -3.1157e-03, -4.9926e-03,  1.9296e-02, -4.2392e-02,\n",
      "          2.2009e-02,  3.4838e-02, -1.0386e-02, -1.3218e-03, -2.1059e-03,\n",
      "          1.0575e-03, -4.8794e-03, -9.6916e-03,  7.1032e-03, -1.2329e-02,\n",
      "         -1.9782e-02,  1.1065e-02,  2.6737e-02, -1.2611e-02,  2.2381e-02,\n",
      "          2.5892e-02,  8.7350e-03,  1.0401e-02,  2.5508e-02, -2.0500e-02,\n",
      "         -1.6853e-02,  3.7091e-02,  1.2296e-02,  1.1423e-02, -1.9505e-02,\n",
      "          2.3865e-02,  2.4395e-02, -5.4948e-03, -2.7226e-02,  3.3419e-02,\n",
      "          1.1764e-02,  6.4181e-03,  2.3728e-02,  5.6789e-03,  5.2467e-04,\n",
      "          1.6368e-02, -1.7444e-02,  1.3246e-02, -8.8681e-03,  1.2246e-02,\n",
      "         -1.2089e-03, -1.1887e-02, -3.6699e-02,  4.2942e-02,  9.9881e-03,\n",
      "          1.4471e-02,  3.7963e-02,  1.5576e-02, -2.8110e-02,  2.5959e-02,\n",
      "          9.7471e-03, -4.7281e-03,  2.5490e-02, -4.5005e-03, -1.6774e-02,\n",
      "         -2.6208e-02,  3.2089e-02, -1.8086e-02,  2.9730e-03, -1.6149e-02,\n",
      "         -4.4857e-03,  7.1252e-03, -4.0138e-02,  2.9509e-02,  1.5204e-02,\n",
      "         -5.2920e-03,  1.7764e-02, -2.6204e-02,  1.1875e-02, -4.9999e-03,\n",
      "          6.5883e-04, -2.2449e-02,  3.0806e-02,  2.1022e-02,  2.1269e-02,\n",
      "          1.6074e-02,  3.6845e-04, -8.0772e-03, -6.9724e-03, -2.3832e-02,\n",
      "         -4.7312e-02, -4.2722e-03,  6.9120e-02, -1.4051e-02, -1.2498e-02,\n",
      "          1.8991e-02, -2.4029e-02, -2.0295e-02,  1.0544e-02, -3.8544e-02,\n",
      "          2.3641e-03, -2.5312e-02,  4.1246e-02, -3.3587e-03, -5.1175e-03,\n",
      "         -1.8118e-02, -4.8489e-03, -4.4691e-03, -4.0180e-03,  6.9109e-03,\n",
      "          8.2951e-03, -3.8626e-02, -5.5006e-02, -9.6413e-03,  2.8485e-02,\n",
      "         -1.6817e-02,  1.3015e-02,  2.7391e-02,  3.2975e-02, -1.2255e-02,\n",
      "         -6.6473e-04,  2.7665e-02,  1.8262e-02, -2.9776e-03, -4.3546e-03,\n",
      "         -2.2409e-03, -2.0317e-02, -2.7871e-02,  2.8956e-02, -7.6914e-04,\n",
      "          8.1252e-03, -3.1823e-02, -4.1439e-03,  3.7156e-03,  1.1216e-02,\n",
      "         -2.5217e-02, -1.1701e-02, -3.9106e-03,  4.6674e-03,  1.2896e-02,\n",
      "          1.2367e-03,  7.3299e-03, -1.3300e-02,  2.2317e-03,  1.1444e-02,\n",
      "          2.0627e-02, -1.3029e-02,  1.6818e-02,  1.2302e-02,  3.6634e-02,\n",
      "         -2.4355e-02,  1.7717e-02, -2.6362e-02,  1.8723e-02, -7.3978e-03,\n",
      "          2.2741e-02, -3.4500e-03, -1.5665e-03,  3.2917e-02,  2.5446e-03,\n",
      "          5.7741e-02,  8.9677e-03, -2.5487e-02,  2.1174e-02,  3.2881e-02,\n",
      "         -2.1202e-02, -1.1410e-02, -1.4030e-02, -1.2345e-02, -1.1233e-02,\n",
      "         -1.0546e-02, -4.3930e-02, -2.4132e-03, -1.8648e-02,  1.3398e-02,\n",
      "          1.7718e-02,  1.0308e-02,  8.5986e-04, -3.2698e-03, -2.2775e-02,\n",
      "         -8.9635e-05,  2.5368e-02, -2.0180e-02,  2.7505e-02, -8.1323e-04,\n",
      "         -2.5531e-02, -6.4861e-03,  2.8655e-04,  1.6701e-02, -3.1748e-03,\n",
      "         -2.7630e-03, -3.3760e-03,  1.0301e-02, -2.1078e-02,  1.3197e-02,\n",
      "         -3.9922e-03, -1.6810e-02, -7.6337e-03, -1.4054e-03, -2.0569e-02,\n",
      "         -1.0239e-02,  8.9737e-03, -3.3204e-02,  1.4825e-02, -3.7124e-03,\n",
      "          2.5779e-03,  2.4659e-02, -2.4259e-02,  3.6934e-02, -1.4483e-02,\n",
      "         -2.6320e-02,  2.6214e-02,  1.2681e-02, -5.2167e-02, -2.7317e-02,\n",
      "          1.1475e-02,  3.4639e-02, -3.4618e-02,  6.0217e-03, -2.9060e-02,\n",
      "         -4.9894e-04, -3.8789e-03, -2.6996e-03,  4.2994e-02, -2.8975e-02,\n",
      "          4.8250e-02,  2.0781e-03,  8.0088e-03, -1.7981e-02, -6.7341e-03,\n",
      "          3.2074e-02,  1.6347e-02,  4.2463e-02, -1.2109e-02,  1.0811e-02,\n",
      "          1.6729e-02,  5.0741e-02, -2.0860e-02, -2.7541e-02,  5.2543e-03,\n",
      "          6.3391e-03, -1.5486e-02, -1.3270e-03,  1.5522e-02, -4.1004e-03,\n",
      "         -7.5074e-05,  3.3224e-02,  6.7412e-03, -7.9158e-03,  1.3597e-02,\n",
      "          5.8815e-03, -1.4346e-02, -2.4292e-02,  9.8541e-03,  1.1389e-02,\n",
      "          9.8739e-03, -9.1069e-03, -2.1661e-02, -5.8636e-03,  2.2889e-02,\n",
      "         -1.4413e-03,  8.8861e-03,  4.8192e-02, -2.4693e-03,  1.3725e-02,\n",
      "          2.1362e-03, -2.0102e-02, -3.1164e-02,  1.8475e-02, -2.1169e-02,\n",
      "         -1.9649e-03, -5.9278e-02, -1.3795e-02,  8.2672e-03,  4.7721e-02,\n",
      "         -1.5142e-02, -2.1053e-02, -2.1896e-03,  1.0120e-02, -2.9935e-04,\n",
      "          1.4119e-02,  3.8502e-04, -1.8580e-02, -1.3856e-03,  1.3767e-03,\n",
      "          3.8497e-02, -1.9624e-02,  2.4448e-02, -1.3983e-02,  6.8732e-03,\n",
      "          1.7298e-02,  5.7507e-03, -2.6389e-02, -3.7577e-02,  1.0459e-02,\n",
      "         -1.4432e-02,  1.8216e-03,  2.2104e-02, -3.2634e-02,  1.8471e-02,\n",
      "         -2.2801e-03, -1.0535e-02, -1.1640e-02, -3.3428e-02, -2.7385e-02,\n",
      "          2.4178e-03,  3.6146e-02,  1.7716e-02,  2.1135e-02,  1.5933e-02,\n",
      "         -1.4255e-02,  4.1776e-03, -4.6820e-03, -3.7961e-02, -7.7656e-03,\n",
      "         -6.1303e-03,  4.9737e-03,  2.2793e-02,  2.1253e-02, -1.1755e-02,\n",
      "          2.6883e-02,  1.4867e-02, -1.4776e-02,  4.0126e-03,  1.1899e-02,\n",
      "          2.3491e-02,  6.7053e-03, -1.0441e-02, -5.4201e-03,  2.0346e-02,\n",
      "          1.3269e-02, -2.7458e-02,  1.6644e-03, -1.9784e-02,  1.1367e-02,\n",
      "          1.6615e-02,  5.6641e-03,  1.3556e-02,  3.8642e-02, -1.7332e-02,\n",
      "          2.1143e-02, -4.0711e-02,  2.9935e-02,  1.2353e-02,  3.5286e-04,\n",
      "          6.1346e-02, -1.0590e-03, -1.4601e-02, -6.1740e-03, -1.8523e-02,\n",
      "         -1.4719e-02, -1.3007e-02, -1.8594e-02,  3.7134e-02, -9.0930e-04,\n",
      "          1.5140e-02,  2.7310e-03,  2.5651e-03, -1.5558e-02, -2.4515e-02,\n",
      "          1.2616e-02, -1.1056e-02, -1.3968e-02, -2.1966e-03,  4.4187e-03,\n",
      "          1.0297e-02, -8.6130e-03, -3.0553e-02,  4.2734e-02,  1.2691e-02,\n",
      "         -9.2637e-03,  2.4772e-02,  9.9795e-03, -1.4423e-03, -3.5114e-03,\n",
      "          3.3046e-02,  2.0543e-02,  2.2846e-02,  1.9838e-02,  5.6851e-04,\n",
      "         -3.0677e-02, -1.2797e-03,  1.8748e-02,  1.7907e-02, -1.7273e-02,\n",
      "          8.4129e-03,  5.5966e-03,  1.9793e-03,  1.4046e-02, -6.0594e-06,\n",
      "         -1.2943e-02, -3.1268e-02, -1.7632e-02,  2.2725e-03, -1.1371e-02,\n",
      "          9.6902e-03, -4.4193e-02,  1.4779e-02,  1.2877e-03,  2.0768e-04,\n",
      "         -1.3867e-02, -1.9094e-02, -2.1482e-02,  3.0189e-03,  4.6928e-03,\n",
      "         -1.7416e-02,  4.5793e-02,  2.0135e-02,  7.6512e-03, -1.0038e-02,\n",
      "         -1.2811e-02, -3.4576e-02,  4.7157e-03,  2.3403e-02, -3.8379e-02,\n",
      "          2.4755e-02,  7.7204e-03, -2.9903e-02]])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer = RewardModelTrainer()\n",
    "    trainer.run()\n",
    "    weights = trainer.extract_weights()\n",
    "    print(type(weights))\n",
    "    print(weights.shape)\n",
    "    print(weights[:5])\n",
    "except Exception as e:\n",
    "    log.error(f\"Reward model training failed: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d4cd7",
   "metadata": {},
   "source": [
    "**5. Evaluate**\n",
    "- EvaluationRunner computes ROUGE and BERTScore between generated and reference summaries, and scores the generated summaries with the trained reward model.\n",
    "- Replace the example summaries with real outputs for a full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2699c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:41:23 - INFO     - absl - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-28 19:41:24 - INFO     - evaluation_runner - ROUGE: {'rouge1': np.float64(0.5196078431372549), 'rouge2': np.float64(0.26666666666666666), 'rougeL': np.float64(0.5196078431372549), 'rougeLsum': np.float64(0.5196078431372549)}\n",
      "2025-09-28 19:41:24 - INFO     - evaluation_runner - BERTScore avg: {'precision': 0.924, 'recall': 0.9141, 'f1': 0.919}\n",
      "2025-09-28 19:41:24 - INFO     - evaluation_runner - Reward model scores: [0.05041991174221039, 0.04892019182443619]\n",
      "=== Evaluation Results ===\n",
      "ROUGE (aggregate):\n",
      "  rouge1: 0.5196\n",
      "  rouge2: 0.2667\n",
      "  rougeL: 0.5196\n",
      "  rougeLsum: 0.5196\n",
      "\n",
      "BERTScore (averaged):\n",
      "  precision: 0.9240\n",
      "  recall: 0.9141\n",
      "  f1: 0.9190\n",
      "\n",
      "BERTScore (first 3 raw values per metric):\n",
      "  precision: [0.953811764717102, 0.894155740737915]\n",
      "  recall: [0.9362310171127319, 0.8918960690498352]\n",
      "  f1: [0.9449396133422852, 0.8930245041847229]\n",
      "\n",
      "Reward model scores:\n",
      "[0.05041991174221039, 0.04892019182443619]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    generated_summaries = [\n",
    "        \"This is a generated summary of the paper.\",\n",
    "        \"Another candidate summary with different wording.\",\n",
    "    ]\n",
    "    reference_summaries = [\n",
    "        \"This is the gold reference summary of the paper.\",\n",
    "        \"Another gold reference summary for comparison.\",\n",
    "    ]\n",
    "\n",
    "    runner = EvaluationRunner()\n",
    "\n",
    "    rouge, bertscore_raw, bertscore_avg = runner.compute_metrics(\n",
    "        generated_summaries, reference_summaries\n",
    "    )\n",
    "    reward_scores = runner.score_with_reward_model(generated_summaries)\n",
    "\n",
    "    print(\"=== Evaluation Results ===\")\n",
    "    print(\"ROUGE (aggregate):\")\n",
    "    for k, v in rouge.items():\n",
    "        print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\nBERTScore (averaged):\")\n",
    "    for k, v in bertscore_avg.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\nBERTScore (first 3 raw values per metric):\")\n",
    "    for k, v in bertscore_raw.items():\n",
    "        if isinstance(v, list):\n",
    "            print(f\"  {k}: {v[:3]}\")\n",
    "\n",
    "    print(\"\\nReward model scores:\")\n",
    "    print(reward_scores)\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Evaluation failed: {e}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
