"""
Voice Assistant API

This FastAPI application accepts short audio clips and returns:
- A speech-to-text transcription (ASR) using Whisper.
- A text response generated by a Hugging Face LLM.
- A synthesized speech reply using gTTS.

Enhancements:
- Conversation memory: rolling context preserved and exposed via /memory.
- Prompt formatting: consistent, role-based prompt with a system instruction.
- Async optimization: blocking work (ASR/LLM/TTS, file I/O) is offloaded via asyncio.to_thread.

Environment variables:
- HUGGINGFACE_TOKEN (required): Token for HF model access.
- TRANSCRIPT_FILE (optional): Path for JSON transcript; default "transcript.json".
- RESPONSE_FOLDER (optional): Folder for TTS outputs; default "response".
- PORT (optional): Server port; default 8000.
"""

import json
import logging
import os
import subprocess
import tempfile
import wave
import asyncio
from contextlib import asynccontextmanager, closing
from typing import List, Dict

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from transformers import pipeline
from dotenv import load_dotenv
import whisper
from gtts import gTTS
import utils

# -----------------------------------------------------------------------------
# Configuration & logging
# -----------------------------------------------------------------------------

load_dotenv()
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
TRANSCRIPT_FILE = os.getenv("TRANSCRIPT_FILE", "transcript.json")
RESPONSE_FOLDER = os.getenv("RESPONSE_FOLDER", "response")

if not HUGGINGFACE_TOKEN:
    raise RuntimeError("Missing Hugging Face token. Please set HUGGINGFACE_TOKEN.")

# Configure structured logging for service observability
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("voice-assistant")

# -----------------------------------------------------------------------------
# Global state
# -----------------------------------------------------------------------------

asr_model = None  # Whisper ASR model instance (loaded at startup)
llm = None       # Hugging Face text-generation pipeline (loaded at startup)

# In-memory rolling conversation history. Each item: {"role": "user"|"assistant", "text": str}
conversation_history: List[Dict[str, str]] = []
# Lock protects concurrent access/mutation of conversation_history
history_lock = asyncio.Lock()
# Limit to last N turns (a turn is user+assistant, thus history capped at 2 * MAX_TURNS messages)
MAX_TURNS = 5

# Accepted audio MIME types for upload validation
ACCEPTED_AUDIO_TYPES = {
    "audio/mpeg",
    "audio/wav",
    "audio/x-wav",
    "audio/x-m4a",
    "audio/ogg",
    "audio/webm",
    "application/octet-stream",
}

# -----------------------------------------------------------------------------
# Prompt formatting logic
# -----------------------------------------------------------------------------

# Single system instruction to steer the model's behavior
DEFAULT_SYSTEM = (
    "You are a concise, helpful voice assistant. "
    "Use the conversation history to stay in context. "
    "If information is missing, ask a brief clarifying question."
)

def normalize_role(role: str) -> str:
    """
    Normalize free-form role labels into a consistent set the model expects.
    """
    role = role.lower().strip()
    if role in ("user", "human"):
        return "User"
    if role in ("assistant", "bot", "ai"):
        return "Assistant"
    if role in ("system",):
        return "System"
    return role.capitalize() or "User"

def get_recent_history() -> List[Dict[str, str]]:
    """
    Return the most recent window of messages, capped to 2 * MAX_TURNS.
    """
    return conversation_history[-(2 * MAX_TURNS):]

def format_prompt(user_text: str, system_message: str = DEFAULT_SYSTEM) -> str:
    """
    Construct a role-formatted prompt with:
    - A top-level system instruction.
    - The rolling conversation history.
    - The latest user message and an 'Assistant:' cue.

    This yields a stable, predictable input for the LLM.
    """
    lines = [f"System: {system_message}", ""]
    for m in get_recent_history():
        lines.append(f"{normalize_role(m['role'])}: {m['text'].strip()}")
    lines.append(f"User: {user_text.strip()}")
    lines.append("Assistant:")
    prompt = "\n".join(lines)
    logger.debug("Formatted prompt (first 300 chars): %s", prompt[:300])
    return prompt

# -----------------------------------------------------------------------------
# Application lifecycle: load models at startup
# -----------------------------------------------------------------------------

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Load heavy resources (ASR, LLM) once at startup and ensure output directory exists.
    """
    global asr_model, llm
    logger.info("Initializing application resources...")
    asr_model = whisper.load_model("small")
    logger.info("Whisper ASR model loaded.")
    llm = pipeline("text-generation", model="meta-llama/Llama-3.1-8B-Instruct", token=HUGGINGFACE_TOKEN)
    logger.info("LLM pipeline initialized: meta-llama/Llama-3.1-8B-Instruct")
    utils.get_script_dir(RESPONSE_FOLDER)
    logger.info("Response folder ensured: %s", RESPONSE_FOLDER)
    yield
    logger.info("Application shutdown complete.")

app = FastAPI(title="Voice Assistant", version="2.2", lifespan=lifespan)

# -----------------------------------------------------------------------------
# Health and readiness checks
# -----------------------------------------------------------------------------

def check_ffmpeg_installed() -> bool:
    """
    Verify ffmpeg availability on PATH.
    """
    try:
        subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        return True
    except Exception as e:
        logger.error("FFmpeg check failed: %s", e)
        return False

def check_asr_model_loaded() -> bool:
    """
    Confirm Whisper can process a tiny silent WAV without raising errors.
    """
    path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            path = tmp.name
            with closing(wave.open(path, "w")) as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(16000)
                wf.writeframes(b"\x00\x00" * 16000)
        asr_model.transcribe(path, verbose=False)  # type: ignore[union-attr]
        return True
    except Exception as e:
        logger.error("ASR readiness check failed: %s", e)
        return False
    finally:
        if path:
            try:
                os.remove(path)
            except Exception:
                pass

def check_llm_ready() -> bool:
    """
    Confirm the text-generation pipeline produces an output.
    """
    try:
        result = llm("Hello", max_new_tokens=5)  # type: ignore[operator]
        ok = bool(result and result[0].get("generated_text"))
        if not ok:
            logger.error("LLM readiness produced empty output.")
        return ok
    except Exception as e:
        logger.error("LLM readiness check failed: %s", e)
        return False

def check_gtts_ready() -> bool:
    """
    Confirm gTTS can synthesize and write a tiny audio file.
    """
    path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            path = tmp.name
        gTTS("OK", lang="en").save(path)
        return os.path.exists(path) and os.path.getsize(path) > 0
    except Exception as e:
        logger.error("gTTS readiness check failed: %s", e)
        return False
    finally:
        if path:
            try:
                os.remove(path)
            except Exception:
                pass

def ensure_ready() -> None:
    """
    Aggregate readiness checks; raise HTTP 500 if any subsystem is unavailable.
    """
    if not check_ffmpeg_installed():
        raise HTTPException(500, "FFmpeg not found")
    if not check_asr_model_loaded():
        raise HTTPException(500, "ASR check failed")
    if not check_llm_ready():
        raise HTTPException(500, "LLM not generating")
    if not check_gtts_ready():
        raise HTTPException(500, "gTTS failed")

# -----------------------------------------------------------------------------
# I/O utilities and blocking-task wrappers
# -----------------------------------------------------------------------------

async def save_temp_audio(file: UploadFile) -> str:
    """
    Persist the uploaded audio to a temporary file and return its path.
    Validates MIME type and extension against common audio formats.
    """
    if file.content_type not in ACCEPTED_AUDIO_TYPES and not file.filename.lower().endswith(
        (".mp3", ".wav", ".m4a", ".ogg", ".webm")
    ):
        logger.warning("Unsupported file: %s (%s)", file.filename, file.content_type)
        raise HTTPException(400, "Unsupported file format")

    suffix = os.path.splitext(file.filename)[1] or ".bin"
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        data = await file.read()
        tmp.write(data)
        logger.info("Uploaded audio saved to %s (%d bytes)", tmp.name, len(data))
        return tmp.name

def transcribe_audio_sync(audio_path: str) -> str:
    """
    Blocking ASR. Reads audio_path with Whisper and returns the transcribed text.
    Ensures the temp file is removed afterward.
    """
    logger.info("Transcribing: %s", audio_path)
    try:
        result = asr_model.transcribe(audio_path, verbose=False)  # type: ignore[union-attr]
        text = result.get("text", "").strip()
        logger.info("Transcription complete (chars=%d)", len(text))
        return text
    except Exception as e:
        logger.error("Transcription failed: %s", e)
        raise RuntimeError(f"Transcription failed: {e}")
    finally:
        try:
            os.remove(audio_path)
            logger.debug("Temp audio removed: %s", audio_path)
        except Exception:
            pass

def llm_generate_sync(prompt: str) -> str:
    """
    Blocking LLM generation. Returns the assistant's first continuation line.
    """
    logger.info("Generating response (prompt_len=%d)", len(prompt))
    outputs = llm(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)  # type: ignore[operator]
    full_text = outputs[0]["generated_text"]
    reply = full_text[len(prompt):].strip()
    reply = reply.split("\n")[0].strip()
    logger.info("Generated reply (chars=%d)", len(reply))
    logger.debug("Reply preview: %s", reply[:200])
    return reply

def synthesize_speech_sync(text: str, input_filename: str) -> str:
    """
    Blocking TTS. Synthesizes text to MP3 in RESPONSE_FOLDER using input file's base name.
    """
    base_name = os.path.splitext(os.path.basename(input_filename))[0] or "response"
    out_path = utils.get_file_path(f"{base_name}_response.mp3", RESPONSE_FOLDER)
    logger.info("Synthesizing TTS to %s", out_path)
    gTTS(text=text, lang="en").save(out_path)
    return out_path

def save_chat_transcript_sync() -> str:
    """
    Persist the rolling conversation history as JSON to TRANSCRIPT_FILE.
    """
    path = utils.get_file_path(TRANSCRIPT_FILE)
    recent_turns = get_recent_history()
    with open(path, "w", encoding="utf-8") as f:
        json.dump(recent_turns, f, indent=2, ensure_ascii=False)
    logger.info("Transcript saved (%d messages) -> %s", len(recent_turns), path)
    return path

# -----------------------------------------------------------------------------
# API endpoints
# -----------------------------------------------------------------------------

@app.post("/chat/")
async def chat(file: UploadFile = File(...)):
    """
    Process an uploaded audio file end-to-end:
    - Save temp audio
    - Transcribe (ASR)
    - Update memory and build prompt
    - Generate reply (LLM)
    - Update memory and persist transcript
    - Synthesize TTS and return paths and texts
    """
    logger.info("Received /chat request")
    ensure_ready()

    # Save uploaded audio
    audio_path = await save_temp_audio(file)

    # Run ASR off-thread
    user_text = await asyncio.to_thread(transcribe_audio_sync, audio_path)

    # Update memory with the user message and format the prompt
    async with history_lock:
        conversation_history.append({"role": "user", "text": user_text})
        if len(conversation_history) > MAX_TURNS * 2:
            del conversation_history[: len(conversation_history) - MAX_TURNS * 2]
        prompt = format_prompt(user_text)

    # Generate reply off-thread
    assistant_reply = await asyncio.to_thread(llm_generate_sync, prompt)

    # Update memory with the assistant message and snapshot the window
    async with history_lock:
        conversation_history.append({"role": "assistant", "text": assistant_reply})
        if len(conversation_history) > MAX_TURNS * 2:
            del conversation_history[: len(conversation_history) - MAX_TURNS * 2]
        memory_snapshot = list(get_recent_history())

    # Persist transcript off-thread
    await asyncio.to_thread(save_chat_transcript_sync)

    # Synthesize TTS off-thread
    tts_path = await asyncio.to_thread(synthesize_speech_sync, assistant_reply, file.filename)

    logger.info("Completed /chat request")
    return JSONResponse(
        content={
            "transcription": user_text,
            "response": assistant_reply,
            "tts_audio": tts_path,
            "conversation_memory": memory_snapshot,
            "debug_prompt": prompt,
        }
    )

@app.get("/memory")
async def memory():
    """
    Return the current rolling conversation window (for debugging/inspection).
    """
    async with history_lock:
        snapshot = get_recent_history()
    logger.info("Returned memory snapshot (%d messages)", len(snapshot))
    return snapshot

@app.delete("/memory")
async def clear_memory():
    """
    Clear the in-memory history and delete the transcript file if present.
    """
    async with history_lock:
        count = len(conversation_history)
        conversation_history.clear()
    logger.info("Cleared memory (removed %d messages)", count)
    try:
        path = utils.get_file_path(TRANSCRIPT_FILE)
        if os.path.exists(path):
            os.remove(path)
            logger.info("Deleted transcript file: %s", path)
    except Exception as e:
        logger.warning("Failed to delete transcript file: %s", e)
    return {"cleared": True}

@app.get("/health")
def health():
    """
    Health probe that checks FFmpeg, ASR, LLM, and gTTS readiness.
    """
    logger.info("Health check requested")
    status = {
        "ffmpeg_installed": check_ffmpeg_installed(),
        "asr_model_loaded": check_asr_model_loaded(),
        "llm_ready": check_llm_ready(),
        "gtts_ready": check_gtts_ready(),
    }
    logger.info("Health status: %s", status)
    return status

# -----------------------------------------------------------------------------
# Local run entrypoint
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8000"))
    logger.info("Starting server on 0.0.0.0:%d", port)
    uvicorn.run(app, host="0.0.0.0", port=port)
