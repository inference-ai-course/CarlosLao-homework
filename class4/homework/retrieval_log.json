[
  {
    "timestamp": "2025-08-17T23:36:01.354487",
    "query": "Model",
    "results": [
      {
        "chunk_id": "2508.10142v1_chunk20.txt",
        "text": "that you \u2019 re taking the most optimal actions to guess the s e c r e t word . Al l of your guesses should be structured as follows : <attempt>Your guess</attempt> Make sure that a l l of your attempts are s t r i c t l y from the vocabulary s p e c i f i e d above . Start with your f i r s t attempt and make sure to follow the format s p e c i f i e d above . model <attempt>RETRO</attempt> user green , grey , grey , grey , yellow . . . 20"
      },
      {
        "chunk_id": "2508.10308v1_chunk15.txt",
        "text": "recipe generalizes across model families. 11 Topic Cov. Sem.Sim. Cor. of Claims Abs. of Hal. Ana. Depth Act.Ins. Adh. to Guide. DeepReviewer 3.94 3.83 3.92 4.03 3.80 3.70 3.94 CycleReviewer 3.74 3.67 3.72 3.87 3.00 2.86 3.73 ReviewRL 4.36 4.16 4.52 4.62 4.18 4.12 4.37 ReviewRL (w/o RL) 4.07 4.01 4.18 4.15 3.99 3.97 4.08 ReviewRL (w/o Retrieval) 4.12 4.07 4.35 4.35 4.04 4.03 4.15 ReviewRL(w/o GenRM) 4.05 3.99 4.17 4.18 3.96 3.90 4.06 Table 5: Model-based evaluation scores on seven quality dimensions for baselines, ablation variants, and our proposed REVIEWRL system (higher is better). This table contains the same quantitative results visualised in Figure 1. GENERATE QUERIES PROMPT You are now an academic paper review expert capable of conducting thorough analyses of research papers to provide the most reliable review results. You are now allowed to use the search tool to obtain background information on the paper\u2014please provide three different questions. I will assist you with the search. Please present the three questions in the following format: 1.xxx 2.xxx 3.xxx Do not include any additional content. Here is a research paper: {paper} Table 6: Prompt for Generate queries prompt RETRIEVAL SYSTEM PROMPT You are an academic expert who specializes in answering questions by retrieving information from arXiv. Table 7: Retrieval system prompt OPEN SOURCE MODEL EVALUATION PROMPT Here is a research paper: {paper} You are a senior reviewer for top-tier AI conferences (NeurIPS/ICML/CVPR/ACL). You must be strict and professional enough. Read the Paper Carefully: Analyze each paragraph of each section critically. Identify any logical flaws, technical inconsistencies, missing citations, or unclear explanations. Detailed Paragraph-by-Paragraph Review: Provide a detailed critique of each paragraph in every section. If a paragraph contains multiple issues, list them separately. Highlight strengths, but be critical of weaknesses. Use <think> </think> tags to document your detailed thought process during the review. Comprehensive Structured Review: After the detailed paragraph-by-paragraph critique, provide a structured review using the following format: ## Summary (3\u20135 sentences: core contribution + methodology) ## Strengths - Bullet points focusing on: Technical merit | Novelty | Empirical validation ## Weaknesses - Bullet points labeled [Major] or [Minor]: Methodology flaws | Experimental issues | Presentation problems ## Rating One integer from: [1, 3, 5, 6, 8, 10] (10=Strong Accept; 8=Accept; 6=Borderline Accept; 5=Borderline Reject; 3=Reject; 1=Strong Reject) Table 8: Open source model evaluation prompt 12 RETRIEVAL EFFECTIVENESS EVALUATION PROMPT Factual Accuracy: You are an extremely meticulous domain expert. Task: Compare Answer-A (which uses retrieval) with Answer-B (which does not) only on factual accuracy / faithfulness. Scoring rule \u2022 If Answer-A is fully correct or clearly more accurate than Answer-B \u2192output 0 \u2022 If Answer-B is clearly more accurate \u2192output 1 \u2022 If both are equally correct but Answer-A supplies extra verifiable details, still treat Answer-A as better \u2192output 0 Output format: a single character 0 or 1\u2014nothing else. Evidence Quality: You are an academic reviewer. Judge the two answers solely on the quality and usefulness of their evidence or citations. Decision rule 0 = Answer-A provides stronger or clearer evidence / citations. 1 = Answer-B provides stronger or"
      },
      {
        "chunk_id": "2508.10751v1_chunk29.txt",
        "text": "better understanding, we present a test instance in Figure 15. MathVision. MathVision selects 3,040 high-quality problems from human math competitions, each accompanied by relevant images. Solving these problems requires both careful interpretation of the visual information and rigorous mathematical reasoning. MathVision provides a benchmark for assessing a model\u2019s multimodal understanding as well as its ability to perform rigorous mathematical reasoning. For better understanding, we present a test instance in Figure 16. MMMU. MMMU includes college-level reasoning and comprehension tasks across six academic subjects, including Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. Moreover, MMMU includes a wide range of image types, enabling a comprehensive assessment of a model\u2019s capability to process and reason over different forms of visual information. For better understanding, we present a test instance in Figure 17. A.2 Implementation Details Training. In our experiment, we adapt Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct as the backbone model and train it through DAPO. To enhance the efficiency of the training process, we only retain the clip-higher (i.e., \ud835\udf00low = 0.2 and \ud835\udf00high = 0.28) and token-level policy gradient loss, and remove other optimizations. For the training hyper-parameters, we set the learning rate for the policy model as 1 \u00d7 10\u22126 with 10 warmup steps, and employ 128, 32, and 32 as prompt batch size \ud835\udc35\ud835\udc46prompt, mini-batch size \ud835\udc35\ud835\udc46mini, and rollout times 25 You need to solve the following maze. '*' denotes the wall that you cannot walk through, '.' denotes available area that you can walk through. 'S' denotes the starting point, 'E' denotes the destination. You need to start from the starting point and cross through the available area to reach the destination. There are four movement actions, including Left, Right, Up, Down. You need to use L to denote Left movement, R to denote Right movement, U to denote Up movement, and D to denote Down movement. You can analysis the maze to find the correct path, and you should write the final path in the <answer> </answer>, e.g., <answer> LLRRDUL </answer>. ## Maze ********* *.......* *.*****.* *E......* *.*.*.*.* *.*.*...* *.*.*.*.* *S......* ********* Now try to analyze the maze and put the final path in the <answer> </answer>. <answer> UUUU <answer> Question Ground Truth Figure 14 An example of Maze Task. \ud835\udc5brollout, respectively. For the reward, the responses that pass the verification (named as positive responses) will be assigned the positive reward \ud835\udc45pos = 1, while the other responses (named as negative responses) will be endowed with the negative reward \ud835\udc45neg = 0. Additionally, we do not employ any regularization methods, such as KL or Entropy regularization. Evaluation. To evaluate the performance of LLMs, we adopt 1.0 and 0.95 as the Temperature and Top_P. For each question, we sample 32 responses from LLMs for the Maze task and sample 8 responses from LLMs for other tasks, and then utilize the sampled response to compute the Pass@1 and Pass@k scores. B Details of Analytical Derivation We present the details of the analytical derivation procedure mentioned in Section 2.4, including the derivation of the average of"
      }
    ]
  },
  {
    "timestamp": "2025-08-17T23:36:37.466707",
    "query": "transformer",
    "results": [
      {
        "chunk_id": "2508.10553v1_chunk13.txt",
        "text": "Center of Friedrich-Alexander- Universit\u00a8at (FAU) Erlangen-N\u00a8urnberg for providing access to HPC infrastructure during the evaluation phase. Furthermore, we gratefully acknowledge the collaboration with the NDIF team in the United States, in particular David Bau (North- eastern University Boston) and Emma Bortz (Northeastern 8 University Boston), whose technical advice and exchange greatly contributed to the success of the eDIF feasibility study. REFERENCES [1] N. Nanda and J. Bloom, \u201cTransformerLens,\u201d GitHub reposi- tory, 2022. [Online]. Available: https://github.com/TransformerLensOrg/ TransformerLens [2] C. Tang, B. Lake, and M. Jazayeri, \u201cAn explainable transformer circuit for compositional generalization,\u201d arXiv preprint arXiv:2502.15801, Feb. 2025. [Online]. Available: http://arxiv.org/abs/2502.15801 [3] L. Bereska and E. Gavves, \u201cMechanistic interpretability for AI safety \u2013 A review,\u201d arXiv preprint arXiv:2404.14082, Apr. 2024. [Online]. Available: http://arxiv.org/abs/2404.14082 [4] R. Bommasani, K. Klyman, S. Kapoor, S. Longpre, B. Xiong, N. Maslej, and P. Liang, \u201cThe 2024 foundation model transparency index,\u201d arXiv preprint arXiv:2407.12929, Mar. 2025. [Online]. Available: http://arxiv. org/abs/2407.12929 [5] R. Sapkota, S. Raza, and M. Karkee, \u201cComprehensive analysis of transparency and accessibility of ChatGPT, DeepSeek, and other SoTA large language models,\u201d arXiv preprint arXiv:2502.18505, Feb. 2025. [Online]. Available: http://arxiv.org/abs/2502.18505 [6] J. Fiotto-Kaufman et al., \u201cNNsight and NDIF: Democratizing ac- cess to open-weight foundation model internals,\u201d arXiv preprint arXiv:2407.14561, Apr. 2025. [Online]. Available: http://arxiv.org/abs/ 2407.14561 [7] Z. Wu, A. Geiger, A. Arora, J. Huang, Z. Wang, N. D. Goodman, C. D. Manning, and C. Potts, \u201cpyvene: A library for understand- ing and improving PyTorch models via interventions,\u201d arXiv preprint arXiv:2403.07809, Mar. 2024. [Online]. Available: http://arxiv.org/abs/ 2403.07809 [8] D. Bau, \u201cbaukit: Tools for inspecting and intervening in PyTorch models,\u201d GitHub repository, 2022. [Online]. Available: https://github. com/davidbau/baukit [9] LUMI Supercomputer, \u201cLUMI: One of the EuroHPC world-class su- percomputers,\u201d [Online]. Available: https://lumi-supercomputer.eu [Ac- cessed: Jul. 18, 2025]. [10] Barcelona Supercomputing Center, \u201cMareNostrum 5,\u201d [Online]. Avail- able: https://www.bsc.es/marenostrum/marenostrum-5 [Accessed: Jul. 18, 2025]. [11] NVIDIA, \u201cNVIDIA RTX A6000 Datasheet,\u201d 2023. [Online]. Avail- able: https://www.nvidia.com/en-us/design-visualization/rtx-a6000/ [Ac- cessed: Jul. 18, 2025]. [12] Canonical, \u201cUbuntu 22.04 LTS released,\u201d Apr. 21, 2022. [Online]. Avail- able: https://canonical.com/blog/ubuntu-22-04-lts-released [Accessed: Jul. 18, 2025]. [13] J. Nickolls, I. Buck, M. Garland, and K. Skadron, \u201cScalable parallel programming with CUDA,\u201d ACM Queue, vol. 6, no. 2, pp. 40\u201353, 2008. DOI: 10.1145/1365490.1365500. [14] Anyscale Inc., \u201cOverview \u2014 Ray,\u201d [Online]. Available: https://docs.ray. io/en/latest/ray-overview/index.html [Accessed: Jul. 12, 2025]. [15] MinIO Inc., \u201cMinIO: High-performance, S3 compatible object storage,\u201d [Online]. Available: https://min.io/ [Accessed: Jul. 12, 2025]. [16] Grafana, \u201cGrafana Loki\u201d [Online]. Available: https://grafana.com/oss/ loki/ [Accessed: Jul. 22, 2025]. [17] Prometheus, \u201cPrometheus: Overview,\u201d [Online]. Available: https:// prometheus.io/docs/introduction/overview/ [Accessed: Jul. 22, 2025]. [18] influxdata, \u201cInfluxDB Overview\u201d [Online]. Available: https://www. influxdata.com/products/influxdb-overview/ [Accessed: Jul. 22, 2025]. [19] Grafana, \u201cGrafana\u201d [Online]. Available: https://grafana.com/grafana/ [Accessed: Jul. 22, 2025]. [20] NVIDIA Corporation, \u201cNCCL \u2013 NVIDIA Collective Communications Library,\u201d [Online]. Available: https://developer.nvidia.com/nccl [Ac- cessed: Aug. 3, 2025]. [21] Advanced Micro Devices, Inc., \u201cRCCL \u2013 ROCm Collective Communica- tion Library,\u201d [Online]. Available: https://rocm.docs.amd.com/projects/ rccl [Accessed: Aug. 3, 2025]. [22] J. Fiotto-Kaufman et al., NDIF, \u201cNDIF: Development Guide,\u201d [Online]. Available: https://github.com/ndif-team/ndif [Accessed: Jul. 12, 2025]. [23] Hugging Face, \u201cGPT-2,\u201d [Online]. Available: https://huggingface.co/ openai-community/gpt2 [Accessed: Jul. 22, 2025]. [24] Hugging Face, \u201cDeepSeek-R1-Distill-Llama-8B,\u201d [Online]. Available: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B [Accessed: Jul. 22, 2025]."
      },
      {
        "chunk_id": "2508.10222v1_chunk8.txt",
        "text": "that received an F1-score of 0, but often at a slight cost to other classes. Figure 3. Learning Curve for Transformer Network Lastly, we look at our learning curve showing our loss over epochs for training/validation data as shown in Fig- ure 3. We see that we have lower loss than that of our feedforward network, but we have a gap between our train- ing/validation loss that continues to worsen over epochs. This suggests that our model is overfitting as validation loss remains steadfast and training loss continues to de- crease. Despite extensive hyperparameter tuning to reduce model complexity (fewer transformer layers, smaller em- bedding dimensions, fewer attention heads) and adjusting learning rate and weight decay; the generalization gap re- mained. Nevertheless, the transformer still delivers strong performance. With better regularization or data augmenta- tion to counteract overfitting and class imbalance, it\u2019s likely the transformer could outperform the feedforward network more substantially. Label Precision Recall F1-Score Support :heart: 0.92 0.66 0.77 10798 :heart eyes: 0.18 0.11 0.14 4830 :joy: 0.25 0.31 0.28 4534 :two hearts: 0.17 0.06 0.09 2605 :fire: 0.46 0.28 0.35 3716 :blush: 0.09 0.03 0.05 1613 :sunglasses: 0.17 0.08 0.10 1996 :sparkles: 0.20 0.12 0.15 2749 :blue heart: 0.09 0.11 0.10 1549 :kiss: 0.09 0.25 0.13 1175 :camera: 0.25 0.49 0.33 1432 :flag-us: 0.47 0.35 0.40 1949 :sunny: 0.26 0.43 0.33 1265 :purple heart: 0.05 0.13 0.08 1114 :wink: 0.06 0.24 0.10 1306 :100: 0.08 0.08 0.08 1244 :grin: 0.05 0.10 0.07 1153 :christmas tree: 0.51 0.70 0.59 1545 :camera with flash: 0.33 0.07 0.12 2417 :stuck out tongue winking eye: 0.04 0.08 0.05 1010 Accuracy 0.30 50000 Macro Avg 0.24 0.23 0.21 50000 Weighted Avg 0.38 0.30 0.32 50000 Table 3. Emoji Classification Report (Transformer Model) 3.4. Convolutional Neural Network Initially overfitting with a test accuracy of 35% and a training accurarcy of 72%, the model was redesigned in or- der to achieve more balanced results and healthier training patterns. The tuning process included speeding up the learn- ing rate from 1e-4 to 1e-3, epochs was increased to 20, but early stopping was implemented with a patience of 3 (ear- lier stoppage than default patience of 5). However, the ac- curacy would peak on the validation data and then drop af- ter 5 epochs while training accuracy increased rapidly. But due to early stopping the run was getting cut off around 8 epochs, so it made sense to drop epochs down to 5 since the model was learning too fast and much that it was starting to overfit after that point. This resulted in a very balanced final accuracy of 31.35% (training) and 32.7% (test). Our model success while including accuracy also was focused on weighted F-1 score in order to account for class imbalance. The final CNN had an overall accuracy 33% with a weighted F-1 score of 0.34, which was a subtle im- provement over the transformer and the feedforward base- line. The learning curve [4] shows that the model undergoes stable learning with consistent training loss across the five epochs. There is a very small"
      },
      {
        "chunk_id": "2508.10824v1_chunk25.txt",
        "text": "(2025a)), and hierarchical controllers for reasoning (e.g., HRM Wang et al. (2025a)), reflecting a shift from static pattern recognition to adaptive, experience-driven intelligence. Architecture: hybrid dominance. Parameter-encoded memory offers immediate access but risks catas- trophic interference when updated; state-based memory supports rapid adaptation but is capacity-limited; external stores scale but add retrieval/consistency overhead. Hybrid designs increasingly combine these 19 Table 2: Comprehensive feature matrix for memory-augmented Transformer models: Evolution from 2019- 2025 Year Model Architecture Generality Memory Dynamics Management Storage Class Integration Method Backbone Compatibility Input Modality Memory Span Write Trigger Plasticity Memory Scope Retrieval Mechanism Forgetting Mechanism 2019 Transformer-XL (Dai et al., 2019) S Wrp \u00d7 T S Stc F Lyr Attn FIFO Compressive Transformer Rae et al. (2019) S Wrp \u00d7 T M Stc F Lyr Attn Dec+FIFO 2020 SAM (Le et al., 2020) E Plg \u2713 M A Pol TT Gbl Outer Rst Memformer Wu et al. (2020) H(SE) Plg \u2713 M L G TT Gbl Attn Dec 2021 ABC Peng et al. (2021) E Plg \u2713 T S Pol F Gbl Pol \u2014 WorkMATe (Kruijne et al., 2021) S Plg \u2713 T S Pol TT Lyr Attn \u2014 2022 EMAT (Wu et al., 2022b) E Plg \u2713 T L Stc F Gbl MIPS \u2014 RETRO (Borgeaud et al., 2022) E Plg \u2713 T L Stc F Gbl kNN \u2014 DSI (Tay et al., 2022) P Bsp \u00d7 T L Stc F Lyr Attn \u2014 Memorizing Transf. Wu et al. (2022a) E Plg \u2713 T L Stc F Gbl kNN \u2014 MemBART (Wu & Yu, 2022) S Plg \u00d7 T L G TT Hrch Dual Rst 2023 LongMem (Wang et al., 2023) E Plg \u2713 T L G TT Gbl Attn Prn MemGPT Packer et al. (2023) E Plg \u2713 T M G TT Hrch kNN LRU Think-in-Memory (Liu et al., 2023) E Plg \u2713 T L G TT Gbl Trip Dec AdaTape (Xue et al., 2023) H(PS) Plg \u2713 T S Pol F Lyr Tape \u2014 MemWalker (Chen et al., 2023) E Bsp \u2713 T M Pol TT Hrch Tree Rst AiT (Sun et al., 2023) H Plg \u2713 M A Pol TT Gbl Assoc Dec MeMOTR Gao & Wang (2023) H(SE) Bsp \u00d7 M L Stc TT Gbl Attn Dec 2024 MemoryBank Zhong et al. (2024a) E Plg \u2713 T L G TT Hrch kNN Dec TransformerFAM Hwang et al. (2024) S Plg \u2713 T S Stc F Lyr Attn \u2014 HMT He et al. (2024a) H(SE) Plg \u2713 T L Stc F Hrch Attn Dec MemoryLLM (Wang et al., 2024b) H(SE) Plg \u2713 T L Stc TT Gbl Attn Dec HippoRAG Guti\u00e9rrez et al. (2024) E Plg \u2713 T L Stc TT Gbl Graph Rst MATTER Lee et al. (2024) H(PE) Wrp \u2713 T M Stc F Gbl MIPS \u2014 Memory3 Yang et al. (2024) H Plg \u2713 T M Stc+Pol F Gbl kNN \u2014 ARMT (Rodkin et al., 2024) E Plg \u2713 T A Pol TT Hrch Assoc Cyc MemLong Liu et al. (2024) E Plg \u2713 T L G TT Gbl kNN Prn Schr\u00f6dinger\u2019s"
      }
    ]
  },
  {
    "timestamp": "2025-08-17T23:37:01.955381",
    "query": "controller",
    "results": [
      {
        "chunk_id": "2508.10839v1_chunk16.txt",
        "text": "You always respond by wrapping your thoughts in the \u21a9\u2192correct XML tags. Max response length: 200 words (tokens). <|im_end|> <|im_start|>user {environment_prompt} [Game State] {game_state} <|im_end|> <|im_start|>assistant Respond using ONLY valid XML with <observe>...</observe>, <think>...</think>, < \u21a9\u2192plan>...</plan>, and <action>...</action> tags. Stop responding after the \u21a9\u2192 </action> tag. [Response Template] <observe>{Describe the situation concisely}</observe> <think>{Think about the situation - what you should aim to do and what you \u21a9\u2192should avoid doing.}</think> <plan>{Describe the immediate plan you will follow to achieve your goal and \u21a9\u2192avoid bad outcomes. Be explicit about the actions you will take: name the \u21a9\u2192 actions.}</plan> <action>{Up/Down/Left/Right || Up/Down/Left/Right...}</action> <|im_end|> <|im_start|>assistant DQN Agent We trained a Deep Q-Network (Mnih et al., 2013) to act as a comparison in our experiments. We used a convolutional neural network with architecture detailed in Table 2, using a Rectified Linear Unit (ReLU) activation function after each convolutional and hidden fully-connected layer. We also performed a parameter sweep to determine the training hyperparameters. The search space is as follows: Learning Rate (\ud835\udefc) over {10\u22125, 10\u22124, 10\u22123} Discount Factor (\ud835\udefe) over {0.9, 0.95, 0.99} \ud835\udf16-greedy Decay Steps over {5 \u00d7 103, 2 \u00d7 104, 105, 5 \u00d7 105, 106} The model presented for comparison in the Results, as determined by the greatest evaluation environment reward, used \ud835\udefc= 10\u22125, \ud835\udefe= 0.9, \ud835\udf16decay = 105. The following hyperparameters were used for all DQN training experiments: initial \ud835\udf16= 1.0, final \ud835\udf16= 0.1, replay buffer size of 104, batch size of 128, and a target network update frequency of 1,000 steps. All experiments used 6 million training episodes. Experiment Configuration Training Protocol We trained two types of agent: one exclusively on the Snake-Standard environment and another on FrozenLake-Standard. When training, we limited the number of episode steps to 10. We repeated training 8 times with different random seeds. All agents were then evaluated on all four environment variants. Training was conducted for 700 steps of Algorithm 1 from the Methodology. We generated a group of \ud835\udc3a= 100 episodes and sampled \ud835\udc3a\u2032 = 25 from those, with an episode sampling temperature of 2 Table 2: Overview of the DQN architecture structure. Input shape is (\ud835\udc35, \ud835\udc36, \ud835\udc3b, \ud835\udc4a), where \ud835\udc35is the batch size, \ud835\udc36is the number of input channels, \ud835\udc3bis the height of the environment grid and \ud835\udc4ais its width. \ud835\udc34is the number of discrete actions. Layer Block Layer Type Parameters / Details Output Shape Input - - (\ud835\udc35, \ud835\udc36, \ud835\udc3b, \ud835\udc4a) Conv 1 Conv2d 32 filters, kernel 3x3, stride 1, pad 1 (\ud835\udc35, 32, \ud835\udc3b, \ud835\udc4a) ReLU - (\ud835\udc35, 32, \ud835\udc3b, \ud835\udc4a) Conv 2 Conv2d 64 filters, kernel 3x3, stride 1, pad 1 (\ud835\udc35, 64, \ud835\udc3b, \ud835\udc4a) ReLU - (\ud835\udc35, 64, \ud835\udc3b, \ud835\udc4a) Conv 3 Conv2d 64 filters, kernel 3x3, stride 1, pad 1 (\ud835\udc35, 64, \ud835\udc3b, \ud835\udc4a) ReLU - (\ud835\udc35, 64, \ud835\udc3b, \ud835\udc4a) Flatten - - (\ud835\udc35, 64 \u00d7 \ud835\udc3b\u00d7 \ud835\udc4a) FC 1 Linear 512 output units (\ud835\udc35, 512) ReLU - (\ud835\udc35, 512) FC 2 (Output) Linear \ud835\udc34output units (\ud835\udc35, \ud835\udc34) \ud835\udc47ep = 0.1. In each episode, a maximum of 5 LAP actions (each of which can specify multiple sequential environment"
      },
      {
        "chunk_id": "2508.10416v1_chunk6.txt",
        "text": "train- ing. Although the model has been trained on these data, we found that it still makes errors when evaluated on the train- ing set. We realize that this is an excellent source for col- lecting correction data. The training dataset not only con- tains abundant data but also includes ground truth reference points. Therefore, we collect error trajectories produced dur- ing model evaluation on the training set. These trajectories can be denoted by Tm = (M1, . . . , Mm), where Mi repre- sents the position of the robot at the i timestep. Step 2 - Trajectory Deviation Detection Since the col- lected error trajectories lack annotations indicating where Navigation Finetuning Large Language Model Projector Vision Encoder Self-correction Flywheel Post-training Instruction \u2026 Summarized Instruction Height RGB Observation with Domain Randomization \u2026 FoV Resolution Light Step 1 Model Evaluation Step 3 Data Creation Step 4 Continued Training Step 2 Deviation Detection Tokenizer Multiple Frames Figure 2: The overview of CorrectNav training. CorrectNav is first finetuned on the navigation tasks (Left), including action prediction and instruction generation. To enhance vision diversity, we implement a suite of domain randomization strategies. Subsequently, CorrectNav is post-trained with our proposed Self-correction Flywheel paradigm (Right). This paradigm operates in a continuous loop of model evaluation, deviation detection, data creation, and continued training. Specifically, the data creation part can automatically collect error-correcting tra- jectory and keyframe perception data. Through multiple training iterations, CorrectNav can learn how to recover from deviations. deviations occur, we develop a method to detect such devia- tions. The key principle is to assess deviations by measuring the distance between the error trajectories and the oracle tra- jectories. To compute the distance from a robot position Mi to the oracle trajectory Tg, we begin by uniformly interpolat- ing between reference points, which forms an evenly spaced sequence T \u2032 g. For each robot position Mi \u2208Tm, we define the distance from Mi to the Tg as hi = min x\u2208T \u2032g \u2225Mi \u2212x\u22252 We further define the orthogonal foot of Mi on Tg as Pi = arg min P \u2208T \u2032g \u2225Mi \u2212P\u22252. Let S be a predefined threshold. If there exists a timestep t such that ht > S and hi \u2264S, \u2200i < t, i \u2208N \u2217 Then we claim that the model begins to deviate from the oracle trajectory at Mt. The observations near timestep t can be marked as keyframes for error correction. Step 3 - Self-correction Data creation By analyzing de- viations in error trajectories, we identify that navigation er- rors primarily originate from perception and action. Accord- ingly, we propose self-correction tasks and data creation methods addressing these two aspects. Error-correcting Trajectory To teach the model how to re- cover from deviations, we collect error-correcting trajecto- ries based on the detected deviations. Given an oracle trajec- tory Tg and model trajectory Tm with deviations, we already detect the deviation point Mt and the corresponding orthog- onal foot Pt in Step 2. If Pt lies on the segment GkGk+1 (Gk, Gk+1 \u2208Tg), we can know the model has correctly passed"
      },
      {
        "chunk_id": "2508.10416v1_chunk1.txt",
        "text": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model Zhuoyuan Yu*12, Yuxing Long*\u202012, Zihan Yang12, Chengyan Zeng2, Hongwei Fan12, Jiyao Zhang12, Hao Dong\u202112 1CFCS, School of Computer Science, Peking University, 2PKU-Agibot Lab *Equal contribution, \u2020 Project Leader, \u2021 Corresponding author https://correctnav.github.io Crowded Objects Avoidance Open-vocabulary Landmark Move Forward and Turn right at the human-like robot. Continue moving to stop near the yellow box. \u2026 Z-Shape Building Structure Walk down the corridor hallway in front of you and you will see an opened meeting room. Enter the \u2026 \u2026 \u2026 Walk straight along the hallway until you reach the red fire extinguisher box at the end and stop when you reach \u2026 Pedestrian Avoidance \u2026 Error Correction \u2026 in front of a white wall, turn right. Walk forward. When you see a green plant on your right front, stop. Drift Correction Walk straight and turn left in front of a wall. Walk straight and turn right at the opened door. Enter and walk to the wooden table. \u2026 \u2026 Walk until you reach the plant and turn left. Walk straight, turn left at the next corner, walk forward to the \u2026 Instruction Across Rooms Walk out of the kitchen room you are in and turn left. Move across the living room, walk to the end of the hallway and turn right .Walk into the bedroom and stop by the bed. Landmark State Change Move forward and turn right to walk through an opened doorway. \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 C rrectNa Figure 1: Diverse Capabilities of CorrectNav. The model takes only monocular RGB video and language instructions as inputs, predicting navigation actions. Empowered by the Self-correction Flywheel post-training, CorrectNav not only maintains outstanding multimodal rea- soning (Blue), but also displays improved deviation correction (Red), obstacle avoidance (Green), and complex action execution (Yellow). Abstract Existing vision-and-language navigation models often devi- ate from the correct trajectory when executing instructions. However, these models lack effective error correction capa- bility, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post- training paradigm. Instead of considering the model\u2019s error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these er- ror trajectories and devised innovative techniques to automat- ically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model\u2019s continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncov- ering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state- of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate CorrectNav\u2019s superior capability of error correction, dynamic obstacle avoidance, and long instruction following. Introduction In the Vision-and-Language Navigation (VLN) task, users control the robot to"
      }
    ]
  },
  {
    "timestamp": "2025-08-17T23:38:53.542990",
    "query": "data creation",
    "results": [
      {
        "chunk_id": "2508.10492v1_chunk28.txt",
        "text": "into two types: one is inquiries or inference based on objective medical knowledge, such as the causes of a specific disease or determining the possible disease based on the patient\u2019s specific symptoms, and so on. These questions are marked as \u201c<LLM>\u201d and their answers can be finished by LLM itself. The other type of question involves inquiring about the clinical operations for diagnostic test or communication to patients. They are marked as \u201c<Physician>\u201d and their answer must be finished with the help of human physicians. When the reasoning is finished, the \u201c[Final Content]\u201d is generated, it is the summary of the reasoning process, with the number of reasoning steps marked at the corresponding positions. This enhances the credibility and error-correctability of the AI-generated diagnostic process. It is worth noting that GPT-4o cannot effectively perform full-process diagnosis. Therefore, to construct this data, we add the patient\u2019s detailed clinical information to the context to GPT-4o. GPT-4o simulates the full-process diagnosis under the premise of already knowing all 23 related information, which simplifies the task significantly. This approach enables us to construct a large amount of data that meets the requirements at a relatively low cost. After transformation, we inject the deep thinking content for each step of \u201cInitial Response\u201d. As shown in deep thinking injection stage in Figure 15, we use o1-preview to generate detailed thinking content for each step of \u201cInitial Response\u201d. This thinking should fully consider the clinical information at the current step and combine it with the ultimate clinical goal to reason about the optimal strategy that should be taken at the current step, which simulates the human \"slow thinking\" process. Deep thinking makes the logical connection between each step in the whole process of clinical diagnosis closer. These contents often only appear in the minds of human physicians and are not written in electronic medical records. The explicit generation of these contents enables DxDirector-7B to have the \"slow thinking\" ability like human physicians. We do not generate deep thinking end-to-end during the data transformation stage because we find that doing so will result in deep thinking revealing currently unknown clinical information in advance. The data instance in final instruction-response pairs for instruction-tuning for full-process clinical diagnosis is the instruction consisting of patient\u2019s chief complaint and clinical question, and the response consisting of multi-step reasoning, deep thinking and final diagnosis. During data construction, we randomly sample the transformed instruction-response pairs and provide them to human medical experts for evaluation to determine whether this data aligns with real clinical diagnostic scenarios. We collect feedback from the medical experts and continuously refine our prompts to optimize the quality of the data. The detailed prompts in data construction can be found in Supplementary Fig. 11 to 20. Finally, we obtain 10, 178 high-quality instruction-response pairs for training. 4.2.2 Training with Decoupled Reasoning and Knowledge We train DxDirector-7B to perform full-process clinical diagnosis on the constructed dataset above. As shown in instruction-response pair of Figure 15, given the instruction, DxDirector- 7B is trained to generate the response consisting of numbered \u201c[Deep Think]\u201d and numbered \u201c[Question]-[Answer]\u201d"
      },
      {
        "chunk_id": "2508.10239v1_chunk24.txt",
        "text": "for Computing Machinery, New York, NY, USA, Article 449, 19 pages. https://doi.org/10.1145/3613904.3642573 [36] Panayu Keelawat. 2023. NBGuru: Generating Explorable Data Science Flowcharts to Facilitate Asynchronous Communication in Interdisciplinary Data Science Teams. In Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing (CSCW \u201923 Companion). Association for Computing Machinery, New York, NY, USA, 6\u201311. https://doi.org/10.1145/ 3584931.3607020 [37] John Lee and Chak Yan Yeung. 2018. Personalizing Lexical Simplification. In Proceedings of the 27th International Conference on Computational Linguistics, Emily M. Bender, Leon Derczynski, and Pierre Isabelle (Eds.). Association for Computational Linguistics, Santa Fe, New Mexico, USA, 224\u2013232. https:// aclanthology.org/C18-1019 [38] Daniel Li, Thomas Chen, Albert Tung, and Lydia B Chilton. 2021. Hierarchi- cal Summarization for Longform Spoken Dialog. In The 34th Annual ACM Symposium on User Interface Software and Technology (Virtual Event, USA) (UIST \u201921). Association for Computing Machinery, New York, NY, USA, 582\u2013597. https://doi.org/10.1145/3472749.3474771 [39] Jian Liao, Adnan Karim, Shivesh Singh Jadon, Rubaiat Habib Kazi, and Ryo Suzuki. 2022. RealityTalk: Real-Time Speech-Driven Augmented Presentation for AR Live Storytelling. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (Bend, OR, USA) (UIST \u201922). Association for Computing Machinery, New York, NY, USA, Article 17, 12 pages. https: //doi.org/10.1145/3526113.3545702 [40] Xingyu \u2019Bruce\u2019 Liu, Vladimir Kirilyuk, Xiuxiu Yuan, Peggy Chi, Alex Olwal, Xiang \u2019Anthony\u2019 Chen, and Ruofei Du. 2023. Experiencing Visual Captions: Aug- mented Communication with Real-time Visuals using Large Language Models. In Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Soft- ware and Technology (UIST \u201923 Adjunct). Association for Computing Machinery, New York, NY, USA, 1\u20134. https://doi.org/10.1145/3586182.3615978 [41] Yuhan Liu, Aadit Shah, Jordan Ackerman, and Manaswi Saha. 2025. Exploring the Design Space of Real-time LLM Knowledge Support Systems: A Case Study of Jargon Explanations. arXiv:2503.00715 [cs.HC] https://arxiv.org/abs/2503.00715 [42] Kyle Lo, Joseph Chee Chang, Andrew Head, Jonathan Bragg, Amy X. Zhang, Cas- sidy Trier, Chloe Anastasiades, Tal August, Russell Authur, Danielle Bragg, Erin Bransom, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Yen-Sung Chen, Evie Yu-Yen Cheng, Yvonne Chou, Doug Downey, Rob Evans, Raymond Fok, Fangzhou Hu, Regan Huff, Dongyeop Kang, Tae Soo Kim, Rodney Kinney, Aniket Kittur, Hyeonsu B. Kang, Egor Klevak, Bailey Kuehl, Michael J. Lan- gan, Matt Latzke, Jaron Lochner, Kelsey MacMillan, Eric Marsh, Tyler Murray, Aakanksha Naik, Ngoc-Uyen Nguyen, Srishti Palani, Soya Park, Caroline Paulic, Napol Rachatasumrit, Smita Rao, Paul Sayre, Zejiang Shen, Pao Siangliulue, Luca Soldaini, Huy Tran, Madeleine van Zuylen, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Marti A. Hearst, and Daniel S. Weld. 2024. The Semantic Reader Project. Commun. ACM 67, 10 (Sept. 2024), 50\u201361. https://doi.org/10.1145/3659096 [43] Li Lucy, Jesse Dodge, David Bamman, and Katherine Keith. 2023. Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 6929\u20136947. https://doi.org/10. 18653/v1/2023.findings-acl.433 [44] Louis Martin, \u00c9ric de la Clergerie, Beno\u00eet Sagot, and Antoine Bordes. 2020. Con- trollable Sentence Simplification. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Nicoletta Calzolari, Fr\u00e9d\u00e9ric"
      },
      {
        "chunk_id": "2508.10492v1_chunk7.txt",
        "text": "(urea nitrogen 28 mg/dL, creatinine 1.6 mg/dL) LLM-Generated Texts Physician Input Texts Retrieved Medical Literature Physician input \u2026 Physician \u2026 Physician Physician input Physician input \u2026 Physician Physician input \u2026 Physician Final Diagnosis Deep Think 1 Deep Think 2 Deep Think 3 Deep Think 4 Deep Think 5 Deep Think 6 Deep Think 7 Figure 3: A case of DxDirector-7B performing the full-process diagnosis starting with only a chief complaint. Most of this process is driven by DxDirector-7B step-by-step reasoning (green texts) and physicians only need to follow its instructions to complete some basic clinical operations (blue texts). is constructed based on publicly available medical question-answering data [16]. We use general- purpose LLM GPT-4o to convert patients\u2019 case reports in the datasets into step-by-step reasoning, and use powerful reasoning LLM o1-preview to enrich the thinking process of each step (details of this can be found in Section 4.2). The automated construction process of this synthetic data is supervised by medical experts. After data construction, we get 10, 178 high-quality instruction- response pairs covering multiple clinical tasks such as diagnosis, differential diagnosis, designing treatment plan, screening, analyzing etiology, and so on. Instruction tuning based on this dataset endows our DxDirector-7B with the preliminary capability to drive a full-process clinical diagnosis and perform deep thinking. The technical details of training can be found in Section 4.2. The third stage is step-level strategy preference optimization. We call the question to be solved in each step derived by deep thinking of DxDirector-7B as \u201cstrategy\u201d. After the second stage, our DxDirector- 7B can generate the strategy step-by-step just like Fig. 2 (c). The third stage enables DxDirector-7B to implicitly compare multiple potential strategies in deep thinking at each step and select the optimal strategy. This ensures that each step in complex clinical reasoning is correct and efficient, so that the diagnosis can be completed accurately while relying on the minimal human physicians\u2019 efforts. The optimization of this stage is performed at step-level. In training data construction, we use multiple sampling to make DxDirector-7B generate multiple different strategies for each step (given the same 5 prefix) and assign different rewards to these strategies. The reward value is determined by both the correctness of the final answer and the quantified physician workload derived from the strategies. Strategies with more correct answers are assigned higher rewards. For strategies with the same correct answers, the strategies that seek more assistance from human physicians will have a lower reward value. In training, DxDirector-7B learns to refine deep thinking to generate the strategy with the highest reward by reward-based reinforcement learning [42] with the principle of ensuring correctness while minimizing the workload of human physicians. Details can be found in Section 4.3. 2.2 Overview of Experiments Evaluation Datasets The evaluation datasets consist of two parts: one is four publicly available medical datasets evaluated automatically based on their provided correct answers, and the other is set of cases in real-world clinical diagnosis with the evaluation participated by medical specialists. For the publicly available datasets, we first collect raw data and then reconstruct them to"
      }
    ]
  },
  {
    "timestamp": "2025-08-17T23:39:15.963310",
    "query": "international conference",
    "results": [
      {
        "chunk_id": "2508.10553v1_chunk11.txt",
        "text": "designed to improve researcher experience and foster reproducibility across experimental se- tups. Initial communications with national and international stakeholders have taken place to explore potential funding opportunities. Further steps are currently under consideration. In parallel, grant proposals have been submitted for one strategic areas: the development of no-code and low-code interfaces to broaden accessibility. Finally, community devel- opment is progressing. A European research network centered on eDIF is being prepared, with the goal of enabling long-term collaboration and shared tooling across institutions. Feedback gathered during the MVP will be formalized into onboarding improvements, training documentation, and platform tutorials and website for eDIF. Future user recruitment will target online communities such as Discord and the NNsight developer network, with the aim of reaching a broader and more diverse user base. A dedicated platform is already accessible through the eDIF website, offering streamlined access to onboarding materials, comprehensive documentation, and supporting the develop- ment of a growing user community. 4 In parallel, online community spaces (e.g., Discord, GitHub) and community events such as reading groups, workshops, or hackathons are 4https://www.edif.ai/ 7 being planned to foster engagement, shared learning, and long- term collaboration. B. Strategic Exploration and Research Directions Beyond operational continuity, several research-driven and usability-focused initiatives are under consideration for the coming project phase. These efforts aim to increase scientific utility, platform flexibility, and accessibility. In parallel, online community spaces (e.g., Discord, GitHub) and community events such as reading groups, workshops, or hackathons are being planned to foster engagement, shared learning, and long-term collaboration. Model and Experiment Diversity A key focus will be enabling cross-model interpretability studies, such as com- paring the internal behavior of DeepSeek and Qwen models. These experiments are intended to explore the generalizability of interpretability methods across architectures. In addition to large-scale international models, particular attention will also be given to smaller European models that are otherwise hard to access, such as BCC/Salamander (developed at HPC Barcelona) and Teuken (developed at Fraunhofer IIS). The deployment of multimodal models (e.g., Qwen-2.5-VL) is also under evaluation, allowing for interpretability research in vision-language contexts. Moreover, the project plans to investigate how model quantization and GPU-level sharding influence interpretability performance. This includes analyzing trade-offs between reduced resource consumption and the fidelity of activation analysis, probing accuracy, or causal tracing workflows. To support such studies, hot swapping between different model instances within the same session is a highly requested feature. This functionality would signifi- cantly enhance the efficiency of comparative experiments and aligns with future developments outlined in the broader NDIF roadmap. Usability and Access Expansion To improve researcher productivity and lower entry barriers, several platform and tooling enhancements are being considered: \u2022 Integration of visual, no-code interfaces and UI compo- nents (e.g., Transluce, Neuronpedia, activation patching, logit lens) to support intuitive and modular model explo- ration without requiring code-level interaction. \u2022 Support for JupyterLab and VS Code to streamline local development through improved debugging capabilities, kernel compatibility, and seamless integration with ex- isting workflows. \u2022 Provision of standalone Python packages\u2014including all required standard libraries and dependencies\u2014for NNsight-based workflows with simplified setup. \u2022 Expansion of"
      },
      {
        "chunk_id": "2508.10421v1_chunk21.txt",
        "text": "are 55,315 instances in total. C.2 Quality Control Figure 5: Example quiz question. We use Prolific to recruit annotators who are native Chinese speakers fluent in English. We only collect the annotations on the data we provide, and no information about the annotators themselves is collected. We have received full consent from the annotators on this task. The annotation process is divided into two phases: pilot phase and main phase. In the pilot phase, 20 participants are selected for pre-screening. They are provided with detailed guidelines and must complete a quiz to demonstrate their understanding. The quiz consists of 30 questions, covering each category and subcategory with 2-3 questions. An example quiz question is shown in Figure 5. The top 5 performers from the pilot phase are selected for the main phase. In the main phase, we randomly assign each translation pair to three annotators while we ensure each annotator sees an equal number of pairs. We sample 100 annotations and evaluate them against the authors\u2019 own annotations, achieving a category accuracy of 66% and subcategory accuracy of 88%. For the final annotations, we take the majority vote and the first author manually resolves any tie cases. This resolution improves the average Cohen\u2019s \u03ba among annotators on category (subcategory) agreement from 0.47 (0.38) to 0.73 (0.69), indicating strong annotator agreement given some translations may have more than one answer. For instance, translating \u865a\u60c5\u5047\u610f(false feelings and intentions, indicating insincerity in one\u2019s intention) into \u201cfalse\u201d can be considered as either a Mistranslation or a Partial Translation. Annotators are compensated at $22 per hour. 18 Published as a conference paper at COLM 2025 GPT-4 Qwen Alma mT0 GTrans 0 0.25 0.5 0.75 1 0.38 0.19 0.21 0.24 0.18 0.29 0.23 0.38 0.34 0.24 0.24 0.35 0.33 0.21 0.28 Mis. Lit. Par. No. Unnat. GPT-4o Qwen2.5-7b Qwen2.5-14b Qwen2.5-72b 0 0.1 0.2 0.3 0.4 0.5 0.08 0.07 0.11 0.07 0.1 0.11 0.11 0.14 0.11 0.09 0.12 0.09 2023 2024 0 1 0.63 0.75 0.62 0.62 0.35 0.29 0.21 0.38 0.23 0.59 0.08 0.04 0.15 0.06 Minor Somewhat A lot 0 1 0.46 0.40 0.50 0.52 0.27 0.36 0.31 0.24 0.27 0.24 0.19 0.24 Figure 6: Left: Top three most commonly seen errors for each system. Mis.: Mistranslation; Lit.: Literal Translation; Par.: Partial Translation; No.: No Translation. Right: Ratio of each severity score for each system, calculated overall all errors and domains. D Further Analysis on Translation Systems D.1 Category Breakdown Figure 7 shows the subcategory composition for Good Translation, Mistranslation, and Partial Translation. It can be seen that subcategories vary across domains and systems. For instance, for Partial Translations, On News and Wikipedia, Alma is likely to omit modifiers (100% and 75%), while on Web and Social Media, it misses the core meaning in most cases (100% and 67%). mT0 is accomplished by a significant number of No Translations in each domain, where its translations are mostly missing from the output. D.2 Frequency Breakdown Figure 9 illustrates each system\u2019s performance in each frequency range across domains. It can be seen that the frequency ranges of idioms are"
      },
      {
        "chunk_id": "2508.10553v1_chunk2.txt",
        "text": "experimentation accessible without local deployment. Beyond its technical design, NDIF explicitly aims to democratize access to inter- pretability workflows by enabling multiple research groups to share pooled GPU resources and pre-loaded models, thereby lowering the barrier to entry for academic institutions lacking large-scale compute infrastructure. Building on this concept, this project proposes the European Deep Inference Fabric (eDIF). A scalable, open infrastructure for mechanistic interpretability research based in Europe. In cooperation with the NDIF team, we conducted a feasibility study to explore key questions: Can NDIF-like infrastructure be effectively deployed under European institutional, funding, and regulatory conditions? What technical adaptations and onboarding processes are required? And how well does such a system support real-world interpretability workflows by academic users? Our study evaluates these dimensions through a pilot deployment, with a particular focus on reproducibility, usability, and alignment with European research goals. eDIF aims to lower access barriers, foster collaborative experimenta- tion, and contribute practical insights for building sustainable, open research infrastructure for large-scale LLM analysis in Europe. By doing so, it promotes transparency, reproducibility, arXiv:2508.10553v1 [cs.CL] 14 Aug 2025 and wider societal access to interpretability research in AI. II. RELATED WORK A number of open-source tools have emerged to support mechanistic interpretability in transformer models. Trans- formerLens [1] enables detailed analysis of internal compo- nents, with support for activation patching and causal tracing. Frameworks like pyvene [7] and baukit [8] offer comple- mentary abstractions for inspecting and modifying model internals. However, all of these tools require researchers to host models locally and manage large-scale compute environments, a significant barrier when working with frontier models. To overcome this limitation, NDIF was developed as a remote infrastructure for interpretability research [6]. NDIF builds on NNsight, a deferred execution API that allows researchers to define interventions in standard PyTorch code, which are then executed remotely on shared, preloaded model instances. This decoupling of experiment design and execution enables co-tenancy, reduces compute costs, and improves accessibility for academic users. At present, Europe lacks equivalent infrastructure. While supercomputing centers such as LUMI [9] or MareNostrum 5 [10] offer general-purpose HPC access, they are not opti- mized for interactive, model-centric interpretability workflows. Regulatory, funding, and access constraints further limit the practical availability of compute for smaller research groups. The eDIF project addresses this gap by establishing a European NDIF-compatible cluster designed for mechanis- tic interpretability. By aligning with existing tooling (e.g., NNsight) and focusing on open-access infrastructure, eDIF supports reproducible, collaborative research within the Eu- ropean context. III. SYSTEM ARCHITECTURE The eDIF project infrastructure encompasses three computa- tional environments: a primary server at Ansbach University of Applied Sciences, and experimental deployments at Friedrich- Alexander University (FAU) and Heilbronn University, en- abling distributed access to neural network introspection ca- pabilities across German research institutions. A. Hardware and Software Components As part of the feasibility study, the server infrastructure was configured with eight NVIDIA RTX A6000 GPUs, each offer- ing 48 GB of GDDR6 memory [11]. This high-memory setup was selected to support concurrent deployment of multiple large language model instances per GPU, which is particularly beneficial in the current"
      }
    ]
  }
]